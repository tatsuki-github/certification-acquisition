{
    "questions": [
        {
            "category": "データエンジニアリング",
            "question": "あるモバイルネットワーク事業者が、Amazon AthenaとAmazon S3を使用して会社の運用を分析・最適化するための分析プラットフォームを構築しています。ソースシステムは、データをCSV形式で送信します。データエンジニアリングチームは、このデータをApache Parquet形式に変換してからAmazon S3に保存したいと考えています。最も労力が少ないソリューションを選択する必要があります。",
            "options": {
                "A": "Amazon EC2インスタンス上のApache Kafka Streamsを使用し、Parquet形式にシリアル化するためにKafka Connect S3を使用。",
                "B": "Amazon Kinesis Data StreamsからCSVデータを取り込み、Amazon Glueを使用してデータをParquet形式に変換。",
                "C": "Apache Spark Structured Streamingを使用し、Parquet形式に変換するためにAmazon EMRクラスタ上でApache Sparkを使用。",
                "D": "Amazon Kinesis Data Firehoseを使用し、Parquet形式に変換。"
            },
            "correct_answer": [
                "D"
            ],
            "explanation": "Amazon Kinesis Data Firehoseはデータの取り込み、変換、保存を自動化でき、Parquet形式への変換もサポートしているため、最も労力が少ないソリューションです。他の選択肢が間違っている理由は以下の通りです。\n\nA: Amazon EC2インスタンス上のApache Kafka Streamsを使用する方法は、Kafkaのセットアップと管理が必要であり、労力がかかります。また、Kafka Connect S3を使用するための追加の設定も必要です。\n\nB: Amazon Kinesis Data StreamsとAmazon Glueを組み合わせる方法は、データの取り込みと変換のために複数のサービスを統合する必要があり、設定と管理が複雑になります。\n\nC: Apache Spark Structured Streamingを使用する方法は、Amazon EMRクラスタの管理とSparkジョブの設定が必要であり、労力がかかります。"
        },
        {
            "category": "データエンジニアリング",
            "question": "機械学習スペシャリストがデータセットに対してセキュリティを強化するための環境を準備しています。このデータセットはAmazon S3に保存され、個人識別情報（PII）が含まれています。要件には、次の事項が含まれています：1. データはVPC外からアクセス可能でなければならない。2. 最小限のメンテナンスでセキュリティを確保する必要がある。この要件を満たす最適な解決策を尋ねています。",
            "options": {
                "A": "VPCエンドポイントを作成し、バケットアクセスポリシーを適用してVPCエンドポイント経由でデータアクセスを制御。",
                "B": "VPCエンドポイントを作成し、VPC外からのアクセスを可能にするバケットアクセスポリシーを適用。",
                "C": "VPCエンドポイントを作成し、AWS IAMロールとWeb ACLを使用してアクセスを制限。",
                "D": "Amazon EC2インスタンスを介して、IAMユーザーグループを使用してアクセスを制限。"
            },
            "correct_answer": [
                "A"
            ],
            "explanation": "VPCエンドポイントを作成し、バケットポリシーでアクセスを制御することで、VPC内から安全にS3データにアクセスできます。これにより、データのセキュリティを確保しつつ、VPC外からのアクセスも可能になります。また、最小限のメンテナンスでセキュリティを確保できます。選択肢B（VPC外からのアクセスを可能にする）は、セキュリティが不十分であり、PIIデータの保護に適していません。選択肢C（Web ACLの使用）は過剰な設定であり、最小限のメンテナンスという要件を満たしません。選択肢D（EC2インスタンスの使用）は、追加のインフラストラクチャが必要となり、メンテナンスが増加する可能性があります。また、VPC外からのアクセスという要件を満たすのが難しくなります。"
        },
        {
            "category": "データエンジニアリング",
            "question": "ジャーナリストが30,000行のログデータをAmazon S3からAmazon Kinesis Data Firehoseに取り込む必要があります。ログの内容に基づいて属性を変換し、変換されたデータをS3に保存するプロセスに関して、最も少ない開発労力で実現できるソリューションを尋ねています。",
            "options": {
                "A": "ローカル環境でログをキャプチャし、Amazon S3に格納。",
                "B": "Amazon RDSを使って変換処理し、その結果をAmazon S3に保存。",
                "C": "Amazon EC2を使用してログを解析し、変換してS3に格納。",
                "D": "Kinesis Data Analyticsを使用して、変換処理を行い、Amazon S3にデータを格納。"
            },
            "correct_answer": [
                "D"
            ],
            "explanation": "Kinesis Data Analyticsを使用すると、ストリーミングデータのリアルタイム変換とS3への保存が可能で、開発労力を最小限に抑えられます。このサービスは、SQLクエリを使用してデータを変換し、結果を直接S3に保存できるため、最も効率的です。また、サーバーレスで管理が容易です。選択肢A（ローカル環境での処理）は、大量のデータを扱う場合に非効率的です。ローカル環境での処理は、スケーラビリティが低く、データの安全性も懸念されます。選択肢B（Amazon RDSの使用）は、リレーショナルデータベースを使用するため、この用途には過剰です。ストリーミングデータの処理に適していません。選択肢C（EC2の使用）は、インフラストラクチャの管理が必要となり、開発労力が増加します。また、スケーリングも手動で行う必要があり、効率的ではありません。"
        },
        {
            "category": "データエンジニアリング",
            "question": "ある監視サービスは、毎分1TBのログデータを生成しています。研究チームは、このデータに対してAmazon Athenaを使用してクエリを実行していますが、クエリパフォーマンスの向上のため、データをどの形式でAmazon S3に保存するのが最適かを尋ねています。",
            "options": {
                "A": "CSVファイル",
                "B": "Parquetファイル",
                "C": "圧縮JSON",
                "D": "Avroファイル"
            },
            "correct_answer": [
                "B"
            ],
            "explanation": "Parquetファイルは列指向形式であり、Athenaによるクエリパフォーマンスを向上させるために最適です。特に大規模データセットに対して効果的です。選択肢A（CSV）は一般的ですが、大規模データに対しては効率が悪くなります。選択肢C（圧縮JSON）は読みやすいですが、クエリ性能は低下します。選択肢D（Avro）は行指向形式であり、Athenaのクエリ最適化には適していません。",
            "references": [
                "https://docs.aws.amazon.com/ja_jp/athena/latest/ug/columnar-storage.html",
                "https://www.youtube.com/watch?v=ch4Z-i8VjNE"
            ]
        },
        {
            "category": "データエンジニアリング",
            "question": "金融サービス会社が、Amazon S3にサーバーレスデータレイクを構築しています。データレイクは以下の要件を満たす必要があります：新旧データのクエリサポート、イベント駆動型ETLパイプラインのサポート、クイックで簡単なメタデータ検索。この要件を満たすソリューションを尋ねています。",
            "options": {
                "A": "AWS Glue Crawlerを使ってS3データをクロールし、AWS LambdaでAWS Glue ETLジョブをトリガーする。",
                "B": "AWS Glue Crawlerを使ってS3データをクロールし、AWS Batchジョブをトリガーする。",
                "C": "AWS Glue CrawlerとCloudWatchを使ってS3データをクロールし、メタデータを収集。",
                "D": "AWS Glue Crawlerを使い、Apache Hiveメタストアでメタデータを検索。"
            },
            "correct_answer": [
                "A"
            ],
            "explanation": "AWS Glue Crawlerでデータをカタログ化し、AWS LambdaでETLジョブをイベント駆動型でトリガーすることで、要件を満たすサーバーレスデータレイクが構築できます。これは、新旧データのクエリ、イベント駆動型ETL、簡単なメタデータ検索のすべての要件を満たします。選択肢B（AWS Batch）はサーバーレスではありません。選択肢C（CloudWatch）はモニタリングに適していますが、ETL処理には適していません。選択肢D（Apache Hiveメタストア）はAWSのマネージドサービスではなく、サーバーレス要件を満たしません。",
            "references": [
                "https://aws.amazon.com/jp/blogs/news/build-and-automate-a-serverless-data-lake-using-an-aws-glue-trigger-for-the-data-catalog-and-etl-jobs/"
            ]
        },
        {
            "category": "データエンジニアリング",
            "question": "機械学習スペシャリストが、ストリーミングデータをApache Parquetファイルに保存し、分析を行うための最適なサービスを尋ねています。",
            "options": {
                "A": "AWS DMS",
                "B": "Amazon Kinesis Data Streams",
                "C": "Amazon Kinesis Data Firehose",
                "D": "Amazon Kinesis Data Analytics"
            },
            "correct_answer": [
                "C"
            ],
            "explanation": "Amazon Kinesis Data Firehoseはストリーミングデータを直接Parquet形式でS3に保存でき、分析に適しています。これは最も効率的で管理の手間が少ないソリューションです。選択肢A（AWS DMS）はデータベース移行に特化しており、ストリーミングデータの継続的な処理には適していません。選択肢B（Kinesis Data Streams）はリアルタイムストリーミング処理に適していますが、Parquet形式での直接保存機能がありません。選択肢D（Kinesis Data Analytics）はストリーミングデータのリアルタイム分析に適していますが、Parquet形式での保存機能は含まれていません。"
        },
        {
            "category": "データエンジニアリング",
            "question": "機械学習スペシャリストが、SageMakerノートブックをS3バケットにアップロードして、キー管理サービス（KMS）を使用して暗号化する方法を尋ねています。",
            "options": {
                "A": "SageMakerノートブックインスタンスにKMSキーを割り当てる。",
                "B": "SageMakerノートブックとS3を関連付け、KMSキーを使用する。",
                "C": "SageMakerにHTTPアクセスを許可し、KMSキーを使用してデータを暗号化する。",
                "D": "S3にアクセスするためのIAMロールを作成し、KMSキーを適用する。"
            },
            "correct_answer": [
                "D"
            ],
            "explanation": "S3へのアクセスを制御するIAMロールを作成し、KMSキーを適用することで、データの暗号化とアクセス制御を実現できます。これにより、S3バケットへのアップロード時に自動的に暗号化が行われます。選択肢Aはノートブックインスタンス自体の暗号化には有効ですが、S3へのアップロードには直接関係しません。選択肢Bは具体的な方法が不明確です。選択肢Cは不適切で、HTTPアクセスの許可は暗号化とは無関係です。",
            "references": [
                "https://docs.aws.amazon.com/ja_jp/sagemaker/latest/dg/encryption-at-rest-nbi.html"
            ]
        },
        {
            "category": "データエンジニアリング",
            "question": "ある大手企業のデータサイエンスチームは、Amazon S3バケットに保存されたデータにアクセスするために、Amazon SageMakerのノートブックを使用しています。ITセキュリティチームは、インターネットに接続可能なノートブック・インスタンスが、悪意のあるプログラムによるデータ・プライバシーの侵害を許さず、セキュリティ・リスクになるのではないかと懸念しています。\n\nこの企業は、すべてのインスタンスが外部ネットワークにアクセスできない保護されたVPC内に留まり、すべてのデータ通信トラフィックがAWSネットワーク内に留まることを要求しています。\n\nデータサイエンスチームは、これらの要件に合わせてノートブックインスタンスの配置にどのような構成を行うべきでしょうか？",
            "options": {
                "A": "Amazon SageMakerノートブックをVPC内のプライベートサブネットに関連付けます。Amazon SageMakerのエンドポイントとS3バケットを同じVPC内に配置する。",
                "B": "Amazon SageMakerノートブックをVPC内のプライベートサブネットに関連付けます。IAMポリシーを使用して、Amazon S3とAmazon SageMakerへのアクセスを許可する。",
                "C": "Amazon SageMakerノートブックをVPC内のプライベートサブネットに関連付けます。VPCにS3 VPCエンドポイントとAmazon SageMaker VPCエンドポイントが接続されていることを確認する。",
                "D": "Amazon SageMakerノートブックをVPC内のプライベートサブネットに関連付けます。VPCにNATゲートウェイと、Amazon S3とAmazon SageMakerへのアウトバウンド接続のみを許可する関連セキュリティグループがあることを確認する。"
            },
            "correct_answer": [
                "C"
            ],
            "explanation": "アウトバウンドも含めて全ての通信が閉域内で完結していることが今回の要件です。\n\nS3はVPC外部のサービスであるため、EC2からVPCエンドポイントを使用せずにS3にアクセスする場合は必ずインターネットを経由してしまいます。VPCエンドポイントを用いることで、閉域内にアクセス経路を確保することができます。選択肢AはVPC内に配置するだけでエンドポイントの設定がないため不十分です。選択肢BはIAMポリシーだけではネットワーク経路を制御できません。選択肢DはNATゲートウェイを使用するため、インターネットを経由する可能性があります。",
            "references": [
                "https://docs.aws.amazon.com/ja_jp/sagemaker/latest/dg/notebook-interface-endpoint.html"
            ]
        },
        {
            "category": "データエンジニアリング",
            "question": "ある製造業では、Amazon S3バケットを使用して構造化データと非構造化データを保存しています。機械学習のスペシャリストは、SQLを使ってこのデータを照会したいと考えています。このデータをクエリするために必要な作業量が最も少ないオプションはどれですか？",
            "options": {
                "A": "AWS Data Pipelineでデータを変換し、Amazon RDSでクエリを実行する。",
                "B": "AWS Glueを使用してデータをカタログ化し、Amazon Athenaでクエリを実行する。",
                "C": "AWS Batchを使用してデータのETLを実行し、Amazon Auroraでクエリを実行する。",
                "D": "AWS Lambdaを使用してデータを変換し、Amazon Kinesis Data Analyticsでクエリを実行する。"
            },
            "correct_answer": [
                "B"
            ],
            "explanation": "AWS Glueのデータカタログには、AWS Glueでの抽出・変換・ロード（ETL）ジョブのソースやターゲットとして使用されるデータの参照が含まれています。データウェアハウスやデータレイクを作成するには、このデータをカタログ化する必要があり、Athenaは標準的なSQLを使用してAmazon S3のデータを簡単に分析できるクエリサービスです。シナリオでは、AWS Glueを使ってデータのETLジョブを実行し、Amazon Athenaを使って処理済みのデータに対してSQLクエリを実行することができます。両方のサービスがサーバーレスであるため、低い運用オーバーヘッドでリソリューションを構築することができます。選択肢A、C、Dはそれぞれ異なるサービスを使用しており、AthenaとGlueの組み合わせほど効率的ではありません。"
        },
        {
            "category": "データエンジニアリング",
            "question": "データサイエンティストは、ある企業のeコマースプラットフォームで偽のユーザーアカウントを発見しなければなりません。この企業は、新たに作成されたアカウントが、以前に特定された不正なユーザーとつながっているかどうかを確認したいと考えています。データサイエンティストはAWS Glueを使用して、企業のアプリケーションログを取り込み時にデータを抽出しています。データサイエンティストが偽のアカウントを検出することができる技術はどれですか？",
            "options": {
                "A": "組み込みのFindDuplicates Amazon Athenaクエリを実行する。",
                "B": "AWS GlueでFindMatches ML変換を作成する。",
                "C": "AWS Glueのクローラーを作成し、ソースデータ内の重複アカウントを推測する。",
                "D": "AWS Glueデータカタログで重複したアカウントを検索する。"
            },
            "correct_answer": [
                "B"
            ],
            "explanation": "Glueを利用したレコードのマッチング機能を選択する必要があります。AWS Lake Formation FindMatchesは、新しい機械学習（ML）変換により、ほとんど人手を介さずに、異なるデータセット間のレコードのマッチングや、重複するレコードの検出を簡単にします。FindMatchesは、異なるデータセット間で顧客のレコードや取引の一致をリンクして結びつけることができます。選択肢AはAthenaのクエリであり、重複検出には不適切です。選択肢Cはクローラーの使用であり、重複検出には直接関係しません。選択肢Dはデータカタログの使用であり、重複検出には不適切です。したがって正解は、AWS GlueでFindMatches ML変換を作成することです。"
        },
        {
            "category": "データエンジニアリング",
            "question": "Amazon AthenaとAmazon S3を使用して、あるモバイルネットワーク事業者は、ビジネスのオペレーションを分析して最適化するための分析プラットフォームを開発しています。ソースシステムは、データをCSV形式でリアルタイムに送信します。データエンジニアリングチームは、Amazon S3にデータを保存する前に、データをApache Parquet形式に変換したいと考えています。実装に必要な作業量が最も少ないのはどの方法でしょうか？",
            "options": {
                "A": "Amazon EC2インスタンス上でApache Kafka Streamsを使用してCSVデータを取り込み、Kafka Connect S3を使用してデータをParquetとしてシリアライズする。",
                "B": "Amazon Kinesis Data StreamsからCSVデータをインジェストし、AWS Glueを使用してデータをParquetに変換する。",
                "C": "Amazon EMRクラスターでApache Spark Structured Streamingを使用してCSVデータを取り込み、Apache Sparkを使用してデータをParquetに変換する。",
                "D": "Amazon Kinesis Data StreamsからCSVデータをインジェストし、Amazon Kinesis Data Firehoseを使用してデータをParquetに変換する。"
            },
            "correct_answer": [
                "B"
            ],
            "explanation": "既存リソースを活用しつつ、オーバーヘッドを最小限にするソリューションが求められます。AWS Glueは、Amazon Kinesis Data Streamsからデータを消費し、継続的に実行されるストリーミング抽出・変換・ロード（ETL）ジョブを作成することができます。Glueジョブでストリーミングデータを処理する際には、Spark Structured Streamingの全機能を利用して、集約、分割、フォーマットなどのデータ変換を行うことができます。この方法でオーバーヘッドを最小化してETL処理を行うことができます。選択肢A、C、Dはそれぞれ異なるサービスを使用しており、GlueとKinesisの組み合わせほど効率的ではありません。",
            "references": [
                "https://dev.classmethod.jp/articles/aws-glue-now-supports-serverless-streaming-etl/"
            ]
        },
        {
            "category": "データエンジニアリング",
            "question": "機械学習のスペシャリストは、Amazon Athenaを使用してAmazon S3に保存されているデータセットを照会する手順を開発しています。データセットには800,000以上のレコードが含まれており、暗号化されていないCSVファイルに保存されています。各レコードのサイズは約1.5MBで、200カラムで構成されています。機械学習のスペシャリストは、検索にかかる時間を短縮するために、データセットをどのように変更すればよいでしょうか？",
            "options": {
                "A": "レコードをApache Parquet形式に変換する。",
                "B": "レコードをJSON形式に変換する。",
                "C": "レコードをGZIP形式のCSVに変換する。",
                "D": "レコードをXML形式に変換する。"
            },
            "correct_answer": [
                "A"
            ],
            "explanation": "データは200ものカラムがあるため、読み取りの際には列指向のフォーマットに変換することで、検索にかかる時間を短縮することが可能です。Apache Parquetは、列指向フォーマットであり、クエリ実行時に特定の列のみをスキャンするため効率的です。選択肢B、C、Dはそれぞれ異なるフォーマットを使用しており、Parquetほど効率的ではありません。"
        },
        {
            "category": "データエンジニアリング",
            "question": "機械学習のスペシャリストが、様々なETL操作を含む毎日のETLルーチンを作成しています。ワークフローは以下のステップで構成されています。\n\n- Amazon S3にデータがアップロードされたらすぐに手順を開始する。\n- すべてのデータセットがAmazon S3でアクセス可能になったら、新しいデータセットを既にAmazon S3にある多数のテラバイトサイズのデータセットに結合するETLタスクを開始する。\n- データセットを結合した結果をAmazon S3に保存する。\n- タスクの1つが失敗した場合、管理者に通知する。\n\nどのアーキテクチャがこれらの要件を満たすことができるでしょうか？",
            "options": {
                "A": "AWS Lambdaを使用してAWS Step Functionsのワークフローをトリガーし、Amazon S3でデータセットのアップロードが完了するのを待ちます。AWS Glueを使用してデータセットを結合する。Amazon CloudWatchアラームを使用して、障害が発生した場合に管理者にSNS通知を送信する",
                "B": "AWS Lambdaを使用してETLワークフローを開発し、Amazon SageMakerノートブックインスタンスを起動する。ライフサイクル構成スクリプトを使用してデータセットを結合し、Amazon S3で結果を永続化する。Amazon CloudWatchアラームを使用して、障害が発生した場合に管理者にSNS通知を送信する",
                "C": "AWS Batchを使用してETLワークフローを開発し、Amazon S3にデータがアップロードされたときにETLジョブの開始をトリガーする。AWS Glueを使用して、Amazon S3のデータセットを結合する。Amazon CloudWatchアラームを使用して、障害が発生した場合に管理者にSNS通知を送信する",
                "D": "AWS Lambdaを使用して他のLambda関数をチェーンし、データがAmazon S3にアップロードされるとすぐにAmazon S3のデータセットを読み込みで結合する。Amazon CloudWatchのアラームを使用して、障害が発生した場合に管理者にSNS通知を送信する"
            },
            "correct_answer": [
                "A"
            ],
            "explanation": "AWS Step Functionsは、AWS Lambda関数や複数のAWSサービスを、ビジネスクリティカルなアプリケーションに簡単に記列できるサーバーレス関数オーケストレーターです。\n\nAWS Step Functionsを通じて、アプリケーションの状態を維持する一連のチェックポイントと設定されたイベントドリブンなワークフローを作成し、実行することができます。\n\nこのシナリオでは、AWS Step Functionsを使って複数のETLジョブを効率的にオーケストレーションすることができます。また、タスク状態、待機状態、失敗状態など、ソリューションの実装に使用できる様々な有用な機能が含まれます。選択肢BのSageMakerノートブックは、ETLタスクの実行には適していません。選択肢CのAWS Batchは、バッチ処理に適しており、リアルタイム処理には不向きです。選択肢DのLambda関数のチェーンは、複雑なワークフローの管理には不向きです。\n\nしたがって正解は、以下の通りです。\n- AWS Lambdaを使用してAWS Step Functionsのワークフローをトリガーし、Amazon S3でデータセットのアップロードが完了するのを待ちます。AWS Glueを使用してデータセットを結合する。Amazon CloudWatchアラームを使用して、障害が発生した場合に管理者にSNS通知を送信する",
            "references": [
                "https://docs.aws.amazon.com/ja_jp/step-functions/latest/dg/tutorial-cloudwatch-events-s3.html"
            ]
        },
        {
            "category": "データエンジニアリング",
            "question": "機械学習のスペシャリストは、ストリーミングデータを取り込み、分析や調査のためにApache Parquetファイルに保存する必要があります。次のサービスのうち、どのサービスがこのデータを正しく取り込み、保存しますか？",
            "options": {
                "A": "AWS DMS",
                "B": "Amazon Kinesis Data Streams",
                "C": "Amazon Kinesis Data Firehose",
                "D": "Amazon Kinesis Data Analytics"
            },
            "correct_answer": [
                "C"
            ],
            "explanation": "AWS上でのデータ変換の仕様を正しく理解している必要があります。Amazon Kinesis Data Firehoseは、データをAmazon S3に保存する前に、入力データ形式をJSONからApache ParquetまたはApache ORCに変換できます。ParquetとORCは、容量を節約し、JSONのような行指向の形式に比べ、より高速なクエリを可能にするカラム型のデータ形式です。したがって正解は、Amazon Kinesis Data Firehoseです。他の選択肢は、データの変換や保存に関してParquet形式への変換をサポートしていないため不適切です。"
        },
        {
            "category": "データエンジニアリング",
            "question": "オンラインファッション企業に勤務する機械学習のスペシャリストが、組織のAmazon S3ベースのデータレイクのためのデータインジェストソリューションを必要としています。スペシャリストは、以下のような将来の機能を可能にする一連のインジェストメカニズムを開発したいと考えています。\n- リアルタイム・アナリティクス\n- 過去のデータのインタラクティブなアナリティクス\n- クリックストリーム・アナリティクス\n- 製品推薦\n\nスペシャリストが利用すべきサービスはどれですか？",
            "options": {
                "A": "データカタログとしてのAWS Glue、リアルタイムデータインサイトのためのAmazon Kinesis Data StreamsとAmazon Kinesis Data Analytics、クリックストリーム分析のためのAmazon ESへの配信用のAmazon Kinesis Data Firehose、パーソナライズされた製品推薦を生成するためのAmazon EMR",
                "B": "データカタログとしてのAmazon Athena、Amazon Kinesis Data StreamsとAmazon Kinesis Data Analyticsによるリアルタイムデータインサイト、Amazon Kinesis Data Firehoseによるクリックストリーム分析、AWS Glueによるパーソナライズされた製品推薦の生成",
                "C": "データカタログとしてのAWS Glue、履歴データのインサイトのためのAmazon Kinesis Data StreamsとAmazon Kinesis Data Analytics、クリックストリーム分析のためのAmazon ESへの配信用のAmazon Kinesis Data Firehose、パーソナライズされた製品レコメンデーションを生成するためのAmazon EMR",
                "D": "データカタログとしてのAmazon Athena、履歴データのインサイトのためのAmazon Kinesis Data StreamsおよびAmazon Kinesis Data Analytics、クリックストリーム分析のためのAmazon DynamoDBストリーム、パーソナライズされた製品レコメンデーションを生成するためのAWS Glue"
            },
            "correct_answer": [
                "A"
            ],
            "explanation": "4つの要件に適合するサービスを組み合わせる必要があります。そのためには、選択肢で登場するサービスの役割を理解している必要があります。\n\nまず、Athenaは履歴データに対してインタラクティブな分析を行うことができますが、データカタログとしての役割を果たすことができません。データカタログとしての役割を果たすサービスはAWS Glueです。\n\nまた、Kinesis Data StreamsとKinesis Data Analyticsの組み合わせによってリアルタイムで生成されるストリーミングデータの分析を行うことが可能になります。\n\nESを用いることでインタラクティブなログ分析、リアルタイムのアプリケーションモニタリング、ウェブサイト検索などを簡単に実行できます。\n\nAmazon EMRとは、Apache Spark、Apache Hive、Prestoなどのオープンソースフレームワークを使用して、データ処理、相互分析、機械学習を行なう業界をリードするクラウドビッグデータプラットフォームです。EMRを用いたデータ処理によって、レコメンデーションを行うことが可能になります。\n\nしたがって正解は以下の通りです：\n- データカタログとしてのAWS Glue、リアルタイムデータインサイトのためのAmazon Kinesis Data StreamsとAmazon Kinesis Data Analytics、クリックストリーム分析のためのAmazon ESへの配信用のAmazon Kinesis Data Firehose、パーソナライズされた製品推薦を生成するためのAmazon EMR。選択肢B、C、Dはそれぞれの要件を満たすための適切なサービスの組み合わせが不足しています。",
            "references": [
                "https://docs.aws.amazon.com/ja_jp/glue/latest/dg/catalog-and-crawler.html"
            ]
        },
        {
            "category": "データエンジニアリング",
            "question": "データサイエンティストは、複数のタイプのデータを保存するためにAmazon S3でデータレイクソリューションを構築しています。データレイクは、サーバーレスメタデータストアとイベントドリブンETLパイプラインをサポートしなければなりません。\nサイエンティストはどのAWSサービスの組み合わせを使うべきでしょうか？（2つ選択）",
            "options": {
                "A": "クローラーの実行時にGlue ETLジョブを開始するLambda関数をトリガーするCloudWatchアラームを設定する",
                "B": "AWS Glueのクローラーを使ってS3のデータを抽出し、AWS Glue Data Catalogを使ってメタデータテーブルを作成する",
                "C": "クローラーの実行が完了したときに、Glue ETLジョブを開始するLambda関数をトリガーするCloudWatchイベントを構成する",
                "D": "AWS Glueクローラーを使用してS3データを抽出し、Amazon EMR上で動作するApache Hiveメタストアを使用してメタデータテーブルを作成する",
                "E": "クローラーの実行が完了したときにLambda関数をトリガーしてAWS Batchジョブを開始するCloudWatchアラームを設定する"
            },
            "correct_answer": [
                "B",
                "C"
            ],
            "explanation": "サーバーレスのデータストアと、イベントドリブンなETLパイプラインの統合をAWSサービスの組み合わせで実現する必要があります。\n\nAWS Glueは、分析、機械学習、アプリケーション開発のためのデータの発見、準備、結合を簡単に行うことができるサーバーレスのデータ統合サービスです。AWS Glueには、データ統合に必要なすべての機能を提供するため、数ヶ月ではなく数分でデータの分析と活用を開始することができます。\n\nジョブは、AWS GlueでETL（抽出・変換・ロード）作業を行うビジネスロジックのことであり、クローラーが完了すると、Lambda関数とAmazon CloudWatch Eventsルールを利用してジョブを自動で開始できます。\n\nしたがって正解は以下の通りです：\n- AWS Glueのクローラーを使ってS3のデータを抽出し、AWS Glue Data Catalogを使ってメタデータテーブルを作成する\n- クローラーの実行が完了したときに、Glue ETLジョブを開始するLambda関数をトリガーするCloudWatchイベントを構成する。選択肢A、D、Eはサーバーレスやイベントドリブンの要件を完全には満たしていません。",
            "references": [
                "https://docs.aws.amazon.com/ja_jp/glue/latest/dg/tutorial-add-crawler.html"
            ]
        },
        {
            "category": "データエンジニアリング",
            "question": "監視サービスは毎分1TBのスケールメトリクスレコードデータを作成します。Amazon Athenaは、このデータに対するクエリを実行するために研究チームによって使用されています。データ数が多いため、クエリの実行速度が遅く、チームは速度向上を求めています。 クエリのパフォーマンスを最適化するために、Amazon S3のレコードはどのように保持すべきでしょうか？",
            "options": {
                "A": "CSVファイル",
                "B": "パーケットファイル",
                "C": "圧縮されたJSON",
                "D": "レコードIO"
            },
            "correct_answer": [
                "B"
            ],
            "explanation": "Amazon Athenaは、CSV、TSV、JSON、またはテキストファイルなどの多様なデータフォーマットをサポートし、Apache ORCやApache Parquetなどのオープンソースの列挙型フォーマットもサポートしています。また、AthenaはSnappy、Zlib、LZO、およびGZIP形式の圧縮データもサポートしています。正確、パーティショニング、カラムナフォーマットを使用することで、パフォーマンスを向上させ、コストを削減することができます。 ParquetとORCのファイルフォーマットは、ともに高速のプッシュダウン（述語フィルタリングともいう）をサポートしています。ParquetとORCはどちらも、カラム値を表すデータブロックを持っています。各ブロックには、最大値や最小値など、そのブロックの統計情報が含まれています。クエリを実行する際には、これらの統計情報をもとに、ブロックを読み取るべきかスキップすべきかを判断します。 Athenaでは、クエリごとにスキャンされたデータ量に応じて課金されます。 データを分割したり、データを圧縮したり、Apache Parquetのような列挙型フォーマットに変換したりすれば、コストを削減し、パフォーマンスを向上させることができます。 Apache Parquetは、オープンソースの列挙型ストレージフォーマットで、他のテキストフォーマットと比較して、アンロードが2倍速く、Amazon S3でのストレージ使用量が6倍少なくなります。選択肢A、C、Dはクエリのパフォーマンスを最適化するための最適なフォーマットではありません。"
        },
        {
            "category": "データエンジニアリング",
            "question": "機械学習のスペシャリストは、AWS Key Management Service（KMS）を使ってサーバー側で暗号化されたAmazon S3バケットにデータセットをアップロードします。機械学習のスペシャリストは、Amazon S3に保存されている同じデータセットにアクセスできるように、Amazon SageMakerノートブックインスタンスをどのように構成すべきですか？",
            "options": {
                "A": "すべてのHTTPインバウンド/アウトバウンドトラフィックを許可するセキュリティグループを定義し、それらのセキュリティグループをAmazon SageMakerノートブックインスタンスに割り当てます",
                "B": "Amazon SageMakerのノートブックインスタンスがVPCにアクセスできるように設定する。KMSキーポリシーで、ノートブックのKMSロールに許可を与えます",
                "C": "Amazon SageMakerノートブックに、データセットへのS3読み取りアクセスを持つIAMロールを割り当てます。そのロールにKMSキーポリシーのパーミッションを付与する",
                "D": "Amazon S3でデータを暗号化するために使用される同じKMSキーをAmazon SageMakerノートブックインスタンスに割り当てます"
            },
            "correct_answer": [
                "C"
            ],
            "explanation": "AWS KMSは、データを保護するための暗号化キーの一元管理を可能にします。SSE-KMSで暗号化されたオブジェクトをS3バケットからダウンロードする際には、AWS KMSキーを複号化するロールが必要です。そのため、正解は以下の通りです。Amazon SageMakerノートブックに、データセットへのS3読み取りアクセスを持つIAMロールを割り当てます。そのロールにKMSキーポリシーのパーミッションを付与する。選択肢Aはセキュリティ上のリスクが高く、選択肢BはVPCアクセスだけでは不十分で、選択肢DはKMSキーの割り当てが誤解を招く可能性があります。"
        },
        {
            "category": "データエンジニアリング",
            "question": "あるセキュリティ企業は、オフィスに配置された100台のカメラを用いた監視ソリューションを試験的に成功させました。カメラの画像はAmazon S3にアップロードされ、Amazon Rekognitionを使ってタグ付けされ、その結果はAmazon ESに保存されました。この企業は現在、このパイロット版を、世界中のオフィス拠点に数百台のビデオカメラを設置したシステムに拡大する可能性を検討しています。その目的は、従業員以外の行動をリアルタイムで検出することです。アーキテクチャとして次のどの選択肢を検討すべきでしょうか？",
            "options": {
                "A": "各ローカルオフィスと各カメラにプロキシサーバーを使用し、RTSP（Real-time Streaming Protocol）によってカメラの映像をAmazon Kinesis Video Streamsビデオストリームにストリーミングする。各ストリームでは、Amazon Rekognition Videoを使用してストリームプロセッサを作成し、既知の従業員のコレクションから顔を検出し、非従業員が検出されたときに警告を発する",
                "B": "各ローカルオフィスと各カメラにプロキシサーバーを使用し、RTSP（Real-time Streaming Protocol）によってカメラの映像をAmazon Kinesis Video Streamsビデオストリームにストリーミングする。各ストリームでは、Amazon Rekognition Imageを使用して、既知の従業員のコレクションから顔を検出し、非従業員が検出されると警告する",
                "C": "AWS DeepLensカメラをインストールし、DeepLens_Kinesis_Videoモジュールを使用して、各カメラのAmazon Kinesis Video Streamsにビデオをストリームする。各ストリームで、Amazon Rekognition Videoを使用してストリームプロセッサを作成し、各ストリームのコレクションから顔を検出し、非従業員が検出されたときに警告を発する",
                "D": "AWS DeepLensカメラをインストールし、DeepLens_Kinesis_Videoモジュールを使用して、各カメラのAmazon Kinesis Video Streamsにストリーミングする。各ストリームで、AWS Lambda関数を実行して画像フラグメントをキャプチャし、Amazon Rekognition Imageを呼び出して既知の従業員のコレクションから顔を検出し、非従業員が検出されたときに警告する"
            },
            "correct_answer": [
                "A"
            ],
            "explanation": "パイロット版ではカメラの台数が大幅に増加するため、スケーラブルなソリューションである必要があります。Amazon Kinesis Video Streamsは、数百万台のデバイスからのストリーミングビデオデータの取り込みに必要なインフラストラクチャを自動的にプロビジョニングし、弾力的にスケーリングします。Amazon Rekognition Videoは、ビデオ内のオブジェクト、シーン、顔を検出し、適切なコンテンツを検出します。そのため、正解は以下の通りです。各ローカルオフィスと各カメラにプロキシサーバーを使用し、RTSP（Real-time Streaming Protocol）によってカメラの映像をAmazon Kinesis Video Streamsビデオストリームにストリーミングする。各ストリームでは、Amazon Rekognition Videoを使用してストリームプロセッサを作成し、既知の従業員のコレクションから顔を検出し、非従業員が検出されたときに警告を発する。選択肢Bは画像処理に適しておらず、選択肢CとDはDeepLensの導入が必要で、スケーラビリティに欠けます。",
            "references": [
                "https://docs.aws.amazon.com/ja_jp/kinesisvideostreams/latest/dg/how-it-works.htmlhttps://docs.aws.amazon.com/ja_jp/kinesisvideostreams/latest/dg/how-it-works.html"
            ]
        },
        {
            "category": "データエンジニアリング",
            "question": "機械学習エンジニアは、ストリーミングデータを取り込んで、調査と分析のためにApache Parquetファイルに保存できる必要があります。次のサービスのうち、このデータを正しい形式で取り込んで保存するのはどれですか。",
            "options": {
                "A": "Amazon Kinesis Data Streams",
                "B": "Amazon Managed Service for Apache Flink",
                "C": "Amazon Kinesis Data Firehose",
                "D": "AWS DMS"
            },
            "correct_answer": [
                "C"
            ],
            "explanation": "Amazon Kinesis Data Firehose は、データを Parquet 形式に変換して S3 や他のストレージサービスに保存する機能をサポートしているため、ストリーミングデータを取り込むことができます。選択肢Aはデータ変換をサポートしておらず、選択肢BはFlinkのセットアップが必要で、選択肢Dはデータ移行に特化しています。"
        },
        {
            "category": "モデリング",
            "question": "ある大手モバイルネットワーク運営会社が、サービスを解約しそうな顧客を予測するための機械学習モデルを構築しています。会社はこれらの顧客にインセンティブを提供する計画を立てていますが、解約のコストはインセンティブのコストよりもはるかに高いとされています。このモデルは、100人の顧客に対してテストした際の混同行列（confusion matrix）を示しています。混同行列（Confusion Matrix）: PREDICTED CHURN（予測された解約） Yes No Yes: 10 4 No: 10 76 質問は、モデル評価結果に基づいて、このモデルが実運用に適している理由を尋ねています。",
            "options": {
                "A": "モデルの精度は86％であり、偽陰性によるコストは偽陽性によるコストよりも少ない。",
                "B": "モデルの精度は86％で、これはモデルの正確さよりも低い。",
                "C": "モデルの精度は86％であり、偽陽性によるコストは偽陰性よりも少ない。",
                "D": "モデルの精度は86％で、これはモデルの正確さよりも高い。"
            },
            "correct_answer": [
                "A"
            ],
            "explanation": "モデルの精度が86％と高く、偽陰性（実際に解約する顧客を解約しないと予測）のコストが偽陽性（実際には解約しない顧客を解約すると予測）のコストよりも低いため、実運用に適しています。他の選択肢が間違っている理由は以下の通りです。\n\nB: モデルの精度が86％であることは、モデルの正確さと同じであり、低いわけではありません。\n\nC: 偽陽性によるコストが偽陰性よりも少ないという記述は誤りです。実際には、偽陰性のコストが高いとされています。\n\nD: モデルの精度が86％であることは、モデルの正確さと同じであり、高いわけではありません。"
        },
        {
            "category": "モデリング",
            "question": "データエンジニアが、顧客のクレジットカード情報を含むデータセットを使用してモデルを構築する必要があります。このデータが暗号化され、クレジットカード情報が安全に保護されていることを確認する方法を尋ねています。",
            "options": {
                "A": "SageMakerインスタンス内のカスタム暗号化アルゴリズムを使用してデータを暗号化し、Amazon SageMakerのDeepARアルゴリズムを使用してクレジットカード番号をランダム化。",
                "B": "IAMポリシーを使用して、S3バケット上のデータを暗号化し、Amazon Kinesisでクレジットカード番号を自動的に破棄し、偽のクレジットカード番号を挿入。",
                "C": "SageMakerランチ設定を使用して、データがVPCにコピーされた後で暗号化し、主成分分析（PCA）アルゴリズムを使用してクレジットカード番号の長さを縮小。",
                "D": "AWS KMSを使用して、S3およびSageMakerのデータを暗号化し、AWS Glueを使用して顧客データからクレジットカード番号を編集。"
            },
            "correct_answer": [
                "D"
            ],
            "explanation": "AWS KMSを使用してデータを暗号化し、AWS Glueでクレジットカード情報を編集することで、データのセキュリティを確保します。これは最も安全で標準的なアプローチです。他の選択肢が間違っている理由は以下の通りです。\n\nA: カスタム暗号化アルゴリズムは、セキュリティの標準化がされておらず、DeepARアルゴリズムは時系列予測に使用されるものであり、クレジットカード番号のランダム化には適していません。\n\nB: IAMポリシーはアクセス制御に使用されるものであり、データの暗号化には直接関与しません。また、Kinesisでのクレジットカード番号の破棄と偽番号の挿入は、データの整合性を損なう可能性があります。\n\nC: SageMakerランチ設定での暗号化は、データの転送中の保護には適していますが、保存時の暗号化には不十分です。また、PCAはデータの次元削減に使用されるものであり、クレジットカード番号の長さを縮小することはセキュリティの観点から不適切です。"
        },
        {
            "category": "モデリング",
            "question": "表示されているグラフは、時系列モデルのテスト結果を示しています。このモデルの動作について、機械学習スペシャリストがどの結論を導くべきかを尋ねています。",
            "options": {
                "A": "モデルはトレンドと季節性を正確に予測している。",
                "B": "モデルはトレンドを正確に予測しているが、季節性はそうではない。",
                "C": "モデルは季節性を正確に予測しているが、トレンドはそうではない。",
                "D": "モデルはトレンドも季節性も正確に予測していない。"
            },
            "correct_answer": [
                "A"
            ],
            "explanation": "グラフが示されていないため、正確な判断はできませんが、選択肢Aが正解とされています。時系列モデルがトレンドと季節性の両方を正確に予測している場合、実際のデータと予測データが非常に近い形で推移します。選択肢Bは、モデルが季節性を捉えられていないことを示しており、トレンドのみの予測に偏っている可能性があります。選択肢Cは、トレンドを捉えられていないことを示しており、季節性のみの予測に偏っている可能性があります。選択肢Dは、モデルが全体的に予測に失敗していることを示しています。これらの選択肢は、モデルの予測能力を部分的または全体的に否定するものであり、正確なモデルの特徴を表していません。ただし、実際のグラフが提供されていない限り、これらの結論を確実に導くことはできません。グラフの詳細な分析が必要です。"
        },
        {
            "category": "モデリング",
            "question": "ある会社が、顧客の行動が「通常」か「不正」かを分類するために、機械学習スペシャリストにバイナリクラスタリングモデルを作成してもらいたいと考えています。データには「アカウントの年齢」と「取引月」が含まれています。図に示されている特徴分布に基づいて、どのモデルが最もよく機能するかを尋ねています。",
            "options": {
                "A": "長短期記憶（LSTM）",
                "B": "勾配ブースティング",
                "C": "ロジスティック回帰",
                "D": "サポートベクターマシン（SVM）"
            },
            "correct_answer": [
                "D"
            ],
            "explanation": "サポートベクターマシン（SVM）は、二値分類問題に強力であり、明確なマージンを持つデータに適しています。特に、「アカウントの年齢」と「取引月」のような特徴を使用して「通常」と「不正」を分類する場合、SVMは効果的です。SVMは非線形の決定境界を学習できるため、複雑なパターンを持つデータにも適しています。選択肢A（LSTM）は時系列データに適していますが、この問題には過剰に複雑で、「アカウントの年齢」と「取引月」という静的な特徴には適していません。選択肢B（勾配ブースティング）は強力な手法ですが、この二値分類タスクにはSVMほど適していない可能性があります。また、過学習のリスクが高くなる可能性があります。選択肢C（ロジスティック回帰）も二値分類に使用されますが、非線形の決定境界を持つデータにはSVMほど効果的ではありません。ロジスティック回帰は線形の決定境界しか学習できないため、複雑なパターンを持つデータには適していません。",
            "references": [
                "https://jp.mathworks.com/discovery/support-vector-machine.html"
            ]
        },
        {
            "category": "モデリング",
            "question": "スペイン語で話す従業員が多い企業のソーシャルメディア投稿を分析し、感情分析を行う必要があります。最も適切な機械学習モデルを尋ねています。",
            "options": {
                "A": "Amazon Translate, Amazon Transcribe, and Amazon Comprehend",
                "B": "Amazon Polly, Amazon Translate, and Amazon Comprehend",
                "C": "Amazon Transcribe, Amazon Translate, and Amazon SageMaker Neural Topic Model (NTM)",
                "D": "Amazon Translate, Amazon Transcribe, and Amazon SageMaker BlazingText"
            },
            "correct_answer": [
                "A"
            ],
            "explanation": "Amazon Comprehendはスペイン語の感情分析をサポートしており、必要に応じてTranslateやTranscribeを組み合わせることで多言語データにも対応できます。Amazon Translateはテキストを翻訳し、Amazon Transcribeは音声をテキストに変換し、Amazon Comprehendは感情分析を行います。これらのサービスの組み合わせが、多言語のソーシャルメディア投稿の感情分析に最も適しています。選択肢B（Amazon Polly）は、テキストを音声に変換するサービスで、この場合には不要です。感情分析には関係ありません。選択肢C（NTM）は、トピックモデリングに使用されるもので、感情分析には直接適していません。選択肢D（BlazingText）は、テキスト分類や単語埋め込みに使用されますが、多言語対応や感情分析に特化したものではありません。",
            "references": [
                "https://aws.amazon.com/jp/what-is/sentiment-analysis/"
            ]
        },
        {
            "category": "モデリング",
            "question": "機械学習スペシャリストがカスタムRekNetモデルをDockerコンテナにパッケージし、NVIDIA GPUを活用するためのセットアップを行いたいと考えています。どのステップを実行するべきかを尋ねています。",
            "options": {
                "A": "NVIDIAドライバーをバンドルしてDockerイメージを構築する。",
                "B": "Dockerコンテナを構成してGPUインスタンスで実行できるようにする。",
                "C": "Amazon SageMaker TrainingJobでGPUフラグを設定する。",
                "D": "Dockerイメージのフレームワーク内でGPUサポートを設定する。"
            },
            "correct_answer": [
                "B"
            ],
            "explanation": "DockerコンテナをGPUインスタンスで実行できるように構成することで、NVIDIA GPUを活用できます。これにより、コンテナがGPUリソースにアクセスできるようになります。具体的には、NVIDIA Docker runtimeを使用し、適切なGPUフラグを設定することで、コンテナがGPUを認識し利用できるようになります。選択肢A（NVIDIAドライバーのバンドル）は、通常ホストマシンに既にインストールされているため不要です。また、ドライバーをコンテナにバンドルすると、ポータビリティが低下する可能性があります。選択肢C（SageMaker TrainingJobでのGPUフラグ設定）は、SageMakerの特定の機能に関連しており、一般的なDockerコンテナの設定には適していません。また、カスタムRekNetモデルのDockerコンテナ化には直接関係ありません。選択肢D（Dockerイメージ内でのGPUサポート設定）は、ホストマシンのGPUリソースへのアクセスを保証しません。GPUサポートはコンテナの実行時に設定する必要があります。"
        },
        {
            "category": "モデリング",
            "question": "ロジスティック回帰モデルを使用してピザを注文するかどうかを予測し、モデルのパフォーマンスを評価するために最も適切な評価指標を尋ねています。",
            "options": {
                "A": "受信者操作特性（ROC）曲線",
                "B": "ミス分類率",
                "C": "平均二乗誤差（MSE）",
                "D": "L2ノルム"
            },
            "correct_answer": [
                "A"
            ],
            "explanation": "ROC曲線は二値分類モデルの性能を評価するのに適しており、ロジスティック回帰モデルに最適です。ROC曲線は、異なる閾値でのモデルの感度（真陽性率）と特異度（1 - 偽陽性率）のトレードオフを視覚化します。これにより、モデルの全体的な性能を評価でき、異なる閾値での性能比較も可能です。選択肢B（ミス分類率）も二値分類の評価に使用されますが、クラスの不均衡に対して敏感ではありません。また、単一の閾値での性能しか示さないため、ROC曲線ほど包括的ではありません。選択肢C（MSE）は回帰問題に適していますが、分類問題には適していません。ロジスティック回帰は確率を出力するため、MSEは適切な評価指標ではありません。選択肢D（L2ノルム）は正則化に使用されますが、モデルの性能評価には直接関係ありません。L2ノルムはモデルのパラメータの大きさを制御するために使用されるもので、予測性能の評価指標ではありません。"
        },
        {
            "category": "モデリング",
            "question": "機械学習スペシャリストが、異常検知モデルを構築しており、システムへのアクセスをリアルタイムで監視する必要があります。ログイン頻度に基づいて不審な動きを検出するためには、どのアプローチが最適かを尋ねています。",
            "options": {
                "A": "PCA",
                "B": "クラスター分析",
                "C": "相関分析",
                "D": "次元削減"
            },
            "correct_answer": [
                "B"
            ],
            "explanation": "クラスター分析は、通常とは異なるパターンを識別するのに適しており、異常検知に有効です。ログイン頻度のような特徴を使用して、通常の行動パターンとは異なるクラスターを特定することができます。これにより、不審な動きを効果的に検出できます。選択肢A（PCA）は主に次元削減に使用され、異常検知には直接適していません。PCAは特徴の圧縮には有効ですが、異常を直接検出するものではありません。選択肢C（相関分析）は変数間の関係を調べるもので、異常検知には適していません。相関分析は2つの変数間の関係を見るもので、多次元のパターンを検出するには適していません。選択肢D（次元削減）は、データの複雑さを減らすために使用されますが、異常検知の主要な手法ではありません。次元削減は前処理として有用ですが、それ自体では異常を検出できません。",
            "references": [
                "https://youtu.be/V4PN2tjEqA8?si=G6Scu7KmZ_-Xir-X"
            ]
        },
        {
            "category": "モデリング",
            "question": "機械学習スペシャリストが回帰モデルを構築していますが、モデルが過学習している可能性があります。これを確認するために、どのメトリクスを使用するべきかを尋ねています。",
            "options": {
                "A": "Root Mean Square Error (RMSE)",
                "B": "コスト関数",
                "C": "混同行列"
            },
            "correct_answer": [
                "A"
            ],
            "explanation": "RMSEはモデルの予測誤差を評価する指標で、トレーニングデータと検証データのRMSEを比較することで過学習を確認できます。トレーニングデータのRMSEが低く、検証データのRMSEが高い場合、モデルが過学習している可能性があります。RMSEは回帰モデルの性能を評価する一般的な指標であり、過学習の検出に適しています。選択肢B（コスト関数）は、モデルのトレーニング中に最小化される関数ですが、過学習の直接的な指標ではありません。コスト関数の値だけでは、モデルが過学習しているかどうかを判断するのは難しいです。選択肢C（混同行列）は分類問題に使用され、回帰問題には適していません。混同行列は真陽性、真陰性、偽陽性、偽陰性の数を示すもので、連続的な予測を行う回帰モデルの評価には使用できません。"
        },
        {
            "category": "モデリング",
            "question": "会社が顧客の行動を通常か不正かを分類するために、データとして「アカウントの年齢」と「取引月」を使用しています。どのモデルが不正な取引を最も高い精度で検出できるかを尋ねています。",
            "options": {
                "A": "決定木",
                "B": "サポートベクターマシン（SVM）",
                "C": "ナイーブベイズ",
                "D": "直線判別関数"
            },
            "correct_answer": [
                "B"
            ],
            "explanation": "サポートベクターマシン（SVM）は、二値分類に強力であり、異なるクラス間の明確な境界を見つけるのに適しています。特に、「アカウントの年齢」と「取引月」のような特徴を使用して不正取引を検出する場合、SVMは効果的です。SVMは非線形の決定境界を学習できるため、複雑なパターンを持つデータにも適しています。選択肢A（決定木）も分類に使用されますが、SVMほど複雑な決定境界を学習できない可能性があります。また、過学習のリスクが高くなる可能性があります。選択肢C（ナイーブベイズ）は、特徴間の独立性を仮定するため、この場合には適していない可能性があります。「アカウントの年齢」と「取引月」の間に相関がある可能性が高いため、ナイーブベイズの仮定に反する可能性があります。選択肢D（直線判別関数）は、線形の決定境界しか学習できないため、複雑なパターンを持つデータには適していません。不正取引のパターンは通常非線形であるため、直線判別関数では十分な精度が得られない可能性があります。",
            "references": [
                "https://avinton.com/academy/naive-bayes/"
            ]
        },
        {
            "category": "モデリング",
            "question": "機械学習スペシャリストが、自然言語処理のモデルを構築し、異なる予測タスクを実行するためにデータを前処理しようとしています。このプロセスで最も適切な方法を尋ねています。",
            "options": {
                "A": "音声データをテキストに変換。",
                "B": "音声データの発音をテキスト化。",
                "C": "テキストデータを音声に変換。",
                "D": "音声テキストの予測モデルを作成。"
            },
            "correct_answer": [
                "A"
            ],
            "explanation": "自然言語処理モデルに音声データを入力する場合、まず音声をテキストに変換する必要があります。これにより、テキストベースの自然言語処理技術を適用できるようになります。音声データをテキストに変換することで、単語の頻度分析、感情分析、トピックモデリングなど、様々なNLPタスクを実行できます。選択肢B（発音のテキスト化）は、音声認識の一部の側面に焦点を当てていますが、一般的な自然言語処理タスクには適していません。発音情報だけでは、文脈や意味を理解するのに十分ではありません。選択肢C（テキストを音声に変換）は、自然言語処理の入力としては逆方向の処理です。NLPモデルは通常テキストデータを扱うため、テキストを音声に変換することは適切ではありません。選択肢D（音声テキストの予測モデル作成）は、具体的な前処理ステップではなく、モデル構築のステップです。前処理の段階では、まずデータを適切な形式に変換することが重要です。"
        },
        {
            "category": "モデリング",
            "question": "自動運転車に関する機械学習モデルの開発において、効率を向上させるためには、どのステップが最も適しているかを尋ねています。",
            "options": {
                "A": "収集したデータに基づいてトレーニングする。",
                "B": "自動ラベル付けを行う。",
                "C": "次元削減を実行する。",
                "D": "モデルにエッジ計算を適用する。"
            },
            "correct_answer": [
                "C"
            ],
            "explanation": "次元削減によりデータの複雑性を低減し、モデルのトレーニング効率を向上させることができます。これにより、計算時間が短縮され、過学習のリスクも軽減されます。選択肢A（データに基づくトレーニング）は基本的なステップですが、効率向上には直接つながりません。選択肢B（自動ラベル付け）はデータ準備の効率を上げますが、モデル自体の効率向上ではありません。選択肢D（エッジ計算）は推論の効率を向上させますが、開発段階での効率向上には適していません。"
        },
        {
            "category": "モデリング",
            "question": "SageMakerでビルトインアルゴリズムを使用してトレーニングジョブを実行する際に、指定する必要があるパラメータを尋ねています。",
            "options": {
                "A": "トレーニングデータのS3位置を指定。",
                "B": "トレーニングアルゴリズムの指定。",
                "C": "ハイパーパラメータの指定。",
                "D": "推論結果の保存場所の指定。"
            },
            "correct_answer": [
                "A"
            ],
            "explanation": "ビルトインアルゴリズムを使用する際には、トレーニングデータのS3位置を指定する必要があります。これは、SageMakerがデータにアクセスするために不可欠です。選択肢B（アルゴリズムの指定）は既にビルトインアルゴリズムを使用すると決めているので不要です。選択肢C（ハイパーパラメータ）は重要ですが、必須ではありません。選択肢D（推論結果の保存場所）はトレーニング時には必要ありません。"
        },
        {
            "category": "モデリング",
            "question": "機械学習スペシャリストがメディア会社と協力して、ウェブサイト上の人気のある記事に対する分類を行っています。曜日を「バイナリ」変数として扱うためにどの変換が最適かを尋ねています。",
            "options": {
                "A": "バイナリゼーション",
                "B": "ワンホットエンコーディング",
                "C": "トークナイゼーション",
                "D": "正規化変換"
            },
            "correct_answer": [
                "B"
            ],
            "explanation": "曜日のようなカテゴリカルな変数はワンホットエンコーディングが最適です。これにより、各曜日を独立した特徴として扱うことができます。選択肢A（バイナリゼーション）は2値分類には適していますが、7つの曜日を表現するには不適切です。選択肢C（トークナイゼーション）はテキストデータの前処理に使用され、この場合には適していません。選択肢D（正規化変換）は数値データに対して使用され、カテゴリカルデータには適していません。"
        },
        {
            "category": "モデリング",
            "question": "データサイエンティストが患者データに基づいて未来の患者アウトカムを予測する機械学習モデルを開発していますが、4,000件の観察のうち451件に欠損データが含まれています。欠損データへの最適な対応を尋ねています。",
            "options": {
                "A": "平均値で補完する。",
                "B": "欠損データを含むすべてのレコードを削除する。",
                "C": "欠損データを持つフィーチャーを無視する。",
                "D": "欠損値のあるフィーチャーに対してデフォルト値を設定する。"
            },
            "correct_answer": [
                "A"
            ],
            "explanation": "平均値での補完は、欠損データが全体の中で適度な割合（この場合約11%）である場合に有効です。これにより、データの損失を最小限に抑えつつ、統計的に妥当な値で補完できます。選択肢B（レコード削除）は多くのデータを失うため不適切です。選択肢C（フィーチャー無視）は重要な情報を失う可能性があります。選択肢D（デフォルト値設定）は、適切なデフォルト値の選択が難しく、バイアスを導入する可能性があります。"
        },
        {
            "category": "モデリング",
            "question": "機械学習スペシャリストが製品推奨システムを展開しており、過去数か月でパフォーマンスが低下しています。モデルが更新されていない原因を尋ねています。",
            "options": {
                "A": "モデルのトレーニングデータが製品の在庫変更を反映していない。",
                "B": "ハイパーパラメータが定期的に更新されていない。",
                "C": "モデルが定期的にトレーニングされていないため、製品の在庫データの変更を反映できていない。",
                "D": "モデルのトレーニングデータが在庫変更に対応していないため。"
            },
            "correct_answer": [
                "C"
            ],
            "explanation": "モデルが定期的にトレーニングされていないと、新しい在庫データの変化を反映できず、パフォーマンスが低下します。これは、推奨システムのような動的な環境で特に重要です。選択肢A、D（トレーニングデータの問題）は部分的に正しいですが、根本的な問題は定期的な再トレーニングの欠如です。選択肢B（ハイパーパラメータ更新）は重要ですが、この場合の主要な問題ではありません。"
        },
        {
            "category": "モデリング",
            "question": "データサイエンスチームがAmazon SageMakerでトレーニングしているが、従来の画像分類アルゴリズムの代わりに、畳み込みニューラルネットワーク（CNN）を使用するためには、どのステップが必要かを尋ねています。",
            "options": {
                "A": "デフォルトの分類アルゴリズムを無効にする。",
                "B": "SageMakerを使用してCNNモデルを設定する。",
                "C": "SageMakerにデータをアップロードしてトレーニングを行う。",
                "D": "SageMakerを使ってニューラルネットワークをテストする。"
            },
            "correct_answer": [
                "B"
            ],
            "explanation": "SageMakerを使用してCNNモデルを設定し、トレーニングを開始することで、従来の分類アルゴリズムの代わりにCNNを利用できます。これには、適切なフレームワーク（例：TensorFlow、PyTorch）の選択とCNNアーキテクチャの定義が含まれます。選択肢A（デフォルトアルゴリズムの無効化）は不要で、SageMakerは様々なアルゴリズムをサポートしています。選択肢C（データのアップロード）は必要ですが、CNNの使用に特化したステップではありません。選択肢D（テスト）はモデル評価の一部ですが、CNNの使用開始には直接関係ありません。"
        },
        {
            "category": "モデリング",
            "question": "機械学習スペシャリストが、ニューラルネットワークのトレーニングでオーバーフィッティングの問題に直面しています。モデルの改善方法を尋ねています。",
            "options": {
                "A": "学習率を増加させる。",
                "B": "データの次元を削減する。",
                "C": "正規化を使用する。",
                "D": "テストデータを増やす。"
            },
            "correct_answer": [
                "C"
            ],
            "explanation": "正規化（例：L2正則化）はモデルの複雑さを抑制し、オーバーフィッティングを防ぐのに有効です。これにより、モデルのパラメータに制約を加え、過度に複雑なモデルを避けることができます。選択肢A（学習率増加）はオーバーフィッティングを悪化させる可能性があります。選択肢B（次元削減）は有効な場合もありますが、重要な情報を失う可能性があります。選択肢D（テストデータ増加）は評価の信頼性を高めますが、直接的にオーバーフィッティングを解決するものではありません。",
            "references": [
                "https://aws.amazon.com/jp/what-is/overfitting/",
                "https://docs.aws.amazon.com/ja_jp/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html"
            ]
        },
        {
            "category": "モデリング",
            "question": "機械学習モデルがSageMaker APIで予測を行った後、AWS CloudWatchに通知を送信するための最適な方法を尋ねています。",
            "options": {
                "A": "SageMaker APIの結果をCloudWatchに送信する。",
                "B": "SageMaker APIの結果をSNSに送信し、CloudWatchに通知を送信する。",
                "C": "CloudWatchを使って通知を送信し、結果を保存する。",
                "D": "SageMaker APIを使って結果を保存し、SNS通知を行う。"
            },
            "correct_answer": [
                "B"
            ],
            "explanation": "SNSを介してCloudWatchに通知を送信することで、柔軟かつスケーラブルに通知を管理できます。これは正しい答えです。選択肢A（直接CloudWatchに送信）は可能ですが、SNSを使用する方が柔軟性が高くなります。選択肢C（CloudWatchで通知と保存）はCloudWatchの主な用途と合致しません。CloudWatchは主にモニタリングとログの収集に使用され、結果の保存には適していません。選択肢D（SageMakerで保存しSNS通知）は、CloudWatchへの通知が含まれていないため不適切です。",
            "references": [
                "https://docs.aws.amazon.com/ja_jp/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html"
            ]
        },
        {
            "category": "モデリング",
            "question": "機械学習スペシャリストが、多くの特徴量を持つ線形回帰やロジスティック回帰モデルを構築しています。しかし、相関の強い特徴量が多く、モデルが不安定になる可能性があります。この問題を軽減するためにどのアプローチが適しているかを尋ねています。",
            "options": {
                "A": "高相関の特徴量に基づいて正規化を実行する。",
                "B": "高相関の特徴量に対して掛け算を行う。",
                "C": "主成分分析（PCA）を使用して、新しい特徴を作成する。",
                "D": "相関係数を使用して特徴を除去する。"
            },
            "correct_answer": [
                "C"
            ],
            "explanation": "主成分分析（PCA）を使用して新しい特徴を作成することが最適な解決策です。PCAは高次元のデータを低次元に圧縮し、相関の強い特徴量を独立した主成分に変換します。これにより、多重共線性の問題を解決し、モデルの安定性を向上させることができます。選択肢A（正規化）は特徴量のスケールを調整しますが、相関の問題は解決しません。選択肢B（掛け算）は問題を悪化させる可能性があります。相関の強い特徴量を掛け算することで、さらに複雑な相関が生まれ、モデルの不安定性が増す可能性があります。選択肢D（特徴の除去）は情報の損失につながる可能性があり、PCAほど効果的ではありません。",
            "references": [
                "https://aiacademy.jp/media/?p=256%E2%80%8B",
                "https://free.kikagaku.ai/tutorial/basic_of_machine_learning/learn/machine_learning_unsupervised"
            ]
        },
        {
            "category": "モデリング",
            "question": "機械学習スペシャリストがディープラーニングモデルを構築しています。トレーニングデータでは良好なパフォーマンスを示していますが、テストデータではパフォーマンスが低下しています。この問題を解決するためにどのアプローチを取るべきかを尋ねています。",
            "options": {
                "A": "正則化を減少させる。",
                "B": "学習率を増加させる。",
                "C": "ドロップアウトを増加させる。",
                "D": "モデルの特徴量を増やす。"
            },
            "correct_answer": [
                "C"
            ],
            "explanation": "ドロップアウトを増加させることで、モデルの過学習を防ぎ、テストデータでのパフォーマンスを改善できます。これは、ニューロン間の共適応を減少させ、モデルの汎化能力を向上させます。選択肢A（正則化の減少）は過学習をさらに悪化させる可能性があります。選択肢B（学習率の増加）は収束を不安定にする可能性があり、問題の解決にはつながりません。選択肢D（特徴量の増加）は、既に過学習している可能性のあるモデルをさらに複雑にし、問題を悪化させる可能性があります。"
        },
        {
            "category": "モデリング",
            "question": "機械学習スペシャリストが車両の画像データセットを使用して、車両の種類とモデルを識別するモデルを再訓練する方法を尋ねています。",
            "options": {
                "A": "すべてのレイヤーをフリーズし、最終的な分類レイヤーだけを再訓練する。",
                "B": "事前訓練された重みを使用して、全層を再訓練する。",
                "C": "事前訓練された重みを使用して、最後のレイヤーを再訓練する。",
                "D": "モデル全体を再訓練する。"
            },
            "correct_answer": [
                "C"
            ],
            "explanation": "事前訓練された重みを使用して最後のレイヤーを再訓練することで、既存の特徴抽出を活用しつつ、新しい分類タスクに適応できます。これは転移学習の一般的なアプローチで、効率的かつ効果的です。選択肢A（全レイヤーをフリーズ）は新しいタスクへの適応が不十分になる可能性があります。選択肢B（全層を再訓練）は計算コストが高く、過学習のリスクがあります。選択肢D（モデル全体を再訓練）は時間がかかり、既存の学習を活用できません。",
            "references": [
                "https://aws.amazon.com/jp/about-aws/whats-new/2022/09/sagemaker-built-in-algorithms-tensorflow-image-classification-algorithms/"
            ]
        },
        {
            "category": "モデリング",
            "question": "ペット保険会社のマーケティングマネージャーが、新規顧客を特定するためのソーシャルメディアを活用した機械学習モデルの構築を検討しています。最適なアプローチを尋ねています。",
            "options": {
                "A": "ソーシャルメディア上で似たプロファイルを見つける。",
                "B": "クラスタリングを使用して、消費者セグメントを識別する。",
                "C": "決定木を使用して、消費者の属性を分析する。",
                "D": "類似の消費者セグメントを特定するために分類モデルを使用する。"
            },
            "correct_answer": [
                "B"
            ],
            "explanation": "クラスタリングを使用することで、類似した消費者セグメントを識別し、ターゲットマーケティングを効果的に行えます。これは教師なし学習の手法で、新規顧客の特定に適しています。選択肢A（似たプロファイルを見つける）は手動のプロセスで、大規模なデータに対しては効率的ではありません。選択肢C（決定木）は特定の属性に基づく分類に適していますが、新規顧客の発見には適していません。選択肢D（分類モデル）は既知のカテゴリーへの分類に適していますが、新しいセグメントの発見には適していません。",
            "references": [
                "https://aws.amazon.com/jp/blogs/news/k-means-clustering-with-amazon-sagemaker/",
                "https://ledge.ai/articles/clustering"
            ]
        },
        {
            "category": "モデリング",
            "question": "機械学習スペシャリストがTensorFlowを使って時系列予測モデルのトレーニングを行っていますが、モデルが大規模化し、トレーニング時間が長くなっています。将来に向けてスケーリングを容易にするための最適なソリューションを尋ねています。",
            "options": {
                "A": "GPUを増設する。",
                "B": "TensorFlowコードを変更して、SageMakerで並列処理を行う。",
                "C": "SageMakerのDeepARモデルに切り替える。",
                "D": "EMRに移行してスケーラビリティを確保する。"
            },
            "correct_answer": [
                "B"
            ],
            "explanation": "TensorFlowコードをSageMakerでの並列処理に適応させることで、トレーニング時間を短縮し、スケーラビリティを向上させることができます。これは将来的なスケーリングも容易にします。選択肢A（GPUを増設する）は一時的な解決策にはなりますが、長期的なスケーラビリティには限界があります。選択肢C（DeepARモデルに切り替える）は時系列予測には適していますが、既存のTensorFlowコードを活用できません。選択肢D（EMRに移行する）はビッグデータ処理には適していますが、機械学習モデルのトレーニングには最適ではありません。",
            "references": [
                "https://docs.aws.amazon.com/ja_jp/sagemaker/latest/dg/model-parallel-customize-training-script-tf.html"
            ]
        },
        {
            "category": "モデリング",
            "question": "機械学習の分類モデルを比較する際に、どのメトリクスが最も適しているかを尋ねています。",
            "options": {
                "A": "リコール",
                "B": "ミス分類率",
                "C": "平均絶対誤差（MAPE）",
                "D": "ROC曲線の下の面積（AUC）"
            },
            "correct_answer": [
                "D"
            ],
            "explanation": "ROC曲線の下の面積（AUC）は分類モデルの性能を包括的に評価できるため、比較に最適です。これは異なる閾値での真陽性率と偽陽性率のトレードオフを示し、モデルの全体的な性能を一つの数値で表現します。選択肢A（リコール）は重要なメトリクスですが、精度とのバランスを考慮していません。選択肢B（ミス分類率）は簡単ですが、クラスの不均衡を考慮していません。選択肢C（MAPE）は回帰問題に適していますが、分類問題には適していません。"
        },
        {
            "category": "モデリング",
            "question": "画像認識モデルをトレーニングしている機械学習スペシャリストが、猫の誤分類に直面しています。この問題に対処するための適切なアプローチを尋ねています。",
            "options": {
                "A": "トレーニングデータを増やす。",
                "B": "エポック数を増やす。",
                "C": "ネットワークのレイヤーを増やす。",
                "D": "ドロップアウトレートを増加させる。"
            },
            "correct_answer": [
                "A"
            ],
            "explanation": "トレーニングデータを増やすことで、モデルがより多くのバリエーションを学習し、誤分類を減少させることができます。これは特に画像認識タスクで効果的です。選択肢B（エポック数を増やす）は過学習のリスクがあり、必ずしも誤分類の問題を解決しません。選択肢C（ネットワークのレイヤーを増やす）は複雑性を増すかもしれませんが、データの多様性が不足している場合は効果が限られます。選択肢D（ドロップアウトレートを増加させる）は過学習を防ぐのに役立ちますが、誤分類の直接的な解決策にはなりません。"
        },
        {
            "category": "モデリング",
            "question": "データサイエンティストがモデルフェーズに進む前に、データセットを適切に分割し、トレーニング、検証、テストを行う手順について尋ねています。",
            "options": {
                "A": "データセットをトレーニング、検証、テストに分割し、リサンプリングを適用する。",
                "B": "データセットを分割し、検証セットにスケーリングを適用する。",
                "C": "データセットを分割し、スケーリング後にテストセットを再適用する。",
                "D": "トレーニング、検証、テストに分割し、テストを独立してスケーリングする。"
            },
            "correct_answer": [
                "A"
            ],
            "explanation": "データセットをトレーニング、検証、テストに分割し、リサンプリングを適用することで、モデルの評価が公平に行えます。これにより、データの偏りを減らし、モデルの汎化性能を適切に評価できます。選択肢B（検証セットのみスケーリング）は不適切で、全セットに一貫したスケーリングが必要です。選択肢C（スケーリング後にテストセットを再適用）はデータリークを引き起こす可能性があります。選択肢D（テストを独立してスケーリング）は、トレーニングデータとテストデータの分布を変えてしまい、評価の信頼性を損なう可能性があります。",
            "references": [
                "https://docs.aws.amazon.com/ja_jp/sagemaker/latest/dg/data-wrangler-transform.html"
            ]
        },
        {
            "category": "モデリング",
            "question": "機械学習スペシャリストが、サイバーセキュリティイベントをリアルタイムで管理する大手企業と協力して、異常検知のための機械学習モデルを設計しています。企業は、データをリアルタイムで処理して分析することを目的としています。最も効率的な解決策を尋ねています。",
            "options": {
                "A": "Kinesis Data FirehoseとKinesis Data Analyticsを使用して異常を検出し、S3に保存。",
                "B": "EMRとSpark Streamingを使用してリアルタイムで異常を検出し、S3に保存。",
                "C": "S3にデータを保存し、AWS GlueとSageMakerで異常を検出。",
                "D": "S3にデータを保存し、AWS GlueとRandom Cut Forest（RCF）を使用して異常を検出。"
            },
            "correct_answer": [
                "A"
            ],
            "explanation": "Kinesis Data FirehoseとKinesis Data Analyticsを組み合わせることで、リアルタイムでデータを処理・分析し、異常を効率的に検出できます。これはストリーミングデータの処理に最適化されており、低レイテンシーでスケーラブルなソリューションを提供します。選択肢B（EMRとSpark Streaming）も有効ですが、セットアップと管理が複雑になる可能性があります。選択肢C（S3、Glue、SageMaker）はバッチ処理に適していますが、リアルタイム処理には適していません。選択肢D（S3、Glue、RCF）も同様にバッチ処理向けで、リアルタイムの要件を満たしません。"
        },
        {
            "category": "モデリング",
            "question": "小売会社が、新製品の予測を行うための機械学習モデルを構築しています。適切なアルゴリズムを選択するための基準について尋ねています。",
            "options": {
                "A": "XGBoostモデル",
                "B": "リカレントニューラルネットワーク（RNN）",
                "C": "決定木モデル",
                "D": "ディープラーニングモデル"
            },
            "correct_answer": [
                "A"
            ],
            "explanation": "XGBoostは高精度な予測を提供するため、販売予測などの回帰問題に最適です。特に、構造化データを扱う場合に効果的で、特徴量の重要度も解釈しやすいです。選択肢B（RNN）は時系列データに適していますが、新製品の予測には過剰に複雑な可能性があります。選択肢C（決定木）は解釈性は高いですが、XGBoostほどの予測精度は期待できません。選択肢D（ディープラーニング）は大量のデータがある場合に有効ですが、新製品の予測には過剰に複雑で、解釈が難しい可能性があります。"
        },
        {
            "category": "モデリング",
            "question": "データサイエンティストが、感情分析を実行するモデルを評価しています。テストの精度が低い場合、改善するためにどのアプローチを使用するべきかを尋ねています。",
            "options": {
                "A": "Amazon Comprehendでシンタックス解析を行う。",
                "B": "SageMaker BlazingTextを使用して単語を抽出する。",
                "C": "自然言語処理ツールキット（NLTK）を使用してストップワードを削除する。",
                "D": "TF-IDFベクトル化を使用して語彙を正規化する。"
            },
            "correct_answer": [
                "D"
            ],
            "explanation": "TF-IDFベクトル化を使用して語彙を正規化することで、モデルの特徴量が改善され、精度が向上する可能性があります。これにより、重要な単語に適切な重みを付けることができます。選択肢A（Amazon Comprehend）は有用なツールですが、モデルの精度向上に直接寄与するわけではありません。選択肢B（BlazingText）は単語の埋め込みに有効ですが、感情分析の精度向上には他のステップも必要です。選択肢C（ストップワードの削除）は前処理として有効ですが、それだけでは精度向上に十分ではありません。",
            "references": [
                "https://shishimaro.co.jp/blog/ai/572",
                "https://www.youtube.com/watch?v=Jf6k8SktA0U"
            ]
        },
        {
            "category": "モデリング",
            "question": "機械学習スペシャリストが、雇用率予測モデルを構築しており、経済データの大きな変動に直面しています。データの規模がモデルにどのように影響するかを最小限にするために、どのアプローチが最適かを尋ねています。",
            "options": {
                "A": "分布の正規化を適用して、データの相関関係を調整する。",
                "B": "カルテシアン積変換を適用して、特徴量の相互関係を確認する。",
                "C": "特徴量スケーリングを適用してデータのばらつきを軽減する。",
                "D": "量子化を適用して類似する特徴を特定する。"
            },
            "correct_answer": [
                "C"
            ],
            "explanation": "特徴量スケーリングを適用することで、データのばらつきを均一化し、モデルの安定性とパフォーマンスを向上させます。これにより、大きな変動を持つ経済データの影響を最小限に抑えることができます。選択肢A（分布の正規化）はデータの分布を変更しますが、必ずしもスケールの問題を解決しません。選択肢B（カルテシアン積変換）は特徴量間の関係を探るのに役立ちますが、スケールの問題を直接解決しません。選択肢D（量子化）はデータの圧縮に有用ですが、連続的な経済データの予測には適していない可能性があります。"
        },
        {
            "category": "モデリング",
            "question": "政府機関が、各州ごとのデータを収集して医療や社会プログラムに関する情報を分析しています。最も適切なアルゴリズムを選択する必要があります。",
            "options": {
                "A": "K-means",
                "B": "線形回帰",
                "C": "ロジスティック回帰",
                "D": "クラスタリング"
            },
            "correct_answer": [
                "A"
            ],
            "explanation": "K-meansは、データポイントを似た特徴を持つグループ（クラスター）に分類するのに適しています。各州のデータを類似した特徴を持つグループに分類し、分析するのに最適です。線形回帰は連続的な値の予測に、ロジスティック回帰は二値分類に使用されるため、この場合には適していません。クラスタリングは手法の総称であり、K-meansはその一種です。",
            "references": [
                "https://aiacademy.jp/media/?p=254",
                "https://qiita.com/shuva/items/bcf700bd32ae2bbb63c7"
            ]
        },
        {
            "category": "モデリング",
            "question": "大手消費財メーカーが新製品の需要予測を行っています。どのアルゴリズムが最適かを尋ねています。",
            "options": {
                "A": "ARIMAモデル",
                "B": "SageMaker DeepARアルゴリズム",
                "C": "SageMaker Linear Learnerアルゴリズム",
                "D": "XGBoostモデル"
            },
            "correct_answer": [
                "B"
            ],
            "explanation": "DeepARアルゴリズムは時系列予測に特化しており、製品の需要予測に最適です。複数の関連する時系列データを同時に予測でき、季節性や休日の影響も考慮できます。ARIMAも時系列予測に使用されますが、複雑なパターンの捕捉にはDeepARの方が優れています。Linear Learnerは線形モデルに適していますが、時系列データの複雑なパターンを捉えるのは難しいです。XGBoostは強力な予測モデルですが、時系列データの特性を活かすにはDeepARの方が適しています。",
            "references": [
                "https://aws.amazon.com/jp/blogs/news/now-available-in-amazon-sagemaker-deepar-algorithm-for-more-accurate-time-series-forecasting/"
            ]
        },
        {
            "category": "モデリング",
            "question": "データサイエンティストが、100件の連続的な数値レコードを基に顧客離脱モデルを構築しています。マーケティングチームはモデルの性能を向上させる方法についてアドバイスを求めています。最適な改良方法を尋ねています。",
            "options": {
                "A": "クラス分類にAdaboostを追加する。",
                "B": "ニューラルネットワークの層を追加する。",
                "C": "追加の特徴量を追加する。",
                "D": "ストキャスティック近傍埋め込み（t-SNE）を使用する。"
            },
            "correct_answer": [
                "C"
            ],
            "explanation": "追加の特徴量を追加することで、モデルがより多くの情報を学習し、性能の向上が期待できます。100件という少ないデータ数では、複雑なモデルよりも有用な特徴量の追加が効果的です。Adaboostの追加は過学習のリスクがあります。ニューラルネットワークの層の追加も、データ数が少ないため効果的ではありません。t-SNEは可視化や次元削減に使用されますが、直接モデルの性能向上にはつながりません。"
        },
        {
            "category": "モデリング",
            "question": "機械学習チームがSageMakerで独自のトレーニングアルゴリズムを使用しています。チームがアルゴリズムをAmazon SageMakerにデプロイするために必要なサービスの組み合わせを尋ねています。",
            "options": {
                "A": "AWS CodeBuild",
                "B": "AWS Lambda",
                "C": "AWS ECS",
                "D": "AWS ECR",
                "E": "AWS CloudFormation"
            },
            "correct_answer": [
                "D",
                "E"
            ],
            "explanation": "AWS ECR（Elastic Container Registry）にDockerイメージを保存し、AWS CloudFormationでインフラを管理することで、SageMakerへのデプロイが可能になります。ECRはDockerイメージの保存と管理に使用され、CloudFormationはインフラストラクチャをコードとして定義し、デプロイを自動化します。CodeBuildはビルドプロセスに使用されますが、必須ではありません。Lambdaはサーバーレスコンピューティングサービスで、この場合は適していません。ECSはコンテナオーケストレーションサービスですが、SageMakerデプロイには直接必要ありません。"
        },
        {
            "category": "モデリング",
            "question": "エネルギー業界のリスク要因を評価するための短期記憶（LSTM）モデルを使用していますが、モデルの性能が期待を下回っています。性能を最大化するためのアプローチを尋ねています。",
            "options": {
                "A": "単語の頻度に基づいたベクトルを使用する。",
                "B": "LSTMの代わりにGRUを使用する。",
                "C": "学習率を減少させる。",
                "D": "単語埋め込みを使用する。"
            },
            "correct_answer": [
                "D"
            ],
            "explanation": "単語埋め込みを使用することで、テキストデータの意味的な特徴を捉え、LSTMモデルの性能を向上させることができます。これにより、単語間の関係性や文脈を考慮した学習が可能になります。単語の頻度ベクトルは単純すぎて、複雑な関係性を捉えられません。GRUはLSTMの代替として有効ですが、必ずしも性能向上につながるとは限りません。学習率の減少は過学習を防ぐ可能性がありますが、モデルの表現力を向上させるわけではありません。"
        },
        {
            "category": "モデリング",
            "question": "機械学習スペシャリストがローカルでトレーニングされたロジスティック回帰モデルをAmazon SageMakerにデプロイするために必要な手順を尋ねています。",
            "options": {
                "A": "Dockerイメージを作成してAmazon ECRにアップロードする。",
                "B": "モデルをシリアライズしてDockerにアップロードする。",
                "C": "モデルを再トレーニングしてDockerにタグ付けする。",
                "D": "モデルのフォーマットをデプロイ用に変換する。"
            },
            "correct_answer": [
                "A"
            ],
            "explanation": "Dockerイメージを作成してAmazon ECRにアップロードすることで、SageMakerにロジスティック回帰モデルをデプロイできます。これにより、モデルと必要な依存関係を含む環境を一貫して再現できます。モデルのシリアライズは必要ですが、それだけでは不十分です。モデルの再トレーニングは不要で、既存のモデルをデプロイすることが目的です。モデルのフォーマット変換も必要ありません。SageMakerは様々なフォーマットのモデルをサポートしています。"
        },
        {
            "category": "モデリング",
            "question": "金融サービス会社が、顧客の信用スコアを構築するモデルを開発し、重要な変数間の相関関係を分析したいと考えています。相関を評価するためのアプローチを尋ねています。",
            "options": {
                "A": "高相関の特徴量を削除する。",
                "B": "データの分散を正規化する。",
                "C": "PCAを使用して次元を削減する。",
                "D": "クラスタリングを使用して相関関係を調べる。"
            },
            "correct_answer": [
                "C"
            ],
            "explanation": "主成分分析（PCA）を使用することで、相関の強い特徴量をまとめて次元を削減し、モデルの安定性と解釈性を向上させることができます。PCAは変数間の相関を考慮しながら、データの主要な構造を保持します。高相関の特徴量の削除は情報損失につながる可能性があります。データの分散の正規化は重要ですが、相関関係の分析には直接関係しません。クラスタリングはデータポイントのグループ化に使用されますが、変数間の相関関係の分析には適していません。"
        },
        {
            "category": "モデリング",
            "question": "ある金融機関では、クレジットカードの不正使用を確認しようとしています。同社によると、クレジットカード取引の約2%が不正行為だそうです。データサイエンティストは、1年分のクレジットカード取引データをもとに、分類器を学習させました。このモデルは、不正な取引（ポジティブ）と正当な取引（ネガティブ）を区別しなければならない。この会社の目的は、できるだけ多くのポジティブな取引を正しくキャッチすることです。\nデータサイエンティストがモデルを最適化するために使用すべきメトリクスはどれですか？（2つ選択）",
            "options": {
                "A": "特異性",
                "B": "偽陽性率",
                "C": "精度",
                "D": "Precision-Recall 曲線（PR曲線）下面積",
                "E": "真陽性率"
            },
            "correct_answer": [
                "D",
                "E"
            ],
            "explanation": "今回のケースであれば確実に不正取引（陽性）を検知することがビジネス上重要であるため、真陽性率を重視する必要があります。また、陽性に関する閾値設定を行うためのグラフとしてPrecision-Recall 曲線（PR曲線）が使用されます。PR曲線は陽性クラスに関する統計量（新陽性率）をプロットしないため、陽性クラスの結果に興味がある場合に有効です。特異性はネガティブクラスの正確さを示すため、今回の目的には適していません。偽陽性率は誤検知の割合を示し、精度は全体の正確さを示しますが、陽性クラスの検出には直接関係しません。",
            "references": [
                "https://qiita.com/TsutomuNakamura/items/a1a6a02cb9bb0dcbb37f",
                "http://www.thothchildren.com/chapter/5c5baad741f88f26724f6b46#:~:text=PR%E6%9B%B2%E7%B7%9A(Precision%2DRecall%20Curve,%E3%81%AB%E6%9B%B2%E7%B7%9A%E3%81%8C%E5%BC%B5%E3%82%8A%E4%BB%98%E3%81%8F%E7%8A%B6%E6%85%8B."
            ]
        },
        {
            "category": "モデリング",
            "question": "企業は、ユーザーの行動を不正なものか正常なものかに分類したいと考えています。機械学習のスペシャリストは、アカウントの年齢と取引日という2つの特徴に基づいてバイナリ分類器を開発したいと考えています。この図は、これらの特徴のクラス分布を示しており、正常データが0、異常データが1のラベル付けがされています。\n\nこの散布図に基づくと、どのモデルが適切でしょうか？",
            "options": {
                "A": "SELU（Scaled Exponential Linear Unit）を用いたLSTM（Long Short-term Memory）モデル",
                "B": "ロジスティック回帰",
                "C": "非線形カーネルを用いたサポートベクターマシン（SVM）モデル",
                "D": "活性化関数がtanhのシングルパーセプトロン"
            },
            "correct_answer": [
                "C"
            ],
            "explanation": "分布を観察すると、それぞれのクラスを線形分離できないことがわかります。したがって、非線形分離が可能な分類モデルを使用する必要があります。\n\nサポートベクターマシン（SVM）は、主に分類タスクに用いられる教師付きアルゴリズムで、決定境界を用いてデータのグループを分離します。放射状基底関数（RBF）カーネルを用いたSVMは、SVM（線形）のバリエーションで、非線形データの分類に使用されます。RBFカーネルは、データをより高い次元にマッピングする効率的な方法を提供します。これによって、線形分離ができないデータであってもより正しい分類を行うことができます。\n\nLSTMは時系列データに適しており、ロジスティック回帰は線形分離に適しています。シングルパーセプトロンは単純な線形モデルであり、非線形分離には不適です。",
            "references": [
                "https://jp.mathworks.com/discovery/support-vector-machine.html​"
            ]
        },
        {
            "category": "モデリング",
            "question": "クラウドに接続されたデバイスを使用して良質な睡眠習慣を奨励している企業が、AWSを使用して睡眠モニタリングアプリケーションをホストしています。このプログラムは、デバイスの使用状況に関する情報をユーザーから収集します。 同社のデータサイエンティストは、ユーザーが同社のガジェットの使用をやめる時期やパターンを予測する機械学習モデルを開発しています。このモデルの予測は、主にアプリケーションで利用され、消費者の行動を予測する最も効果的な方法を決定することを支援します。\n\nデータサイエンティストチームは、機械学習モデルを何度も繰り返し開発し、組織の業務目的と比較しています。モデルの長期的なパフォーマンスを判断するために、チームはモデルが提供する推論の割合を変え、それぞれのバージョンを長期的にテストして実行することを考えています。\n\nどの方法が最も少ない労力でこれらの基準を達成できるでしょうか？",
            "options": {
                "A": "Amazon SageMakerで複数のモデルを構築し、ホストする。各モデルに1つずつ、複数のAmazon SageMakerエンドポイントを作成する。アプリケーション層で推論のために異なるモデルを呼び出すことをプログラムで制御する。",
                "B": "Amazon SageMakerで複数のモデルを構築し、ホストする。複数のプロダクションバリアントを持つAmazon SageMakerのエンドポイント構成を作成する。エンドポイント構成を更新することで、複数のモデルが提供する推論の部分をプログラムで制御する。",
                "C": "異なるタイプの医療機器を考慮して、Amazon SageMaker Neoで複数のモデルを構築し、ホストする。医療機器の種類に応じて、推論のためにどのモデルを起動するかをプログラムで制御する。",
                "D": "Amazon SageMakerで複数のモデルを構築し、ホストする。複数のモデルにアクセスする単一のエンドポイントを作成する。Amazon SageMakerのバッチ変換を使用して、単一のエンドポイントを通じて異なるモデルの呼び出しを制御する。"
            },
            "correct_answer": [
                "B"
            ],
            "explanation": "SageMakerエンドポイントを用いた適切なA/Bテスト手法を考える必要があります。Amazon SageMakerは、プロダクションバリアント機能を使用して、同じエンドポイントの後ろで複数のモデルまたはモデルバージョンをテストすることができます。各ProductionVariantは、MLモデルと、そのモデルをホストするためにデプロイされたリソースを識別します。\n\n複数のモデル間でトラフィック分配を提供するため、リクエストごとに直接バリアントを呼び出すことで、エンドポイントの呼び出しリクエストを複数のproduction variantsに分散させることができます。選択肢Aはエンドポイント管理が煩雑になり、選択肢Cは医療機器に特化しており、選択肢Dはバッチ処理に適しているため、リアルタイム推論には不向きです。",
            "references": [
                "https://aws.amazon.com/jp/blogs/machine-learning/a-b-testing-ml-models-in-production-using-amazon-sagemaker/",
                "https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html"
            ]
        },
        {
            "category": "モデリング",
            "question": "機械学習のスペシャリストは、50の特性を持つ1,000レコードのデータセットに対して、線形最小二乗回帰モデルを実行しています。学習の前に、機械学習のスペシャリストは、2つの特性が互いに完全に線形に依存していることを観察しました。\n\nこれは、線形最小二乗回帰モデルにどのように関連していますか？",
            "options": {
                "A": "バックプロパゲーションアルゴリズムの学習時に失敗する可能性がある。",
                "B": "最適化の際に特異行列を作り、一意の解を定義できなくなる可能性がある。",
                "C": "最適化の際に損失関数を変更してしまい、学習時に失敗する可能性がある。",
                "D": "データに非線形の依存性が生じ、モデルの線形仮定が無効になる可能性がある。"
            },
            "correct_answer": [
                "B"
            ],
            "explanation": "重回帰モデルにおいて、特徴量同士の相関係数が非常に高い（もしくは完全に一致する）場合、多重共線性（マルチコ）という問題を引き起こします。この問題によって、モデルの出力が不安定な状態になってしまいます。選択肢Aはバックプロパゲーションに関するもので、選択肢Cは損失関数の変更に関するもので、選択肢Dは非線形依存性に関するもので、いずれも線形最小二乗回帰の特異行列問題には直接関係しません。",
            "references": [
                "https://docs.aws.amazon.com/ja_jp/sagemaker/latest/dg/data-wrangler-analyses.html",
                "https://www.jspt.or.jp/ebpt_glossary/multicollinearity.html"
            ]
        },
        {
            "category": "モデリング",
            "question": "ある企業は、機械学習モデルの学習データとして使用するためにオンライン広告のクリックデータをAmazon S3データレイクに投入しています。Kinesis Producer Libraryを使用して、クリックデータをAmazon Kinesisデータストリーム（KPL）に追加します。このデータは、データストリームからAmazon Kinesis Data Firehoseの配信ストリームを使ってS3データレイクに投入されます。データ量が増加するにつれ、機械学習のスペシャリストは、データがAmazon S3に供給されるペースが増加していないことに気付きました。さらに、Kinesis Data StreamsやKinesis Data Firehoseで取り込むべきデータのバックログが増加しています。\n\n次のどのステップが、Amazon S3へのデータ取り込みのペースを上げる可能性が最も高いでしょうか？",
            "options": {
                "A": "配信ストリームの書き込み先となるS3プレフィックスの数を増やす。",
                "B": "データストリームの保持期間を短くする。",
                "C": "データストリームのシャードの数を増やす。",
                "D": "Kinesis Client Library（KCL）を使用してコンシューマーを追加する。"
            },
            "correct_answer": [
                "C"
            ],
            "explanation": "Kinesis Data Streamsに送信されるデータは増加している一方で、S3に保存されるデータは増加していないため、Kinesis Data Streamsのキャパシティの問題だと判断できます。Kinesis Data Streamsでは、シャードがデータの読み取りおよび書き込みの単位となるため、シャードの数を増やすことでキャパシティが増加し、S3へのデータ取り込みペースを上げることができます。選択肢AはS3の書き込み先を増やすもので、ストリームのキャパシティには影響しません。選択肢Bはデータ保持期間の変更であり、取り込みペースには影響しません。選択肢Dはコンシューマーの追加であり、ストリームのキャパシティには直接関係しません。"
        },
        {
            "category": "モデリング",
            "question": "このグラフは、時系列を検証するための予測モデルのものです。売り上げの実績と予測\n\nこのグラフを見て、機械学習のスペシャリストはモデルの動作についてどのような結論を導き出すべきでしょうか？",
            "options": {
                "A": "モデルはトレンドと季節性の両方をよく予測しています。",
                "B": "このモデルは、トレンドはよく予測するが、季節性は予測しない。",
                "C": "このモデルは季節性をよく予測しますが、トレンドは予測しません。",
                "D": "このモデルは、トレンドも季節性もうまく予測できません。"
            },
            "correct_answer": [
                "A"
            ],
            "explanation": "このグラフを見ると、実際の利益は上昇傾向にあり、モデルもこの傾向をよく捉えています。また、実際の利益と予測値の両方が1年おきに増加していることも明らかになっています。したがって、正解は「モデルはトレンドと季節性の両方をよく予測しています。」です。選択肢Bは季節性を無視しており、選択肢Cはトレンドを無視しています。選択肢Dはどちらも予測できていないとしていますが、グラフからは両方が予測されていることがわかります。"
        },
        {
            "category": "モデリング",
            "question": "ある小売業者は、機械学習を使って新しい商品を分類することを目指しています。データサイエンスチームは、現在の商品のラベル付きデータセットを提示されました。このデータセットには、1,200個の商品が含まれています。ラベル付きデータセットの各商品には、タイトル、寸法、重量、価格など、15の属性が含まれています。各商品には書籍、ゲーム、ガジェット、映画などのカテゴリがタグ付けされています。\n\n提供された学習データを使って新しい商品を分類するにはどのモデルを使うべきでしょうか？",
            "options": {
                "A": "目的パラメータをmulti:softmaxに設定したXGBoostモデル",
                "B": "最後の層にソフトマックス活性化関数を用いた深層畳み込みニューラルネットワーク（CNN）",
                "C": "商品カテゴリの数と同じ数の木を設定した回帰フォレスト",
                "D": "リカレント・ニューラル・ネットワーク（RNN）に基づくDeepAR予測モデル"
            },
            "correct_answer": [
                "A"
            ],
            "explanation": "テーブルデータに対する多クラス分類モデルを作成する必要があります。XGBoostはテーブルデータの分類タスクにおいて有効なランダムフォレストをベースにしたモデルです。その他のモデルは、テーブルデータにおける多クラス分類モデルには不適切です。選択肢Bは画像データに適しており、選択肢Cは回帰タスクに適しています。選択肢Dは時系列予測に適しており、分類タスクには不向きです。"
        },
        {
            "category": "モデリング",
            "question": "あるテクノロジービジネスでは、複雑なディープニューラルネットワークとGPU処理を用いて、過去の行動履歴に基づいて、現在の顧客に商品を推奨しています。現在、このソリューションは、Amazon S3バケットから各データセットを取得し、会社のGitリポジトリから取得したTensorFlowモデルにロードします。このタスクは、同じS3バケットに継続的に書き込まれながら、何時間も継続するようにスケジュールされています。中央のキューから実行されるこのタスクは、障害が発生した場合には、いつでも中断、再開、継続が可能です。\n\n上層部は、このソリューションのリソース管理の複雑さと、定期的に手順を繰り返すことに伴う費用を心配しています。彼らは、このタスクを週に1回、月曜日から金曜日の営業時までに実行するように自動化したいと考えています。\n\nこのソリューションを効率的に拡張するには、どのアーキテクチャを採用すべきでしょうか？",
            "options": {
                "A": "AWS Deep Learning Containersを使用してソリューションを実装し、GPU対応のスポットインスタンス上でAWS Batchを使用してコンテナをジョブとして実行する。",
                "B": "低コストのGPU対応Amazon EC2インスタンスを使用してソリューションを実装し、AWS Instance Schedulerを使用してタスクをスケジュールする。",
                "C": "AWS Deep Learning Containersを使用してソリューションを実装し、スポットインスタンス上で実行されるAWS Fargateを使用してワークロードを実行し、内蔵のタスクスケジューラを使用してタスクをスケジュールする。",
                "D": "スポットインスタンス上で動作するAmazon ECSを使用してソリューションを実装し、ECSサービススケジューラを使用してタスクをスケジュールする。"
            },
            "correct_answer": [
                "A"
            ],
            "explanation": "適切なコンピューティングリソースを適切なタイミングで動かすことが必要です。スポットインスタンスを用いることで、オンデマンドレートよりも大幅に値引きされた価格で、高性能GPUを利用できます。また、AWS Batchではインスタンスが中断した場合にも再実行できるように設定できます。選択肢Bはインスタンススケジューラを使用しますが、Batchの再実行機能がありません。選択肢CはFargateを使用しますが、GPU対応のスポットインスタンスを活用できません。選択肢DはECSを使用しますが、Batchの再実行機能がありません。",
            "references": [
                "https://youtu.be/IXFm6Ehf1Ic?si=cp6y3wqF43vScRPO"
            ]
        },
        {
            "category": "モデリング",
            "question": "ある医療用画像処理企業が、患者のCT画像上の疑わしい部位を識別するコンピュータビジョンモデルを学習したいと考えています。この企業は、個々の患者に関連付けられ、Amazon S3のバケットに保存されているラベル付けされていないCTスキャンを大量に収集しています。スキャナーの使用は許可されたユーザーに限定する必要があります。機械学習エンジニアは、ラベリング・パイプラインの開発を任されています。\n\n最も少ない労力でラベリングパイプラインを構築するために、エンジニアはどの段階のシーケンスに従うべきでしょうか？",
            "options": {
                "A": "AWS Identity and Access Management (IAM) でワークフォースを作成する。Amazon EC2上にラベリングツールを構築する Amazon Simple Queue Service (Amazon SQS) を使用してラベリング用のキューイメージを作成する。ラベリングの指示を書く。",
                "B": "Amazon Mechanical Turkのワークフォースとマニフェストファイルを作成する。Amazon SageMaker Ground Truthに組み込まれた画像分類タスクタイプを使用して、ラベリングジョブを作成する。ラベリングの手順を書きます。",
                "C": "プライベートワークフォースとマニフェストファイルを作成する。Amazon SageMaker Ground Truthに組み込まれているバウンディングボックスタスクタイプを使用して、ラベリングジョブを作成する。ラベリングの手順を書きます。",
                "D": "Amazon Cognitoでワークフォースを作成する。AWS Amplifyを使用してラベリング Web アプリケーションを構築する。AWS Lambdaを使ってラベリングワークフローバックエンドを構築する。ラベリングの手順を書きます。"
            },
            "correct_answer": [
                "C"
            ],
            "explanation": "今回の要件では、機密性の高い医療データを取り扱うため、社内のスタッフしかアクセスできないようにしながらラベリングジョブを実行する必要があります。このためには、AWS Ground Truthのプライベートワークフォースを構成することが有効な方法です。選択肢AはEC2上にツールを構築する必要があり、手間がかかります。選択肢BはMechanical Turkを使用するため、外部のワークフォースが関与します。選択肢DはCognitoとAmplifyを使用するため、構築が複雑です。",
            "references": [
                "https://docs.aws.amazon.com/ja_jp/sagemaker/latest/dg/sms-workforce-private.html"
            ]
        },
        {
            "category": "モデリング",
            "question": "Amazon SageMakerを使用して、機械学習のスペシャリストが時系列予測のモデルを開発しています。スペシャリストはモデルのトレーニングを完了し、現在はモデルのバリエーションにオートスケーリングを設定するためにエンドポイントのロードテストを行う予定です。\n\nスペシャリストがロードテストのレイテンシー、メモリ使用量、CPU使用率を分析できる方法はどれですか？",
            "options": {
                "A": "Amazon AthenaとAmazon QuickSightを活用して、Amazon S3に書き込まれたSageMakerのログを確認し、生成中のログを可視化する。",
                "B": "Amazon CloudWatchダッシュボードを生成して、Amazon SageMakerによって出力されるレイテンシー、メモリ使用率、およびCPU使用率メトリクスの単一ビューを作成する。",
                "C": "カスタムAmazon CloudWatchログを構築し、Amazon ESとKibanaを活用して、Amazon SageMakerによって生成されるログデータを照会して視覚化する。",
                "D": "Amazon SageMakerによって生成されたAmazon CloudWatch LogsをAmazon ESに送信し、Kibanaを使用してログデータを照会して視覚化する。"
            },
            "correct_answer": [
                "B"
            ],
            "explanation": "今回のケースで注意が必要な点は、メトリクスとログの違いです。今回はあくまでメトリクスを分析することが求められており、ログの可視化や分析は要件外であるということを踏まえる必要があります。Amazon CloudWatchを使用してAmazon SageMakerを監視することができます。選択肢A、C、Dはログの可視化に関するもので、メトリクスの分析には直接関係しません。"
        },
        {
            "category": "モデリング",
            "question": "機械学習のスペシャリストが回帰モデルを開発したものの、最初のイテレーションでは更なる最適化が必要になることがわかりました。スペシャリストは、モデルが目的をより頻繁に過大評価するか過小評価するかを判断しなければなりません。\n\n目標の数値が過大評価されているか過小評価されているかを判断するために、スペシャリストはどのオプションを持っていますか？",
            "options": {
                "A": "平均平方根誤差（RMSE）",
                "B": "残差プロット",
                "C": "曲線下面積",
                "D": "混同行列"
            },
            "correct_answer": [
                "B"
            ],
            "explanation": "回帰問題の評価を行う場合、残差を確認するのが一般的です。残差は、真のターゲットと予測されたターゲットの差です。残差がゼロを中心としたベル型に分布している場合は、モデルがランダムにミスをしており、特定の範囲の目標値を系統的に過大または過小に予測していないことを示しています。選択肢Aは誤差の大きさを示すもので、過大評価や過小評価の判断には不向きです。選択肢Cは分類問題の評価指標であり、選択肢Dは分類問題の混同行列であり、回帰問題には適していません。"
        },
        {
            "category": "モデリング",
            "question": "ペット保険会社のマーケティングマネージャーは、顧客獲得数を増やすために、重点的にソーシャルメディアでのマーケティングキャンペーンを実施しようと考えています。現在、以下のデータがAmazon Auroraに保存されています。\n- 過去および既存のすべての顧客のプロファイル\n- 過去および現在の被保険者であるペットのプロファイル\n- ポリシーレベルの情報\n- 受取保険料\n- 支払われた保険金\nソーシャルメディア上で新規顧客候補を検出するために、機械学習モデルをどのように使用できますか？",
            "options": {
                "A": "消費者セグメントの主要な特性を理解するために、顧客プロファイルデータの回帰を使用する。ソーシャルメディア上で類似したプロフィールを見つける。",
                "B": "顧客プロファイルデータのクラスタリングを使って、消費者セグメントの主要な特性を理解する。ソーシャルメディア上で類似したプロフィールを見つける。",
                "C": "顧客のプロファイルデータにレコメンデーションエンジンを使用して、消費者セグメントの主要な特性を理解する。ソーシャルメディア上で類似したプロフィールを見つける。",
                "D": "決定木分類エンジンを顧客プロファイルデータに使用して、消費者セグメントの主要な特性を理解する。ソーシャルメディア上で類似したプロフィールを見つける。"
            },
            "correct_answer": [
                "B"
            ],
            "explanation": "クラスタリングは、データ間の類似度に応じて、データをグループ分けする教師なし学習の手法です。ターゲットとなる分割方法（優良顧客/通常の顧客）は分かっているものの、そのターゲットを直接表す変数がない場合や、そもそもレコードの特性が十分に理解できておらず、俯瞰的にデータセットの特徴を把握したい場合などで使用されます。クラスタリングの計算方法は大きく分離する「階層クラスタリング」と、階層構造を作らずに分割していく「非階層クラスタリング」の2つに分けられます。したがって正解は、顧客プロファイルデータのクラスタリングを使って、消費者セグメントの主要な特性を理解する。ソーシャルメディア上で類似したプロフィールを見つける。選択肢Aは回帰を使用しており、分類には不向きです。選択肢Cはレコメンデーションエンジンを使用しており、クラスタリングには不適です。選択肢Dは分類エンジンを使用しており、クラスタリングには不向きです。"
        },
        {
            "category": "モデリング",
            "question": "機械学習のスペシャリストは現在、個人がピザを買うかどうかを予測するロジスティック回帰モデルを開発しています。スペシャリストは、最適な分類閾値を持つ最適なモデルを構築しようとしています。様々な分類しきい値がモデルの性能に与える影響を判断するために、スペシャリストはどのモデル評価アプローチを使用すべきでしょうか？",
            "options": {
                "A": "受信者動作特性（ROC）曲線",
                "B": "誤判定率",
                "C": "平均平方根誤差（RMSE）",
                "D": "L1ノルム"
            },
            "correct_answer": [
                "A"
            ],
            "explanation": "ロジスティック回帰分析では、説明変数から「2値の結果」が発生する確率を予測するモデルを構築します。ROC曲線は、分類問題におけるROC曲線を作成し、しきい値に基づく性能の判断に使われます。AUC（Area Under the Curve）は0から1までの値をとり、値が1に近いほど判断能力が高いことを示します。分類問題でPrecisionとRecallのバランスを考慮した場合、ROC曲線が有効です。選択肢Bは誤判定率のみを考慮しており、全体の性能評価には不向きです。選択肢Cは回帰問題の評価指標であり、選択肢Dは正則化の指標であり、分類問題の評価には不適です。"
        },
        {
            "category": "モデリング",
            "question": "あなたは、Amazon SageMakerの組み込みアルゴリズムの一つを使用してトレーニングジョブを開始しようとしています。次のうちどのパラメータを与えなければなりませんか？（3つ選択）",
            "options": {
                "A": "Amazon S3バケット上のトレーニングデータの場所を特定するトレーニングチャンネルです。",
                "B": "Amazon S3バケット上の検証データの場所を示す検証チャンネル。",
                "C": "Amazon SageMakerがユーザーに代わってタスクを実行するために引き受けることのできるIAMロール。",
                "D": "使用されるアルゴリズムのために文書化されたJSON配列のハイパーパラメータ。",
                "E": "トレーニングをCPUまたはGPUのどちらで実行するかを指定するAmazon EC2インスタンスクラス。",
                "F": "訓練されたモデルがAmazon S3バケットのどこに保存されるかを示す出力パス。"
            },
            "correct_answer": [
                "C",
                "E",
                "F"
            ],
            "explanation": "Amazon SageMakerでトレーニングジョブを実行するためには、いくつかの必須パラメータを含む必要があります。IAMロールはSageMakerがユーザーに代わって作業するために必要で、トレーニングに用いるインスタンスクラスを指定することで計算リソースを決定します。また、出力パスはトレーニング結果を保存する場所を指定するため必須です。選択肢AとBはデータの場所を示すもので、必須ではありません。選択肢Dはハイパーパラメータの設定であり、必須ではありません。",
            "references": [
                "https://docs.aws.amazon.com/ja_jp/sagemaker/latest/dg/ex1-train-model.html"
            ]
        },
        {
            "category": "モデリング",
            "question": "ある機械学習のスペシャリストは現在、あるアプリケーションのために特注の動画推薦モデルを作成しています。このモデルの学習に使用するデータセットは非常に大きく、Amazon S3バケットに保存されている数百万のデータポイントを含みます。スペシャリストは、このデータをすべてAmazon SageMakerのノートブックインスタンスに入れるのは避けたいと考えています。なぜなら、それを行うには時間もかかり、ノートブックインスタンスには割り当てられた35GBのAmazon EBSボリュームを超えてしまうからです。この戦略が、スペシャリストが利用可能なすべてのデータを使用してモデルを訓練することを可能にしますか？",
            "options": {
                "A": "データの小さいサブセットをSageMakerノートブックにロードし、ローカルでトレーニングを行う。トレーニングコードが実行され、モデルパラメータが妥当であることを確認する。パイプラインモードを使用して、S3バケットから全データセットを使用してSageMakerのトレーニングジョブを開始する。",
                "B": "AWS Deep Learning AMIを使用してAmazon EC2インスタンスを起動し、S3バケットをインスタンスにアタッチする。少量のデータでトレーニングを行い、トレーニングコードとハイパーパラメータを確認する。Amazon SageMakerに戻って、完全なデータセットを使ってトレーニングする。",
                "C": "AWS Glueを使用して、データの小さなサブセットを使用してモデルをトレーニングし、データがAmazon SageMakerと互換性があることを確認する。パイプラインモードを使用して、S3バケットから完全なデータセットを使用してSageMakerのトレーニングジョブを開始する。",
                "D": "データの小さいサブセットをSageMakerノートブックにロードし、ローカルでトレーニングする。トレーニングコードが実行され、モデルのパラメータが妥当であることを確認する。AWS Deep Learning AMIを使用してAmazon EC2インスタンスを起動し、S3バケットをアタッチしてフルデータセットをトレーニングする。"
            },
            "correct_answer": [
                "A"
            ],
            "explanation": "SageMakerには、より高速のパイプモードが実装されています。パイプモードを使用すると、モデルのトレーニング開始前にローカルのAmazon EBSボリュームにデータをダウンロードせずに直接S3バケットからデータをストリーミングできるため、大規模データセットのトレーニング時間を短縮できます。選択肢BはEC2インスタンスを使用するため、SageMakerの利点を活かせません。選択肢CはAWS Glueを使用するため、データ準備に手間がかかります。選択肢DはEC2インスタンスを使用するため、SageMakerの利点を活かせません。",
            "references": [
                "https://aws.amazon.com/jp/blogs/news/accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker/"
            ]
        },
        {
            "category": "モデリング",
            "question": "ある企業では、店頭に置かれた商品の上部を撮影したカメラの写真を分析し、どの商品が持ち去られ、どの商品が残っているかを確認しています。この企業では、何時間もかけてデータのタグ付けを行った結果、10個の別々のものを含む合計1,000枚の手書きラベル付き写真ができあがりました。しかし、トレーニングしたところモデルは適合不足の状態でした。ビジネスの長期的な目標に最も合致する機械学習の手法はどれか？",
            "options": {
                "A": "画像をグレースケールに変換し、モデルを再学習する。",
                "B": "識別可能なアイテムの数を10から2に減らし、モデルを構築し、繰り返し実行する。",
                "C": "各アイテムに異なるラベルを付け、画像を再度撮影し、モデルを構築する。",
                "D": "反転や翻訳などの画像変形を用いて各アイテムの学習データを補強し、モデルを構築して反復する。"
            },
            "correct_answer": [
                "D"
            ],
            "explanation": "データセットが少ない点に対して、データ拡張（Data Augmentation）の手法を用いてトレーニングデータを増加させることが有効な手法です。データ拡張は既存のデータセットから様々なテクニックを用いてデータ量を数倍〜数十倍に拡張する方法です。例えば、反転、輝度変更、回転、平行移動、合成などを行い、全く異なるデータを多く作成することが可能です。選択肢Aはグレースケール変換であり、データ量の増加には寄与しません。選択肢Bはアイテム数を減らすもので、モデルの精度向上には不向きです。選択肢Cは再撮影が必要であり、手間がかかります。",
            "references": [
                "https://www.codexa.net/data_augmentation_python_keras/"
            ]
        },
        {
            "category": "モデリング",
            "question": "機械学習のスペシャリストは、クレジットカード処理会社に勤務し、取引が不正であるかどうかをニアリアルタイムで予測する責任を負っています。このスペシャリストは、ある取引が不正である可能性を提供するモデルを構築しています。このビジネス課題を解決するために、スペシャリストはどのようなフレームを使うべきでしょうか？",
            "options": {
                "A": "ストリーミングの分類",
                "B": "バイナリ分類",
                "C": "マルチカテゴリー分類",
                "D": "回帰型分類"
            },
            "correct_answer": [
                "B"
            ],
            "explanation": "今回の問題のターゲット変数は、不正か不正でないかの2つの選択肢に限定されています。二つのターゲット変数を予測する問題はバイナリ分類です。選択肢Aはストリーミングデータの処理に関するもので、選択肢Cは多クラス分類に関するもので、選択肢Dは回帰に関するもので、いずれもバイナリ分類には適していません。"
        },
        {
            "category": "モデリング",
            "question": "機械学習のスペシャリストは、特注のアルゴリズムをAmazon SageMakerに統合したいと考えています。スペシャリストは、Amazon SageMakerがサポートするDockerコンテナでアルゴリズムを実行します。Amazon SageMakerが適切にトレーニングを開始するために、スペシャリストはどのようにDockerコンテナをバンドルすべきでしょうか？",
            "options": {
                "A": "コンテナ内のbash_profileファイルを修正し、トレーニングプログラムを起動するbashコマンドを追加する",
                "B": "DockerfileのCMD configを使って、トレーニングプログラムをイメージのCMDとして追加する",
                "C": "訓練用のコードをtrainという名前のENTRYPOINTとして設定する",
                "D": "訓練用のコードを/opt/ml/trainディレクトリにコピーする"
            },
            "correct_answer": [
                "C"
            ],
            "explanation": "Amazon SageMakerがトレーニングイメージを実行するためには、DockerfileでENTRYPOINTコマンドを使用します。SageMakerは、イメージ名の後にtrain引数を指定することで、コンテナ内のデフォルトCMDステートメントを上書きします。選択肢Aはbash_profileの修正であり、SageMakerの標準的な方法ではありません。選択肢BはCMDを使用するもので、ENTRYPOINTの方が適切です。選択肢Dはコードの配置場所に関するもので、実行方法には直接関係しません。",
            "references": [
                "https://aws.amazon.com/jp/blogs/news/sagemaker-custom-containers-pattern-inference/"
            ]
        },
        {
            "category": "モデリング",
            "question": "データサイエンティストは、100個の連続した数値特徴量を持つデータセットを使用して、顧客の解約を予測するモデルを開発しています。マーケティング部門は、どの特徴量が解約予測に重要であるかについてのガイダンスを提供していません。マーケティング部門は、モデルを解釈し、重要な特性がモデルの出力に与える直接的な影響を判断したいと考えています。データサイエンティストは、ロジスティック回帰モデルをトレーニングしているときに、トレーニングセットと検証セットの精度に大きな違いがあることに気づきました。\n\nデータサイエンティストは、モデルのパフォーマンスを向上させ、マーケティングチームの要件を満たすために、どのテクニックを使用できますか？（2つ選択）",
            "options": {
                "A": "分類器にL1正則化を追加",
                "B": "データセットに特徴量を追加する",
                "C": "再帰的特徴量削減（Recursive Feature Elimination, RFE）を実行する",
                "D": "t-Distributed stochastic neighbor embedding（t-SNE）を実行する",
                "E": "線形判別分析を実行する"
            },
            "correct_answer": [
                "A",
                "C"
            ],
            "explanation": "今回のケースのモデルは過剰適合を起こしている可能性があります。L1正則化の制約を増やすと、過剰適合を軽減する効果が期待できます。また、RFE（再帰的特徴量削減）は重要度の低い特徴量を削減することでモデルの解釈性を高めることができ、マーケティング部門の要望に応えることができます。選択肢Bは特徴量を追加するもので、過剰適合を悪化させる可能性があります。選択肢Dは次元削減手法であり、解釈性には不向きです。選択肢Eは線形判別分析であり、解釈性の向上には直接関係しません。",
            "references": [
                "https://qiita.com/c60evaporator/items/784f0640004be4eefc51",
                "http://blog.livedoor.jp/chem_ai/archives/41000285.html"
            ]
        },
        {
            "category": "モデリング",
            "question": "ある保険会社が、カメラを使ってドライバーの行動を監視し、気が散っているように見えるときに警告を発する、新しい自動車用ガジェットを開発しています。この会社では、機械学習のスペシャリストが機械学習モデルの学習と評価を行うために、制御された環境で約1万枚のトレーニング写真を作成しました。\n\nモデルの評価中、スペシャリストは、エポック数が増えるにつれ、トレーニングエラーレートが急速に減少し、モデルが見たことのないテスト写真に対して効果的に推論できなくなることを確認しました。\n\nこの状況を改善するために、次のどのアプローチを使用すべきでしょうか？（2つ選択）",
            "options": {
                "A": "モデルに勾配消失を追加する。",
                "B": "学習データにデータ拡張を行う。",
                "C": "ニューラルネットワークのアーキテクチャを複雑にする。",
                "D": "モデルに勾配チェックを使用する。",
                "E": "モデルにL2正則化を加える。"
            },
            "correct_answer": [
                "B",
                "E"
            ],
            "explanation": "モデルが過剰適合を起こしている可能性があるため、データ拡張によって汎化能力を高めると共に、L2正則化を加えることで過剰適合を抑えることができます。これにより、トレーニングデータに対する適応を抑え、テストデータに対するパフォーマンスが改善されます。選択肢Aは勾配消失を追加するもので、過剰適合には関係しません。選択肢Cはアーキテクチャを複雑にするもので、過剰適合を悪化させる可能性があります。選択肢Dは勾配チェックであり、過剰適合には関係しません。"
        },
        {
            "category": "モデリング",
            "question": "Amazon SageMaker用のデータストレージソリューションは、機械学習のスペシャリストによって開発されています。すでにTensorFlowベースのモデルがtrain.pyスクリプトとして開発されており、TFRecordsとして保存された静的なトレーニングデータを利用しています。\n\nAmazon SageMakerにトレーニングデータを供給するどのアプローチが、最も少ない開発時間でビジネスニーズを満たすでしょうか？",
            "options": {
                "A": "Amazon SageMakerのスクリプトモードを使用し、train.pyを変更せずに使用する。トレーニングデータを再フォーマットすることなく、データのローカルパスにAmazon SageMakerのトレーニング呼び出しを指示する。",
                "B": "Amazon SageMakerのスクリプトモードを使用し、train.pyを変更せずに使用する。TFRecordデータをAmazon S3バケットに入れます。Amazon SageMakerのトレーニング呼び出しをトレーニングデータを再フォーマットせずにS3バケットに向けます。",
                "C": "train.pyスクリプトを書き換えて、TFRecordsをprotobufに変換するセクションを追加し、TFRecordsの代わりにprotobufデータを取り込みます。",
                "D": "Amazon SageMakerで受け入れられる形式のデータを準備する。AWS GlueまたはAWS Lambdaを使用してデータを再フォーマットし、Amazon S3バケットに保存する。"
            },
            "correct_answer": [
                "B"
            ],
            "explanation": "TFRecordをSageMakerで扱うための最も効率的な方法を選択する必要があります。既存のTFRecordデータをそのまま使用し、再フォーマットせずにS3バケットから直接読み込むことで、開発時間を短縮しつつ効率的にデータを扱えます。選択肢Aはローカルパスを使用するもので、S3からの直接読み込みには不向きです。選択肢Cはスクリプトの書き換えが必要であり、手間がかかります。選択肢Dはデータの再フォーマットが必要であり、手間がかかります。",
            "references": [
                "https://aws.amazon.com/jp/about-aws/whats-new/2019/01/amazon-sagemaker-batch-transform-now-supports-tfrecord-format/"
            ]
        },
        {
            "category": "モデリング",
            "question": "ある農業会社は、機械学習を応用して、100エーカーの草地で特定の雑草種を識別することに興味を持っています。この会社では現在、ドローンに搭載したカメラを使って、10×10グリッドのフィールドの写真を広く集めています。さらに、広範なサンプルを用意するため、一般的な雑草分類の写真に注釈を付けて構成された、かなりの量の学習データセットを持っています。\n\nこの組織は、特定の種類の雑草とそのフィールド内の位置を識別することができる雑草識別モデルを開発したいと考えています。このモデルが完成したら、Amazon SageMakerのエンドポイントでテストされる予定です。このモデルは、カメラの画像を使ってリアルタイムに推論を行います。\n\n信頼性の高い予測を実現するために、機械学習のスペシャリストはどの戦略を使うべきでしょうか？",
            "options": {
                "A": "画像をRecordIO形式で用意し、Amazon S3にアップロードする。Amazon SageMakerを使用して、画像を様々な雑草クラスに分類するための画像分類アルゴリズムを使用したモデルのトレーニング、テスト、および検証を行います。",
                "B": "Apache Parquet形式で画像を準備し、Amazon S3にアップロードする。Amazon SageMakerを使用して、オブジェクト検出用シングルショットマルチボックス検出器（SSD）アルゴリズムを使用したモデルのトレーニング、テスト、および検証を行います。",
                "C": "RecordIOフォーマットの画像を準備し、Amazon S3にアップロードする。Amazon SageMakerを使用して、物体検出用シングルショットマルチボックス検出器（SSD）アルゴリズムを使用したモデルのトレーニング、テスト、および検証を行う。",
                "D": "Apache Parquetフォーマットで画像を準備し、Amazon S3にアップロードする。Amazon SageMakerを使用して、画像を様々な雑草クラスに分類する画像分類アルゴリズムを使用したモデルのトレーニング、テスト、および検証を行います。"
            },
            "correct_answer": [
                "C"
            ],
            "explanation": "画像データを用いた物体検知モデルのトレーニングおよびホストを行う必要があるため、SSDアルゴリズムを使った物体検知が最適です。RecordIOフォーマットはAmazon SageMakerのアルゴリズムで効率的に動作します。選択肢Aは画像分類アルゴリズムを使用しており、物体検知には不適切です。選択肢BとDはApache Parquet形式を使用していますが、これは画像データの保存には一般的ではなく、SageMakerのアルゴリズムでの効率性が劣ります。"
        },
        {
            "category": "モデリング",
            "question": "ある不動産会社が、過去のデータセットを使って住宅価格を予測できる機械学習モデルを開発したいと考えています。データセットには32の特徴が含まれています。このビジネス要件に最も適したモデルはどれか？",
            "options": {
                "A": "ロジスティック回帰",
                "B": "線形回帰",
                "C": "K-means",
                "D": "主成分分析（PCA）"
            },
            "correct_answer": [
                "B"
            ],
            "explanation": "今回のケースでは、住宅価格を特定するために、回帰問題を解く必要があります。回帰問題は、分類問題とは異なり、具体的な連続値を予測する問題です。K-means（クラスタリング）、PCAは教師なし学習であるため不適切で、ロジスティック回帰は分類問題に適したモデルであるため誤りです。したがって正解は、以下の通りです。\n- 線形回帰\n\n参考：https://docs.aws.amazon.com/zh_tw/machine-learning/latest/dg/regression-model-insights.html"
        },
        {
            "category": "モデリング",
            "question": "機械学習のスペシャリストは、様々な経済変数を用いて将来の雇用率を予測するモデルを開発しています。データを分析しているとき、スペシャリストは入力特徴量の振幅が大きく変化していることに気づきました。スペシャリストは、モデルがより大きなサイズの要因に支配されることを望んでいません。モデルトレーニングのためにデータを準備する際のスペシャリストの役割は何ですか？",
            "options": {
                "A": "分位値ビニングを適用してデータをカテゴリー別のビンに分類し、大きさを分布に置き換えることでデータの関係性を維持する。",
                "B": "直交積変換を適用して、大きさに依存しないフィールドの新しい組み合わせを作成する。",
                "C": "正規化を適用して、各フィールドの平均が0、分散が1になるようにして、有意な大きさを取り除きます。",
                "D": "直交スパース・ビグラム（OSB）変換を適用して、固定サイズのスライディング・ウィンドウを適用し、類似した大きさの新しい特徴を生成する。"
            },
            "correct_answer": [
                "C"
            ],
            "explanation": "問題文にあるように、特徴量の分散が大きなものは、特徴量が小さなものより重み計算の際に重要視されてしまいます。正規化（Normalization）やスケーリングでは、こういった各特徴量の分散を統一するための処理です。正規化は一般的に、平均0、分散1になるように特徴量を用いて変換します。これは別名Zスコア正規化（Z-Score Normalization）とも呼ばれています。選択肢Aはデータのビニングであり、特徴量のスケールを統一する目的には適していません。選択肢BとDは、データの変換方法としては不適切です。"
        },
        {
            "category": "モデリング",
            "question": "州や都市ごとに必要な医療や社会プログラムを把握するために、ある組織が国勢調査のデータを集めます。一人一人が国勢調査票の約500の質問に答えます。どのアルゴリズムの組み合わせが必要なインサイトを提供しますか？（2つ選択）",
            "options": {
                "A": "因数分解マシン（FM）アルゴリズム",
                "B": "潜在的ディリクレ配分（LDA）アルゴリズム",
                "C": "主成分分析（PCA）アルゴリズム",
                "D": "k-meansアルゴリズム",
                "E": "RCF（Random Cut Forest）アルゴリズム"
            },
            "correct_answer": [
                "C",
                "D"
            ],
            "explanation": "国勢調査の質問項目500個を特徴量とみなし、互いに相関関係のある特徴量が存在するなどといったことが考えられるため、次元圧縮が必要になります。また、インサイトを示すための分類は行いたいため、特徴量を元にグルーピングを行うことが求められます。主成分分析（PCA）は、教師なしの機械学習アルゴリズムで、データセット内の次元（特徴の数）を減らしながら、できるだけ多くの情報を保存するものです。k-meansアルゴリズムは、データ内の類似的なグループ化を見つけようとするクラスタリング手法です。選択肢Aは因数分解マシンであり、主にレコメンデーションシステムに使用されます。選択肢Bはトピックモデリングに使用され、選択肢Eは異常検知に使用されるため、いずれもこのケースには不適切です。"
        },
        {
            "category": "モデリング",
            "question": "Amazon SageMakerでは、機械学習のスペシャリストがトレーニング用のデータを準備しています。スペシャリストは、SageMakerの内蔵アルゴリズムの1つを使用してトレーニングを行っています。データセットはCSV形式で保存され、numpy.arrayに変換されていますが、これがトレーニングプロセスを遅くしているようです。SageMakerのトレーニング用にデータを最適化するために、スペシャリストはどのようなアクションを取るべきでしょうか？",
            "options": {
                "A": "SageMakerのバッチ変換機能を使って、学習データをDataFrameに変換する。",
                "B": "AWS Glueを使用して、データをApache Parquet形式に圧縮する。",
                "C": "データセットをRecordIO protobuf形式に変換する。",
                "D": "SageMakerのハイパーパラメータ最適化機能を使って、データを自動的に最適化する。"
            },
            "correct_answer": [
                "C"
            ],
            "explanation": "Amazon SageMakerのアルゴリズムの多くは、最適化されたprotobuf recordIOデータフォーマットをトレーニングに使用した場合に最適に動作します。このフォーマットを使用することで、Pipeモードを利用することができます。Pipeモードでは、トレーニングジョブはAmazon Simple Storage Service（Amazon S3）から直接データをストリーミングします。これにより、トレーニングジョブの開始時間が短縮され、スループットが向上します。選択肢Aはデータの変換に関するもので、トレーニングの最適化には直接関係しません。選択肢Bはデータの圧縮に関するもので、トレーニングの効率化には不適切です。選択肢Dはハイパーパラメータの最適化に関するもので、データフォーマットの最適化には関係ありません。"
        },
        {
            "category": "モデリング",
            "question": "機械学習のスペシャリストが、モデルトレーニング用のデータセットを作成しています。データセットはAmazon S3でホストされており、個人識別情報（PII）が含まれています。この企業はセキュリティを重視するために、以下の要件の遵守をスペシャリストに求めています。\n- VPCからのみアクセス可能であること。\n- 公共のインターネットを経由してはならない。\nこれらの要件はどのように設定をすることで満たすことができますか？",
            "options": {
                "A": "VPCエンドポイントを作成し、与えられたVPCエンドポイントとVPCへのアクセスを制限するバケットアクセスポリシーを適用する。",
                "B": "VPCエンドポイントを作成し、与えられたVPCエンドポイントとAmazon EC2インスタンスからのアクセスを許可するバケットアクセスポリシーを適用する。",
                "C": "VPCエンドポイントを作成し、ネットワークアクセスコントロールリスト（NACL）を使用して、与えられたVPCエンドポイントとAmazon EC2インスタンスの間のトラフィックのみを許可する。",
                "D": "VPCエンドポイントを作成し、セキュリティグループを使用して、指定されたVPCエンドポイントとAmazon EC2インスタンスへのアクセスを制限する。"
            },
            "correct_answer": [
                "A"
            ],
            "explanation": "S3はVPC外のサービスであるため、EC2からVPCエンドポイントを使用せずにS3にアクセスする場合は必然的にインターネットを経由してしまいます。VPCエンドポイントを用いることで、閉域網内でアクセス経路を確立することができます。Amazon S3 のVPCエンドポイントは、Amazon S3 への接続のみを許可する VPC 内の論理エンティティです。また、Amazon S3 のバケットポリシーを使用して、特定の Virtual Private Cloud（VPC）エンドポイントまたは特定の VPC からのバケットへのアクセスを管理できます。選択肢BはEC2インスタンスからのアクセスを許可するもので、VPC全体の制限には不十分です。選択肢CとDはネットワーク制御に関するもので、バケットポリシーの設定には直接関係しません。"
        },
        {
            "category": "モデリング",
            "question": "ワークステーションの学習スペシャリストが、ローカルマシンでscikit-learnを使ってロジスティック回帰モデルを学習した後、推論のみを目的として本番環境に導入したいと考えています。ローカルでトレーニングしたAmazon SageMakerモデルをホストできることを保証するために、どのようなアクションを行うべきでしょうか？",
            "options": {
                "A": "推論コードでDockerイメージを構築する。レジストリのホスト名でDockerイメージをタグ付けし、Amazon ECRにアップロードする。",
                "B": "学習されたモデルをシリアル化して、デプロイ用にフォーマットを圧縮する。レジストリのホスト名でDockerイメージをタグ付けし、Amazon S3にアップロードする。",
                "C": "訓練されたモデルをシリアル化し、デプロイ用にフォーマットを圧縮する。イメージをビルドして、Docker Hubにアップロードする。",
                "D": "推論コードでDockerイメージを構築する。Docker Hubを構成し、イメージをAmazon ECRにアップロードする。"
            },
            "correct_answer": [
                "A"
            ],
            "explanation": "SageMakerでトレーニングをしていないモデルを、SageMakerでホストする方法は推論コードのDockerイメージを構築することで実現できます。SageMakerはDockerコンテナを多用し、ユーザーがアルゴリズムを学習しデプロイできるようにしています。コンテナ化により、開発者とデータ科学者は、ソフトウェアを標準化されたユニットにパッケージ化し、Dockerをサポートするあらゆるプラットフォームで一貫して実行することができます。コンテナ化により、コード、ランタイム、システムツール、システムライブラリ、設定のすべてが同じ場所にパッケージされ、周囲から隔離され、どこで実行されても一貫したランタイムを保証します。選択肢BはS3にアップロードするもので、SageMakerのホスティングには不適切です。選択肢CとDはDocker Hubを使用しており、SageMakerのECRを活用するには不適切です。",
            "references": [
                "https://aws.amazon.com/jp/blogs/news/sagemaker-custom-containers-pattern-inference/"
            ]
        },
        {
            "category": "モデリング",
            "question": "自動車のエンジンを製造している会社では、走行中の車両からデータを収集しています。タイムスタンプ、エンジンの温度、毎分の回転数、その他のセンサーの測定値をすべて取得しています。この企業では、エンジンが故障する時期を予測し、ドライバーに事前に警告して修理を依頼することを目指しています。「トレーニングのために、エンジンのデータはデータレイクに置かれます。どの予測モデルが本番環境に最も適しているでしょうか？",
            "options": {
                "A": "将来のどの時期にどのようなエンジンの故障が発生するかを示すラベルを時間経過とともに追加し、教師あり学習問題にする。リカレント・ニューラル・ネットワーク（RNN）を使用して、エンジンが特定の故障でメンテナンスを必要とする時期を認識するモデルを学習する。",
                "B": "このデータは、教師なし学習アルゴリズムが必要です。Amazon SageMakerのk-meansを使用してデータをクラスタリングする。",
                "C": "将来のどの時期にどのようなエンジンの故障が発生するかを示すラベルを時間経過とともに追加し、教師あり学習問題にする。畳み込みニューラルネットワーク（CNN）を使用してモデルを学習し、エンジンが特定の故障のためにメンテナンスを必要とする時期を認識する。",
                "D": "このデータはすでに時系列として定式化されています。Amazon SageMakerのseq2seqを使用して時系列をモデル化する。"
            },
            "correct_answer": [
                "A"
            ],
            "explanation": "エンジンの故障という事象を予測する必要があるため、時系列データに関する教師あり学習問題を解くためのモデル選定が必要です。RNN（Recurrent neural network）は内部に循環構造を持つニューラルネットワークの総称です。RNNは数値の時系列データのようなシーケンスデータのパターンを認識するように設計されたニューラルネットワークのモデルです。例えば、多層パーセプトロン（MLP）やCNNなどの標準のニューラルネットワークモデルでは、入力シンボルの順序を処理することはできません。しかし、RNNは過去の情報を記憶しておき、その情報にしたがって新しい事象を処理することができます。選択肢Bは教師なし学習であり、時系列予測には不適切です。選択肢CはCNNを使用しており、時系列データの処理には不向きです。選択肢Dはseq2seqを使用していますが、RNNの方が時系列データの予測には適しています。",
            "references": [
                "https://aws.amazon.com/jp/blogs/news/now-available-in-amazon-sagemaker-deepar-algorithm-for-more-accurate-time-series-forecasting/",
                "https://qiita.com/kazukiii/items/df809d6cd5d7d1f57be3",
                "https://jp.mathworks.com/discovery/rnn.html"
            ]
        },
        {
            "category": "モデリング",
            "question": "次のような映画分類モデルの混同行列が与えられた場合、「ロマンス」の実際のクラス頻度と「アドベンチャー」の予想クラス頻度はどのようになるでしょうか？",
            "options": {
                "A": "ロマンスの真のクラス頻度は77.56%で、アドベンチャーの予測クラス頻度は20.85%である",
                "B": "ロマンスの真のクラス頻度は57.92%で、アドベンチャーの予測クラス頻度は13.12%である",
                "C": "ロマンスのクラス頻度は0.78で、アドベンチャーの予測クラス頻度は（0.47 - 0.32）である",
                "D": "ロマンスの真のクラス頻度は77.56%*0.78で、アドベンチャーの予測されるクラス頻度は20.85%*0.32である"
            },
            "correct_answer": [
                "B"
            ],
            "explanation": "評価データセットにおける真のクラス頻度の評価：最後から2番目の列には、評価データセットで57.92%の評価データがロマンス、21.23%がサスペンス、20.85%がアドベンチャーであることを示しています。評価データの予測クラス頻度の評価：最後の列は各クラスの予測の頻度を示しています。観測値の77.56%はロマンスとして予測され、9.33%はサスペンスとして予測され、13.12%はアドベンチャーとして予測されています。選択肢Aは予測頻度が誤っており、選択肢CとDは計算が不正確です。したがって正解は以下の通りです：ロマンスの真のクラス頻度は57.92%で、アドベンチャーの予測クラス頻度は13.12%である。"
        },
        {
            "category": "モデリング",
            "question": "ある大手モバイルネットワーク事業者は、どの消費者がサービス契約を解約する可能性が高いかを予測する機械学習アルゴリズムを開発しています。この企業は、解約のコストがインセンティブのコストよりもはるかに大きいため、これらの顧客を維持するためのインセンティブを与えようと考えています。100人の消費者からなるテストデータセットでテストした結果、このモデルは次のような混同行列を生成します。モデルの評価結果に基づいて、なぜこれが本番環境に導入可能なモデルなのでしょうか？",
            "options": {
                "A": "モデルの精度は89%で、偽陰性の影響を受けた全体の支出は、偽陽性よりも大きい",
                "B": "モデルの精度は71%で、精度に比べて遅れている",
                "C": "モデルの精度は89%で、偽陰性の影響を受けた全体の支出は、偽陽性よりも少ない",
                "D": "モデルの精度は95%で、精度を大きく上回っている"
            },
            "correct_answer": [
                "C"
            ],
            "explanation": "混同行列を元にモデルの評価を行い、精度は(8 + 81) / 100 = 0.89、すなわち89%です。このビジネス問題では、解約の可能性が高い顧客にインセンティブを提供することで解約を防ぐことが主目的となっています。偽陰性（解約する可能性があるが、予測で解約しないとされたケース）のコストは偽陽性よりも低いため、モデルは本番環境に適しています。",
            "references": [
                "https://qiita.com/TsutomuNakamura/items/a1a6a02cb9bb0dcbb37f%E2%80%8B"
            ]
        },
        {
            "category": "モデリング",
            "question": "機械学習のスペシャリストは、Amazon SageMakerを使用してモデルをトレーニングするために、特注のResNetモデルをDockerコンテナに入れています。このスペシャリストは、Amazon EC2 P3インスタンスでモデルをトレーニングしており、NVIDIA GPUを利用するためにDockerコンテナを効果的にセットアップしたいと考えています。スペシャリストがするべきことは何ですか？",
            "options": {
                "A": "NVIDIAドライバをDockerイメージにバンドルする。",
                "B": "NVIDIA-Docker互換になるようにDockerコンテナをビルドする。",
                "C": "GPUインスタンスで実行するために、Dockerコンテナのファイル構造を整理する。",
                "D": "Amazon SageMaker CreateTrainingJobリクエストボディにGPUフラグを設定する。"
            },
            "correct_answer": [
                "B"
            ],
            "explanation": "NVIDIA GPUを利用できるようにコンテナをセッティングする必要があります。NVIDIA Container Toolkitは、GPUで高速化されたDockerコンテナの構築と実行を可能にします。Amazon SageMakerでは、モデルのトレーニングにGPUデバイスを使用する予定の場合、コンテナがnvidia-dockerに対応していることを確認してください。選択肢Aはドライバのバンドルに関するもので、nvidia-dockerの互換性を確保するには不十分です。選択肢Cはファイル構造の整理に関するもので、GPUの利用には直接関係しません。選択肢DはGPUフラグの設定に関するもので、コンテナの互換性には直接関係しません。",
            "references": [
                "https://docs.aws.amazon.com/ja_jp/sagemaker/latest/dg/adapt-training-container.html"
            ]
        },
        {
            "category": "モデリング",
            "question": "機械学習のスペシャリストが、Amazon SageMakerを使用した決定木ベースのアンサンブルモデルのハイパーパラメータチューニングプロジェクトを開始し、評価指標としてAUC（Area Under the Curve）を設定します。この手法は最終的に、24時間ごとに古いデータのクリックスルーをモデル化するために、毎晩ハイパーパラメータを再学習・最適化するパイプラインに統合されます。スペシャリストは、これらのモデルの学習に必要な時間を短縮し、最終的にはコストを節約するために、入力ハイパーパラメータの範囲を調整したいと考えています。この目標を達成するためには、どのような可視化が必要でしょうか？",
            "options": {
                "A": "最も重要な入力変数がガウシアンであるかどうかを示すヒストグラム。",
                "B": "t-SNE（t-Distributed Stochastic Neighbor Embedding）を用いて、多数の入力変数を読みやすい次元で可視化した、対象変数ごとに色分けされたポイントを持つ散布図。",
                "C": "各学習反復における評価指標の性能を示す散布図。",
                "D": "最大木の深さと評価指標の相関関係を示す散布図。"
            },
            "correct_answer": [
                "D"
            ],
            "explanation": "問題設定がやや難解であるため、要件を再解釈すると、要件としては「ハイパーパラメータの調整範囲を最適化したい」ということになります。最大木の深さは評価指標に影響を及ぼすハイパーパラメータの一つです。したがって正解は、「最大木の深さと評価指標の相関関係を示す散布図」です。選択肢Aは変数の分布に関するもので、ハイパーパラメータの調整には直接関係しません。選択肢Bは次元削減に関するもので、ハイパーパラメータの調整には不適切です。選択肢Cは学習反復の性能に関するもので、ハイパーパラメータの調整には直接関係しません。",
            "references": [
                "https://docs.aws.amazon.com/ja_jp/sagemaker/latest/dg/automatic-model-tuning-considerations.html"
            ]
        },
        {
            "category": "モデリング",
            "question": "あるクレジットカード会社は、新規のクレジットカード申請者がクレジットカードの支払いに失敗するかどうかを判断するのに役立つクレジットスコアモデルを開発したいと考えていました。この企業は、さまざまなソースからデータを収集し、何百もの生の特徴を抽出しました。分類モデルの学習に関する初期の研究では、多くの特徴が密接に関連していること、膨大な数の特徴が学習速度を大幅に低下させていること、オーバーフィッティングの影響があることが明らかになりました。このプロジェクトに携わるデータサイエンティストは、元のデータセットから得られる多くの情報を犠牲にすることなく、モデルのトレーニングを高速化したいと考えています。この目的を達成するために、データサイエンティストはどのような特徴量エンジニアリングのアプローチを用いるべきでしょうか？",
            "options": {
                "A": "すべての特徴量に対して自己相関を行い、相関の高い特徴量を削除する。",
                "B": "すべての数値を0から1の間になるように正規化する。",
                "C": "オートエンコーダーまたは主成分分析（PCA）を使用して、元の特徴を新しい特徴で置き換える。",
                "D": "k-meansを用いて生データをクラスタリングし、各クラスタのサンプルデータを用いて新しいデータセットを構築する。"
            },
            "correct_answer": [
                "C"
            ],
            "explanation": "数百の特徴量に対して次元圧縮を行う必要があります。主成分分析（Principal Component Analysis: PCA）は、教師なしの機械学習アルゴリズムで、データセット内の次元（特徴の数）を減らしながらも、できるだけ多くの情報を保持しようとするものです。特に、特性間に高い相関がある場合や、オーバーフィッティングの影響が出ている場合などで使用されます。"
        },
        {
            "category": "モデリング",
            "question": "ある商品カタログの編集長は、研究開発チームに、一連の写真に写っている人が自社の小売ブランドを身につけているかどうかを判断できる機械学習システムを作成するよう依頼しました。研究開発チームには、学習データに使用できる画像コレクションがあります。研究者はどの機械学習モデルを利用するべきでしょうか？",
            "options": {
                "A": "潜在的ディリクレ配分（LDA）",
                "B": "リカレントニューラルネットワーク（RNN）",
                "C": "K-means",
                "D": "畳み込みニューラルネットワーク（CNN）"
            },
            "correct_answer": [
                "D"
            ],
            "explanation": "画像データセットに対する適切なモデルを選択する必要があります。畳み込みニューラルネットワーク（CNN）は画像認識に適した手法で、ニューラルネットワークに「畳み込み」という操作を導入したものです。ディープラーニングの研究の中で最も進められている画像認識、物体検出、領域推定などの画像分野で活用されています。したがって正解は「畳み込みニューラルネットワーク（CNN）」です。",
            "references": [
                "https://atmarkit.itmedia.co.jp/ait/articles/1804/23/news138.html",
                "https://jp.mathworks.com/discovery/convolutional-neural-network.html"
            ]
        },
        {
            "category": "モデリング",
            "question": "データサイエンティストは、Amazon SageMakerに内蔵されているseq2seqメソッドを使用して、500,000のフレーズペアを組み合わせて、英語から日本語への機械学習翻訳モデルを作成しました。データサイエンティストは、サンプルセンテンスでテストしているときに、5単語の例では翻訳品質が許容できることを発見します。しかし、文の長さが100ワードを超えると、品質は満足のいかないレベルにまで低下します。この問題を解決するためには、どのような対策が必要でしょうか？",
            "options": {
                "A": "n-gramを使うように前処理を変更。",
                "B": "リカレントニューラルネットワーク（RNN）に、最大の文の単語数よりも多くのノードを追加する。",
                "C": "アテンションメカニズムに関連するハイパーパラメータを調整する。",
                "D": "異なるウェイト初期化タイプを選択する。"
            },
            "correct_answer": [
                "C"
            ],
            "explanation": "今回の問題は、seq2seqモデル固有の問題であるため、それを根本的に克服するための調整が必要です。seq2seqなどのエンコーダーデコーダーフレームワークの欠点は、エンコードされた固定長の特徴ベクトルに含めることのできる情報の量には制限があるため、ソースシーケンスの長さが増えるたびにモデルのパフォーマンスが減少する点です。この問題に取り組むために、Bahdanauらは2015年にアテンションメカニズムを提案しました。アテンション機構では、デコーダーが最も重要な情報が存在する可能性があるエンコーダーシーケンス内の場所の検索を試行し、その情報と以前にデコードした単語を使用して、シーケンス内の次のトークンを予測します。選択肢Aは前処理の変更に関するもので、seq2seqの根本的な問題には対処できません。選択肢BはRNNのノード数に関するもので、アテンションメカニズムの導入ほど効果的ではありません。選択肢Dはウェイト初期化に関するもので、seq2seqの問題には直接関係しません。従って正解は、以下の通りです。\n\n- アテンションメカニズムに関連するハイパーパラメータを調整する。",
            "references": [
                "https://docs.aws.amazon.com/ja_jp/sagemaker/latest/dg/seq-2-seq-hyperparameters.html",
                "https://youtu.be/CcL_t5ZJ9fQ?si=Yq0C89F0CGrQrq_2"
            ]
        },
        {
            "category": "モデリング",
            "question": "あるヘルスケア企業が、ニューラルネットワークを使ってX線写真を正常なものと病的なものに分類しようとしています。ラベル付けされたデータは、1,000枚のトレーニングセットと200枚のテストセットに分けられました。500隠れ層を持つニューラルネットワークモデルを最初にトレーニングしたところ、トレーニングセットでは99%の精度が得られましたが、テストセットでは55%の精度しか得られませんでした。この状況を解決するために、スペシャリストはどのような修正を検討すべきでしょうか？（3つ選択）",
            "options": {
                "A": "レイヤーの数が多いモデルを選ぶ。",
                "B": "レイヤーの数が少ないモデルを選ぶ。",
                "C": "学習率を小さくする。",
                "D": "ドロップアウトを有効にする。",
                "E": "テストセットのすべての画像をトレーニングセットに含める。",
                "F": "早期停止を有効にする。"
            },
            "correct_answer": [
                "B",
                "D",
                "F"
            ],
            "explanation": "機械学習モデルを正しく評価するためには、学習データと異なるデータを用いたテストでの性能を評価する必要があります。テストでの性能の低下は過剰適合が原因と考えられることが多いため、この問題の解決策として以下が推奨されます：\n\n- レイヤーの数が少ないモデルを選ぶ：複雑なモデルから単純なモデルへと調整。\n- ドロップアウトを有効にする：過剰適合の抑制に役立ちます。\n- 早期停止を有効にする：過剰適合を防ぎ、モデルの汎化性能を向上させます。選択肢Aはモデルの複雑さを増すため、過剰適合を悪化させる可能性があります。選択肢Cは学習率の調整に関するもので、過剰適合の抑制には直接関係しません。選択肢Eはテストセットのデータをトレーニングに含めるもので、データの一貫性を保つためには不適切です。"
        },
        {
            "category": "モデリング",
            "question": "あるコールセンター企業では、カスタマーサービスの質を高めるために、電話応対者の音声記録から意味のある洞察を得たいと考えています。同社が受ける電話の大半は英語を話す人からのものですが、スペイン語やフィリピン語など英語以外の言語を話す人にも対応したいと考えています。また、分析レポートは英語で出力したいと考えています。機械学習モデルを維持することなく、このソリューションを導入するにはどうすればよいでしょうか。",
            "options": {
                "A": "Amazon Transcribeを使用してすべての音声記録をテキストに変換し、Amazon Translateを使用して英語以外のテキストを英語に翻訳する分析にはAmazon Textractを使用する。",
                "B": "Amazon Transcribeを使用してすべてのオーディオ録音をテキストに変換し、Amazon Translateを使用して英語以外のテキストを英語に翻訳する分析にはAmazon SageMaker BlazingTextを使用する。",
                "C": "Amazon Transcribeを使用してすべての音声記録をテキストに変換し、Amazon Translate を使用して英語以外のテキストを英語に翻訳するAmazon SageMaker Neural Topic Model を使用して分析する。",
                "D": "Amazon Transcribeを使用してすべてのオーディオ録音をテキストに変換し、Amazon Translateを使用して英語以外のテキストを英語に翻訳する分析にはAmazon Comprehendを使用する。"
            },
            "correct_answer": [
                "D"
            ],
            "explanation": "Amazon Transcribeは音声をテキストに変換するためのAWSサービスであり、電話対応者の会話を文字起こしすることができます。次に、Amazon Translateを使用して、英語以外のテキストを英語に翻訳します。最終的に、Amazon Comprehendを使用して、顧客対応のテキストから意味のあるインサイトを得ることができます。これらのサービスは完全に管理された状態で提供されており、独自のモデルをトレーニングする必要がありません。選択肢AはTextractを使用しており、テキスト分析には不適切です。選択肢BはBlazingTextを使用しており、音声データの分析には不適切です。選択肢CはNeural Topic Modelを使用しており、音声データの分析には不適切です。"
        },
        {
            "category": "モデリング",
            "question": "ある企業の機械学習のスペシャリストは、TensorFlowベースの時系列予測モデルのトレーニングペースを上げたいと考えています。現在、トレーニングは1台のGPUを使って行われており、完了までに約23時間かかっています。毎日トレーニングを行わなければなりません。現状、モデルの精度には満足していますが、事業者はトレーニングデータの量が今後も増え続け、モデルの更新は毎日ではなく1時間ごとに行う必要があると考えています。さらに、組織はコーディングの労力とインフラの修正を減らしたいと考えています。機械学習のスペシャリストは、トレーニングソリューションを将来的に拡張するために、どのような修正を加えるべきでしょうか？",
            "options": {
                "A": "TensorFlowのコードは変更しません。マシンをより高性能なGPUを搭載したものに変更し、トレーニングを高速化する。",
                "B": "TensorFlowのコードを変更して、Amazon SageMakerでサポートされるHorovod分散フレームワークを実装する。ビジネス目標を達成するために必要な数のマシンにトレーニングを並列化する。",
                "C": "AWS SageMakerの内蔵DeepARモデルの使用に変更する。ビジネスの目標を達成するために必要な数のマシンにトレーニングを並列化する。",
                "D": "トレーニングをAmazon EMRに移行し、ビジネス目標を達成するために必要な数のマシンにワークロードを分散する。"
            },
            "correct_answer": [
                "B"
            ],
            "explanation": "現状、23時間かかるトレーニングの効率をさらに改善するためには、分散学習の仕組みを導入して複数のマシンにワークロードを分散することが有効です。Amazon SageMakerでHorovodを使用することで、TensorFlowの分散トレーニングが可能になり、複雑なクラスター設定が不要で、インフラ管理も簡素化できます。そのため、正解は「TensorFlowのコードを変更して、Amazon SageMakerでサポートされるHorovod分散フレームワークを実装する。」となります。選択肢Aは単一のGPUの性能向上に依存しており、スケーラビリティが限られています。選択肢Cはモデルの変更に関するもので、既存のモデルの精度を維持するには不適切です。選択肢DはEMRの使用に関するもので、SageMakerの特定の機能を活用するには不適切です。",
            "references": [
                "https://aws.amazon.com/jp/blogs/news/launching-tensorflow-distributed-training-easily-with-horovod-or-parameter-servers-in-amazon-sagemaker/"
            ]
        },
        {
            "category": "モデリング",
            "question": "機械学習のスペシャリストは、Amazon SageMakerを使用して、多数のデータサイエンティストによるノートブックへの同時アクセス、モデルのトレーニング、およびエンドポイントの展開を実現しています。最適な運用パフォーマンスを保証するために、スペシャリストは、サイエンティストがモデルをデプロイする頻度、デプロイされたSageMakerのエンドポイントのGPUおよびCPU使用率、およびエンドポイントが呼び出された際に発生する問題を監視できなければなりません。このデータを追跡する目的で、Amazon SageMakerとリンクしているサービスはどれですか？（2つ選択してください。）",
            "options": {
                "A": "AWS CloudTrail",
                "B": "AWS Health",
                "C": "AWS Trusted Advisor",
                "D": "Amazon CloudWatch",
                "E": "AWS Config"
            },
            "correct_answer": [
                "A",
                "D"
            ],
            "explanation": "Amazon SageMakerのコンピューティングリソースのメトリクス監視と、デプロイ等のアクション監視には、CloudWatchとCloudTrailが最適です。CloudWatchはCPUやGPU使用率のモニタリングに使用され、CloudTrailはSageMaker APIのアクティビティを監視して、アカウント情報を記録します。これにより、最適なパフォーマンスを維持するための監視が可能になります。選択肢BはAWS Healthに関するもので、SageMakerの監視には直接関係しません。選択肢CはTrusted Advisorに関するもので、SageMakerの監視には直接関係しません。選択肢EはConfigに関するもので、SageMakerの監視には直接関係しません。"
        },
        {
            "category": "モデリング",
            "question": "ある物流会社では、10カ所の倉庫に保管されている1つの商品について、翌月の在庫必要量を予測する予測モデルが必要です。機械学習のスペシャリストは、Amazon Forecastを使い、3年分の月次データを使って予測モデルを作成します。データギャップはありません。スペシャリストは、予測器のトレーニングに使用するアルゴリズムを自動選択しましたが、予測器の平均絶対誤差（MAPE）は、現在使用されているデータサイエンティストが生成するMAPEよりもはるかに大きい状態です。MAPEの増加につながる可能性のあるCreatePredictor APIコールをどのように修正するべきですか？（2つ選択）",
            "options": {
                "A": "PerformAutoMLをtrueに設定する。",
                "B": "ForecastHorizonを4に設定する。",
                "C": "ForecastFrequencyをW（ウィークリー）に設定する。",
                "D": "PerformHPOをtrueに設定する。",
                "E": "FeaturizationMethodNameをfillingに設定する。"
            },
            "correct_answer": [
                "A",
                "D"
            ],
            "explanation": "PerformAutoMLをtrueに設定することで、最適なアルゴリズムが自動的に選択され、MAPEの改善が期待できます。また、PerformHPOをtrueに設定することで、ハイパーパラメータの最適化が行われ、予測精度が向上する可能性があります。これらの設定により、予測モデルのパフォーマンスを向上させることができます。選択肢Bは予測の範囲に関するもので、MAPEの改善には直接関係しません。選択肢Cは予測の頻度に関するもので、MAPEの改善には直接関係しません。選択肢Eは特徴量の充填に関するもので、MAPEの改善には直接関係しません。",
            "references": [
                "https://docs.aws.amazon.com/ja_jp/forecast/latest/dg/API_CreatePredictor.html"
            ]
        },
        {
            "category": "モデリング",
            "question": "機械学習のスペシャリストは、現在、写真に写っている自動車のメーカーとモデルを認識するモデルを開発しています。機械学習のスペシャリストは、一般的なものの写真に対して、既に訓練されたモデルを使用したいと考えています。スペシャリストは、複数の自動車ブランドとモデルの画像を集めた大規模なオーダーメイドのコレクションを編集しました。カスタムデータでモデルを再学習するために、スペシャリストは何をすべきでしょうか？",
            "options": {
                "A": "最後の完全連結層を含むすべての層にランダムな重みをつけてモデルを初期化する。",
                "B": "すべての層で事前に学習された重みでモデルを初期化し、最後の完全連結層を置き換える。",
                "C": "すべての層にランダムな重みをつけてモデルを初期化し、最後の完全連結層を置き換える。",
                "D": "最後の完全連結層を含むすべての層で事前学習された重みでモデルを初期化。"
            },
            "correct_answer": [
                "B"
            ],
            "explanation": "転移学習を用いる場合、既に学習済みの重みを活用することが最適です。一般的には、すべての層を事前学習された重みで初期化し、最後の完全連結層のみを再学習のために置き換えることで、モデルを特定のタスクに適応させることができます。このアプローチにより、少量のデータでも精度の高いモデルが得られます。選択肢Aはすべての層をランダムに初期化するもので、事前学習の利点を活用できません。選択肢Cはすべての層をランダムに初期化するもので、事前学習の利点を活用できません。選択肢Dはすべての層を事前学習された重みで初期化するもので、最後の層を置き換える必要があります。"
        },
        {
            "category": "モデリング",
            "question": "あるオンライン小売業者は、自社のWebアプリケーションに新しいクラウドベースの商品提案ツールを提供したいと考えています。データローカリゼーションの必要性から、センシティブなデータはオンプレミスに残し、商品提案モデルはセンシティブではないデータを使ってトレーニングと評価を行う必要があります。クラウドへのデータ転送にはIPsecが必要です。Webアプリケーションはオンプレミスでホストされており、すべてのデータはPostgreSQLデータベースに保存されています。毎日、モデルの再トレーニングのために、データをAmazon S3に安全に送信する必要があります。機械学習のスペシャリストは、これらの要件をどのように実現すべきでしょうか？",
            "options": {
                "A": "AWS Glueジョブを作成して、PostgreSQL DBインスタンスに接続する。AWS Site-to-Site VPN接続を介して機密データを含まないテーブルをAmazon S3に直接インジェストする。",
                "B": "AWS Glueジョブを作成して、PostgreSQL DBインスタンスに接続する。PySparkジョブを使用して機密データを削除しながら、AWS Site-to-Site VPN接続を介してすべてのデータをAmazon S3に取り込みます。",
                "C": "AWS Database Migration Service（AWS DMS）をテーブルマッピングで使用し、SSL接続で機密データのないPostgreSQLテーブルを選択する。データを直接Amazon S3に複製する。",
                "D": "PostgreSQLの論理レプリケーションを使用して、VPN接続のAWS Direct Connectを介してAmazon EC2のPostgreSQLにすべてのデータをレプリケートする。AWS Glueを使用して、Amazon EC2からAmazon S3にデータを移動する。"
            },
            "correct_answer": [
                "A"
            ],
            "explanation": "AWS Glueジョブを利用して、PostgreSQL DBに接続し、VPN接続を介してセキュアにAmazon S3にデータを送信することが可能です。このアプローチにより、オンプレミスのデータベースから必要なデータのみを抽出し、機密情報を扱わずにクラウドへデータ転送を実現できます。選択肢Bはすべてのデータを取り込むもので、機密データの除外が不十分です。選択肢CはDMSを使用していますが、Glueの方が柔軟性があります。選択肢Dはデータのレプリケーションに関するもので、データ転送の効率性が劣ります。"
        },
        {
            "category": "モデリング",
            "question": "ある機械学習のスペシャリストが、10種類の動物を分類する目的で、畳み込みニューラルネットワーク（CNN）を開発しています。スペシャリストは、動物の写真を入力とし、次々と畳み込み層とプーリング層を経て、10個のノードを持つ全結合層を通過させるニューラルネットワークの層のシーケンスを構築しました。スペシャリストは、ニューラルネットワークから、入力された写真が10種類の分類のそれぞれに属する可能性を示す確率分布を表す出力を得たいと考えています。どの関数が望ましい結果を返すでしょうか？",
            "options": {
                "A": "ドロップアウト",
                "B": "スムーズなL1ロス",
                "C": "ソフトマックス",
                "D": "整流型リニアユニット（ReLU）"
            },
            "correct_answer": [
                "C"
            ],
            "explanation": "ソフトマックス関数は、ニューラルネットワークの出力層で確率分布を生成するために使用されます。この関数は、入力値を0から1の範囲に正規化し、出力の合計が1となるように変換します。これにより、分類タスクにおいて各クラスに対する確率を示す出力を得ることができます。選択肢Aのドロップアウトは過学習を防ぐための手法であり、確率分布を生成するためのものではありません。選択肢BのスムーズなL1ロスは損失関数であり、分類の確率分布を生成するためのものではありません。選択肢DのReLUは活性化関数であり、確率分布を生成するためのものではありません。",
            "references": [
                "https://gensasaki.hatenablog.com/entry/2018/08/30/042807"
            ]
        },
        {
            "category": "モデリング",
            "question": "機械学習スペシャリストは、Amazon SageMakerに組み込まれた画像分類アルゴリズムを使用してモデルをトレーニングしています。モデルはトレーニングデータ上でうまく一般化しないので、スペシャリストはInceptionニューラルネットワークを使ってみたいと考えています。スペシャリストがこれを行うには、どのアクションが必要ですか？（2つの選択）",
            "options": {
                "A": "TensorFlow Estimatorを使用してInceptionネットワークを構築し、Dockerイメージとしてバンドルするそのイメージを使ってモデルをトレーニングする",
                "B": "組み込みアルゴリズムをInceptionニューラルネットワークに変更するよう、AWSサポートにリクエストを送信する",
                "C": "Amazon EC2インスタンスを起動し、Inceptionネットワークのコードをインストールする。Amazon SageMakerを使用して、このインスタンスからJupyterノートブックを作成する",
                "D": "Amazon SageMakerで事前に構築されたTensorFlowコンテナイメージを拡張してInceptionネットワークのコードを記述し、これを使用してモデルをトレーニングする",
                "E": "組み込みアルゴリズムを編集して、インセプションネットワークを使用する"
            },
            "correct_answer": [
                "A",
                "D"
            ],
            "explanation": "Amazon SageMakerでカスタムアルゴリズムを使用するには、TensorFlow Estimatorを用いてDockerイメージを作成するか、事前に構築されたTensorFlowコンテナを拡張してInceptionネットワークのコードを利用する方法が有効です。このアプローチにより、柔軟にInceptionモデルをトレーニングできます。選択肢BはAWSサポートにリクエストを送信するもので、直接的な解決策ではありません。選択肢CはEC2インスタンスを使用するもので、SageMakerの利点を活かしていません。選択肢Eは組み込みアルゴリズムを編集するもので、Inceptionネットワークを直接使用する方法ではありません。"
        },
        {
            "category": "モデリング",
            "question": "データサイエンティストは、多クラスのデータセットで多層パーセプトロン（MLP）アルゴリズムを学習しています。対象となるクラスは、データセットの他のクラスとは区別されていますが、十分なリコールスコアが得られていません。データサイエンティストは、MLPの隠れ層の数とサイズを変えて実験しましたが、結果はあまり改善されませんでした。早急にリコール対策を講じなければなりません。これらの要求を満たすには、どの戦略が適切でしょうか？",
            "options": {
                "A": "Amazon Mechanical Turkを使ってより多くのデータを収集してから再学習する",
                "B": "MLPではなく、異常検知モデルを学習する",
                "C": "MLPではなく、XGBoostモデルを学習する",
                "D": "MLPの損失関数にクラスの重みを追加して再学習する"
            },
            "correct_answer": [
                "D"
            ],
            "explanation": "MLPの損失関数にクラスの重みを追加することで、リコールを改善できます。この方法は、特にクラスの不均衡が問題となる場合に有効です。クラスの重みを適切に設定することで、モデルは少数派クラスの識別を重視するようになります。選択肢Aはデータ収集に時間がかかるため、早急な対策には不向きです。選択肢Bの異常検知モデルは、リコール改善の直接的な解決策ではありません。選択肢CのXGBoostモデルは、MLPの問題を解決するための直接的な方法ではありません。"
        },
        {
            "category": "モデリング",
            "question": "機械学習のスペシャリストが、線形回帰モデルを構築しようとしています。与えられた残差プロットを考えると、モデルが失敗する最も可能性の高い原因は何でしょうか？",
            "options": {
                "A": "線形回帰は適切ではない。残差は一定の分散を持っていません。",
                "B": "線形回帰は不適当です。基礎データに外れ値がある。",
                "C": "線形回帰は適切です。残差の平均値はゼロです。",
                "D": "線形回帰が適切です。残差は一定の分散を持つ。"
            },
            "correct_answer": [
                "A"
            ],
            "explanation": "残差プロットは、回帰の適合性を確認するための重要な指標です。線形回帰の仮定には、残差が一定の分散を持つことが含まれますが、このプロットからはその仮定が満たされていないことがわかります。そのため、線形回帰モデルは適していない可能性が高いです。選択肢Bの外れ値の存在は、必ずしも線形回帰が不適当である理由にはなりません。選択肢CとDは、残差の分散に関する誤った仮定をしています。"
        },
        {
            "category": "モデリング",
            "question": "機械学習のスペシャリストは、不正検知チームに配属され、XGBoostモデルがテストデータで正しく動作するようにチューニングする役割を担っています。しかし、未知のデータに対しては想定する性能を得られていません。スペシャリストは、オーバーフィッティングを防ぐために、どのパラメータをチューニングすべきでしょうか？",
            "options": {
                "A": "max_depthパラメータの値を大きくする。",
                "B": "max_depthパラメータの値を下げる。",
                "C": "objectiveをbinary:logisticに更新。",
                "D": "min_child_weightパラメータ値を下げる。"
            },
            "correct_answer": [
                "B"
            ],
            "explanation": "過学習の防止のためには、モデルの複雑さを抑える必要があります。max_depthが深すぎると木構造が複雑になり過学習が生じやすくなります。一般的に、XGBoostでのmax_depthは10以下が適切とされています。選択肢Aはmax_depthを増やすもので、過学習を悪化させる可能性があります。選択肢Cは目的関数の変更であり、過学習の直接的な解決策ではありません。選択肢Dはmin_child_weightを下げるもので、過学習の防止にはつながりません。",
            "references": [
                "https://qiita.com/c60evaporator/items/a9a049c3469f6b4872c6"
            ]
        },
        {
            "category": "モデリング",
            "question": "機械学習のスペシャリストは、線形回帰やロジスティック回帰などの線形モデルを用いて、多数の変数の予測モデルを開発しています。このスペシャリストは、探索的なデータ分析中に、いくつかの特性が互いに強く相関していることを発見します。これにより、モデルが不安定になる可能性があります。このような膨大な数の特徴の悪影響を緩和するためにはどうすればよいでしょうか。",
            "options": {
                "A": "相関性の高い特徴をOne-Hotでエンコーディングする。",
                "B": "相関性の高い特徴に対して、行列の乗算を行う。",
                "C": "主成分分析（PCA）を用いて新しい特徴空間を作る。",
                "D": "ピアソン相関係数を適用する。"
            },
            "correct_answer": [
                "C"
            ],
            "explanation": "重回帰モデルにおいて、特徴量同士の相関係数が非常に高い（もしくは完全に一致する）場合、多重共線性（マルチコ）という問題を引き起こします。この問題によって、モデルの出力が不安定な状態（分散が大きくなる）になってしまいます。回避する方法としては、相関している特徴量を圧縮するために次元圧縮の手法を用いるか、L1やL2正則化項を追加することが一般的です。選択肢AのOne-Hotエンコーディングは、カテゴリカルデータの処理に用いるもので、相関の問題を解決しません。選択肢Bの行列の乗算は、相関の問題を解決する方法ではありません。選択肢Dのピアソン相関係数は、相関の強さを測るものであり、問題の解決策ではありません。\nしたがって正解は、以下の通りです。\n- 主成分分析（PCA）を用いて新しい特徴空間を作る。"
        },
        {
            "category": "モデリング",
            "question": "機械学習のスペシャリストが、放物線状の目標関数に適合するモデルを近似しています。スペシャリストは一貫して間違った予測を出し続けています。さらに、学習されたモデルをプロットすると、一次関数のように見えます。これにはどのような理由があるのでしょうか？",
            "options": {
                "A": "モデルは高い分散を持っている",
                "B": "モデルのバランスが取れている",
                "C": "モデルがオーバーフィッティングしている",
                "D": "モデルの適合度が低い"
            },
            "correct_answer": [
                "D"
            ],
            "explanation": "モデルのフィットについて理解することは、モデルの精度が悪い根本的な原因を知る上で重要です。この知識は、修正措置を取るための指針となります。予測モデルがトレーニングデータに対してアンダーフィットしているかオーバーフィットしているかは、トレーニングデータと評価データの予測誤差を見ることで判断できます。\n予測モデルがトレーニングデータに適合していない場合、モデルはトレーニングデータに適合していません。これは、モデルが入力例（しばしばXと呼ばれる）と目標値（しばしばYと呼ばれる）の間の関係を捉えることができないためです。\nこのシナリオでは、学習されたモデルは、放物線状の関数に適合しようとする線形関数と表現されていました。これは、モデルの適合度が低いことを意味します。これは、モデルが単純すぎて、目標関数の変化を認識できないことを意味します。選択肢Aの高い分散は、モデルがデータに過剰に適合している場合に関連します。選択肢Bのバランスが取れているは、モデルの適合度が低いことを示すものではありません。選択肢Cのオーバーフィッティングは、モデルがトレーニングデータに過剰に適合している場合に関連します。\nしたがって正解は、以下の通りです。\n- モデルの適合度が低い"
        },
        {
            "category": "モデリング",
            "question": "インタラクティブなオンライン辞書では、同じような状況でよく使われる用語を表示するウィジェットが必要になります。機械学習のスペシャリストは、ウィジェットの下流の最近傍モデルに単語の特徴を提供する必要があります。これらの要件に確実に準拠するために、スペシャリストはどのような方法を取るべきでしょうか？",
            "options": {
                "A": "One-Hotワードエンコーディングベクターを作成する。",
                "B": "Amazon Mechanical Turkを使って、すべての単語の同義語を作成する。",
                "C": "他のすべての単語との編集距離を格納する単語埋め込みベクトルを作成する。",
                "D": "大規模なコーパスで事前に学習した単語埋め込みベクトルをダウンロードする。"
            },
            "correct_answer": [
                "D"
            ],
            "explanation": "辞書のような一般的な単語の集合を特徴量で表現する場合は、単語埋め込みベクトルを利用することが最適です。Amazon SageMaker BlazingText アルゴリズムは、Word2vec アルゴリズムとテキスト分類アルゴリズムの高度に最適化された実装を提供します。Word2vecアルゴリズムは、感情分析、名前付きエンティティ認識、機械翻訳など、多くの自然言語処理（NLP）タスクに役立ちます。\nテキスト分類は、ウェブ検索、情報検索、ランキング、文書分類などを行うアプリケーションにとって重要なタスクです。\n単語埋め込みベクトルは、単語のベクトル表現です。意味的に似ている単語は、近接したベクトルに対応します。このようにして、単語埋め込みベクトルは、単語間の意味的な関係を捉えます。\n多くの自然言語処理（NLP）アプリケーションは、大規模な文書のコレクションで学習することにより、単語埋め込みを学習します。\nこれらの事前学習されたベクトル表現は、意味論や単語の分布に関する情報を提供しますが、これは一般的に、後にもしくは限られた量のデータで学習される他のモデルの般化性能を向上させます。選択肢AのOne-Hotエンコーディングは、単語の意味的な関係を捉えることができません。選択肢BのAmazon Mechanical Turkを使って同義語を作成するのは、手間がかかりすぎます。選択肢Cの編集距離を格納する方法は、単語の意味的な関係を捉えるのに適していません。\nしたがって正解は、以下の通りです。\n- 大規模なコーパスで事前に学習した単語埋め込みベクトルをダウンロードする。"
        },
        {
            "category": "モデリング",
            "question": "機械学習のスペシャリストは、ある企業が自社製品に機械学習を取り入れるのを支援しています。この企業は、今後6ヶ月間に解約する可能性に基づいて、顧客を分類したいと考えています。機械学習スペシャリストは、ラベル付きデータセットへの完全なアクセス権を与えられました。このタスクのために、スペシャリストはどの形式の機械学習モデルを採用すべきでしょうか？",
            "options": {
                "A": "線形回帰",
                "B": "分類",
                "C": "クラスタリング",
                "D": "強化学習"
            },
            "correct_answer": [
                "B"
            ],
            "explanation": "機械学習モデルの学習とは、機械学習アルゴリズムに学習データを与えてモデルのパラメータを最適化することです。機械学習モデルは、この学習過程で作成されたモデルの成果物のことを指します。学習データには正解が含まれていなければならず、これをターゲットまたはターゲット属性といいます。学習アルゴリズムは、入力データの属性とターゲット（予測したい答え）を対応付けるパターンを学習データの中から見つけ出し、そのパターンを捉えた機械学習モデルを出力します。\n一般的に機械学習モデルが使用可能なタスクは、回帰と分類で、分類タスクは多クラス分類、2クラス（バイナリ）分類です。今回のケースは、6ヶ月以内に顧客が「解約する」か「解約しないか」の2つのグループに分類する必要があります。これは、2つの結果（2つの可能なクラスのうちの1つ）を予測する2クラス分類モデルが適した問題です。選択肢Aの線形回帰は、連続値を予測するためのものであり、分類には適していません。選択肢Cのクラスタリングは、ラベルなしデータのグループ化に使用され、分類には適していません。選択肢Dの強化学習は、エージェントが環境と相互作用しながら学習する手法であり、分類には適していません。\nしたがって正解は、以下の通りです。\n- 分類"
        },
        {
            "category": "モデリング",
            "question": "ITセキュリティチームは、クレジットカード番号などの機密情報を含むデータセットをAWSにアップロードします。機械学習スペシャリストは、これらのデータセットを利用して機械学習モデルを構築します。コンプライアンス上の理由から、データセットとその中の機密情報は暗号化して保護する必要があります。セキュリティチームはどのようにしてこれらの要件を達成できるでしょうか？",
            "options": {
                "A": "SageMakerのノートブックインスタンスでデータを送信する前に、クライアント側の暗号化キーでデータを暗号化するSageMakerのPCA（Principal Component Analysis）アルゴリズムを使用して、機密情報の長さを短縮する",
                "B": "Amazon S3に保存されているデータをIAMポリシーで暗号化し、Amazon SageMaker DeepARアルゴリズムを使用してクレジットカード番号をランダム化する",
                "C": "SageMakerのノートブックインスタンスでデータを送信する前に、クライアント側の暗号化キーでデータを暗号化するSageMaker BlazingTextアルゴリズムで機密情報を隠す",
                "D": "Amazon S3に保存されているデータをAWS KMSで暗号化し、SageMakerのノートブックインスタンス作成時にKMS暗号化を有効にするAWS Glueを使用して、データセット内の機密情報を再編集する"
            },
            "correct_answer": [
                "D"
            ],
            "explanation": "ノートブック、処理ジョブ、トレーニングジョブ、ハイパーパラメータチューニングジョブ、バッチ変換ジョブ、およびエンドポイントに提案されている機械学習（ML）ストレージボリュームを暗号化するには、AWS KMSキーをSageMakerに渡すことができます。AWSキーを指定しない場合、SageMakerはトランジェントストレージボリュームを暗号化し、ストレージボリュームの寿命が延びるたびに暗号化を行います。コンプライアンス上、KMSキーによる暗号化が必要な機密データは、MLストレージボリュームに加え、Amazon S3に保存する必要がありますが、どちらも指定したKMSキーで暗号化できます。また、AWS GlueでカスタムETLジョブを作成して実行することで、Amazon S3に保存されたデータセット内の機密情報を再編集することができます。選択肢AのPCAアルゴリズムは、データの次元削減に使用されるもので、暗号化には適していません。選択肢BのIAMポリシーは、アクセス制御に使用されるもので、暗号化には適していません。選択肢CのBlazingTextアルゴリズムは、テキストデータの処理に使用されるもので、暗号化には適していません。\nしたがって正解は、以下の通りです。\n- Amazon S3に保存されているデータをAWS KMSで暗号化し、SageMakerのノートブックインスタンス作成時にKMS暗号化を有効にするAWS Glueを使用して、データセット内の機密情報を再編集する。"
        },
        {
            "category": "モデリング",
            "question": "あるデータサイエンティストに保険記録のデータセットが与えられました。それぞれの記録には、記録のID、200カテゴリの請求候補の中から選ばれた最終請求結果、最終請求が行われた日付が記載されています。さらに、保険金請求の内容に関する詳細情報も提供されていますが、それは200カテゴリのうちのほんの一握りです。過去3年間で、それぞれの結果カテゴリについて何百もの記録が提供されています。データサイエンティストは、このデータを用いて各カテゴリの請求件数を月ごとに予測したいと考えています。どの機械学習アルゴリズムを使用すべきでしょうか？",
            "options": {
                "A": "請求内容に応じた200項目のデータを教師あり学習で月ごとに分類する",
                "B": "請求書のIDとタイムスタンプを利用した強化学習により、エージェントは各カテゴリの請求書が月ごとに何件になるかを特定する",
                "C": "請求書IDとタイムスタンプを用いた予測により、各カテゴリの請求件数が月ごとに何件になるかを特定する",
                "D": "請求内容の部分的な情報が提供されているカテゴリについては教師付き学習による分類を行い、それ以外のカテゴリについては請求IDとタイムスタンプを用いた予測を行う"
            },
            "correct_answer": [
                "C"
            ],
            "explanation": "最終請求項目とタイムスタンプデータのみは全てのレコードに対して存在しますが、請求内容については不完全であるため、完全なレコードのみを使用して月ごとの請求件数を予測することが適切です。ある期間の特定の件数を予測するための分析は、時系列データ分析の一種になります。そのため、各カテゴリの請求項目とそのタイムスタンプを用いて将来の予測を行うことが有効です。選択肢Aの教師あり学習での分類は、予測には適していません。選択肢Bの強化学習は、エージェントが環境と相互作用しながら学習する手法であり、予測には適していません。選択肢Dの部分的な情報を用いた分類は、予測には適していません。\nしたがって正解は、以下の通りです。\n- 請求書IDとタイムスタンプを用いた予測により、各カテゴリの請求件数が月ごとに何件になるかを特定する"
        },
        {
            "category": "モデリング",
            "question": "データサイエンティストは、金融取引の正当性を判断する機械学習モデルを構築しています。学習用に提供されたラベル付きデータは、不正ではない100,000個の観測値と、不正である1,000個の観測値で構成されています。学習したモデルを、これは知られていなかった検証データセットに適用すると、データサイエンティストは次のような混同行列を得ます。このモデルの精度は99.1%ですが、データサイエンティストは偽陰性を最小限に抑えるように要求されています。",
            "options": {
                "A": "XGBoostのeval_metricパラメータを、エラーではなくrmseに基づいて最適化するように変更する",
                "B": "XGBoostのscale_pos_weightパラメータを増やして、正負の重みのバランスを調整する",
                "C": "XGBoostのmax_depthパラメータを増やしてください",
                "D": "XGBoostのeval_metricパラメータを変更して、エラーではなくAUCに基づいて最適化する",
                "E": "モデルがデータに適合しすぎているので、XGBoostのmax_depthパラメータを小さくする"
            },
            "correct_answer": [
                "B",
                "D"
            ],
            "explanation": "今回のデータセットを確認すると、陰性データが全体の99%を占めている不均衡データであることがわかります。こういった不均衡なデータを学習する際は、偏りを考慮して各ラベルの重みの更新の割合を変化させる必要があります。\n今回であれば、偽陰性（本当は陰性だが陰性と予測されたデータ）を減らすために、よりデータの少ない陰性データの重み更新を大きくする必要があります。また、不均衡データの場合の評価指標として使用されるものは、エラーではなくAUC（ROC曲線の面積で評価の性能の良さを表す）です。選択肢Aのrmseは回帰問題の評価指標であり、分類問題には適していません。選択肢Cのmax_depthを増やすことは、過学習を引き起こす可能性があります。選択肢Eのmax_depthを小さくすることは、偽陰性を減らす直接的な方法ではありません。\nしたがって正解は、以下の通りです。\n- XGBoostのscale_pos_weightパラメータを増やして、正負の重みのバランスを調整する\n- XGBoostのeval_metricパラメータを変更して、エラーではなくAUCに基づいて最適化する",
            "references": [
                "https://docs.aws.amazon.com/ja_jp/sagemaker/latest/dg/xgboost-tuning.html"
            ]
        },
        {
            "category": "モデリング",
            "question": "ある企業が、ビデオ会議アプリケーションで、ユーザーが仮想背景を選択・作成できる機能を構築しようとしています。この会社で働いている機械学習スペシャリストは、カスタマイズされた背景が元の背景の上にのみマスクされるように、リアルタイムで人の形を識別できるモデルを訓練するように求められました。この問題を解決できるアルゴリズムはどれでしょうか？",
            "options": {
                "A": "セマンティックセグメンテーション",
                "B": "Object2Vec",
                "C": "オブジェクト検出",
                "D": "画像分類"
            },
            "correct_answer": [
                "A"
            ],
            "explanation": "今回の要件では、画像のピクセルレベルでのラベル付けを行い、それに基づく映像処理を行う必要があります。セマンティックセグメンテーションアルゴリズムは、コンピュータビジョンアプリケーションを開発する際に、ピクセルレベルでのきめ細かいアプローチを提供します。\nセマンティックセグメンテーションアルゴリズムは、画像内のすべてのピクセルに、あらかじめ定義されたクラスのセットからクラスラベルを付けます。タグ付けは、シーンを理解するための基本であり、自動運転車、医療画像診断、ロボットセンシングなど、ますます多くのコンピュータビジョンアプリケーションに欠かせないものとなっています。選択肢BのObject2Vecは、オブジェクトのベクトル表現を学習するためのものであり、ピクセルレベルのラベル付けには適していません。選択肢Cのオブジェクト検出は、画像内のオブジェクトの位置を特定するものであり、ピクセルレベルのラベル付けには適していません。選択肢Dの画像分類は、画像全体をクラスに分類するものであり、ピクセルレベルのラベル付けには適していません。\nしたがって正解は、以下の通りです。\n- セマンティックセグメンテーション",
            "references": [
                "https://docs.aws.amazon.com/ja_jp/sagemaker/latest/dg/semantic-segmentation.html",
                "https://jp.mathworks.com/solutions/image-video-processing/semantic-segmentation.html",
                "https://www.youtube.com/watch?v=e16yGrUGz-4"
            ]
        },
        {
            "category": "モデリング",
            "question": "ある金融機関は、Amazon SageMakerを主要なデータサイエンス環境にしたいと考えています。この会社のデータサイエンティストは、機密性の高い金融データ上で機械学習（ML）モデルを実行しています。この企業は、データの流出を懸念しており、環境を保護するために機械学習エンジニアのサービスを希望しています。 SageMakerからのデータ流出を管理するために、機械学習エンジニアが自由に使える方法はどれですか？（3つ選択）",
            "options": {
                "A": "AWS PrivateLinkによるVPCインターフェースのエンドポイントを使用してSageMakerに接続する",
                "B": "SCPを使用してSageMakerへのアクセスを制限する",
                "C": "SageMakerのノートブックインスタンスのルートアクセスを無効にする",
                "D": "トレーニングジョブとモデルのネットワーク分離を有効にする",
                "E": "ノートブックの指定されたURLを会社で使用されている特定のIPに制限する",
                "F": "データを保存時および転送時に暗号化して保護する。AWS Key Management Service（AWS KMS）を使用して暗号化キーを管理する"
            },
            "correct_answer": [
                "A",
                "D",
                "F"
            ],
            "explanation": "ネットワークの観点では、PrivateLinkとネットワーク分離が機密データの流出リスクを最小化するために必要です。また、暗号化の観点で、AWS KMSを使用してデータを保存時および転送時に暗号化することで、データ保護が確保されます。このため、正しい選択肢は次の3つです：AWS PrivateLinkによるVPCインターフェースのエンドポイントを使用してSageMakerに接続する、トレーニングジョブとモデルのネットワーク分離を有効にする、データを保存時および転送時に暗号化して保護する。AWS Key Management Service（AWS KMS）を使用して暗号化キーを管理する。選択肢BのSCPは、組織全体のポリシー管理に使用されるもので、SageMakerのデータ流出防止には直接関係しません。選択肢Cのルートアクセスの無効化は、データ流出防止には直接関係しません。選択肢EのIP制限は、データ流出防止には直接関係しません。",
            "references": [
                "https://docs.aws.amazon.com/ja_jp/sagemaker/latest/dg/encryption-at-rest.html"
            ]
        },
        {
            "category": "モデリング",
            "question": "機械学習のスペシャリストが開発した深層学習ニューラルネットワークモデルは、トレーニングデータでは良好な性能を発揮しますが、テストデータでは高い性能を発揮することができません。これを修正するために、スペシャリストは次のうちどれを調べるべきでしょうか。（3つ選択）",
            "options": {
                "A": "正則化を減らす",
                "B": "正則化を増やす",
                "C": "ドロップアウトを増やす",
                "D": "ドロップアウトを減らす",
                "E": "特徴量の組み合わせを増やす",
                "F": "特徴量の組み合わせを減らす"
            },
            "correct_answer": [
                "B",
                "C",
                "F"
            ],
            "explanation": "機械学習モデルを正しく評価するためには、学習データとは異なるデータを用いたテストでの性能を評価する必要があります。トレーニングデータセットに対する性能が良好だが、テストデータセットに対する性能が低い状況では、過剰適合（overfitting）が考えられます。このような場合、過剰適合を防ぐために以下のアプローチをとります：正則化の規約を増やす、ドロップアウトを増やす、特徴量の組み合わせを減らす。これにより、モデルがトレーニングデータに過度に適合するのを防ぎ、より汎用的な性能を持つようにします。選択肢Aの正則化を減らすことは、過剰適合を悪化させる可能性があります。選択肢Dのドロップアウトを減らすことは、過剰適合を悪化させる可能性があります。選択肢Eの特徴量の組み合わせを増やすことは、過剰適合を悪化させる可能性があります。",
            "references": [
                "https://otafuku-lab.co/aizine/glossary-regularization/"
            ]
        },
        {
            "category": "モデリング",
            "question": "機械学習のスペシャリストが、あるビジネスユースケースのためにバイナリ分類モデルをトレーニングしています。スペシャリストが最も最適なモデルを選択できるように、会社は満たさなければならない以下の条件を提示しました。\n\n- 偽陽性には偽陰性の3倍のコストがかかることから、最もコスト効率の良いモデルを選択する。\n- 回収率が85%以上のモデルを選択する。\n- 偽陽性率が15%以下のモデルを選択する。",
            "options": {
                "A": "TN = 95, FP = 5, FN = 18, TP = 82",
                "B": "TN = 75, FP = 25, FN = 11, TP = 89",
                "C": "TN = 82, FP = 18, FN = 15, TP = 85",
                "D": "TN = 99, FP = 1, FN = 20, TP = 80"
            },
            "correct_answer": [
                "B"
            ],
            "explanation": "この問題に正しく答えるためには、3つの式を知っておく必要があります。\n\n- 回収率 = TP / (TP + FN)\n- 偽陽性率 = FP / (FP + TN)\n- コスト関数 = (3 * FN) + FP\n\nシナリオでは、偽陰性は偽陽性の3倍のコストがかかると述べられています。これを方程式に置き換えると、コスト関数 = (3 * FN) + FP となります。選択肢Aのモデルは、偽陽性率が15%以下であるが、回収率が85%未満です。選択肢Cのモデルは、偽陽性率が15%以下であるが、コスト効率が最適ではありません。選択肢Dのモデルは、偽陽性率が15%以下であるが、回収率が85%未満です。\n\nしたがって正解は、以下の通りです：\n- TN = 75, FP = 25, FN = 11, TP = 89"
        },
        {
            "category": "モデリング",
            "question": "ある機械学習のスペシャリストが、分類の問題を解決するために、ナイーブベイジアンモデルを構築するか、完全ベイジアンネットワークを構築するかを議論しています。スペシャリストは、各特徴量間のピアソン相関係数を計算し、その絶対値が0.1から0.95まで変化することを発見しました。この場合、どのモデルが基礎データを最もよく表しているでしょうか？",
            "options": {
                "A": "ナイーブベイジアンモデル：特徴はすべて条件付き独立である",
                "B": "完全ベイジアンネットワーク：特徴はすべて条件付き独立である",
                "C": "ナイーブベイジアンモデル：特徴のいくつかは統計的に従属している",
                "D": "完全ベイジアンネットワーク：特徴のいくつかは統計的に従属している"
            },
            "correct_answer": [
                "D"
            ],
            "explanation": "ベイジアンモデルは、ベイズの定理という確率論における条件付き確率の定理を用いたモデルです。ナイーブベイジアンモデルでは、各事象が起きる確率を独立であると仮定して事前確率を設定しますが、完全ベイジアンモデル（ベイジアンネットワーク）では、この仮定を行わないモデルです。今回のデータにおいて各特徴量間の相関が無視できない程度であると考えると、ナイーブベイジアンモデルの条件付き独立の仮定は成立しなくなります。そのため、完全ベイジアンネットワークを利用するべきです。選択肢Aのナイーブベイジアンモデルは、特徴が条件付き独立であることを仮定しており、今回のデータには適していません。選択肢Bの完全ベイジアンネットワークは、特徴が条件付き独立であることを仮定しており、今回のデータには適していません。選択肢Cのナイーブベイジアンモデルは、特徴が統計的に従属している場合には適していません。",
            "references": [
                "https://ichi.pro/nai-bubeizu-nitsuite-shitteoku-beki-koto-subete-169976819469219",
                "https://www.msiism.jp/article/what-is-bayesian-network.html"
            ]
        },
        {
            "category": "モデリング",
            "question": "ある機械学習チームは、研究データセットを使用して、Amazon SageMakerを使用してApache MXNetで書き文字分類モデルを学習します。チームは、モデルがオーバーフィットしたときに通知を受けたいと考えています。監査人は、Amazon SageMakerのログアクティビティレポートを検査して、違法なAPIコールが発生していないことを保証したいと考えています。機械学習チームは、可能な限り少ないコード行と手順で基準を満たすために何をすべきでしょうか？",
            "options": {
                "A": "Amazon SageMaker APIの呼び出しをAmazon S3にログするためのAWS Lambda関数を実装する。カスタムメトリックをAmazon CloudWatchにプッシュするコードを追加する。Amazon SNSを使ってCloudWatchにアラームを作成し、モデルがオーバーフィットリングしたときに通知を受け取る",
                "B": "AWS CloudTrailを使用して、Amazon SageMakerのAPIコールをAmazon S3にログする。カスタムメトリックをAmazon CloudWatchにプッシュするコードを追加する。Amazon SNSでCloudWatchにアラームを作成し、モデルがオーバーフィットリングしたときに通知を受け取る",
                "C": "Amazon SageMakerのAPIコールをAWS CloudTrailにログするためのAWS Lambda関数を実装する。カスタムメトリックをAmazon CloudWatchにプッシュするコードを追加する。モデルがオーバーフィットリングしているときに通知を受け取るために、Amazon SNSでCloudWatchにアラームを作成する",
                "D": "AWS CloudTrailを使用して、Amazon S3へのAmazon SageMaker APIコールをログに記録する。モデルがオーバーフィットリングしているときに通知を受け取るために、Amazon SNSを設定する"
            },
            "correct_answer": [
                "B"
            ],
            "explanation": "アクティビティを保存するための正しいサービスの組み合わせを選択する必要があります。AWS CloudTrailでAmazon SageMaker APIのアクティビティログを保存し、Amazon S3にログを配置することで、AWSアカウント内のアクティビティを監査でき、Lambda関数の実装は不要です。また、メトリクス監視についてはCloudWatchが適切です。したがって、正解は「AWS CloudTrailを使用して、Amazon SageMakerのAPIコールをAmazon S3にログする。カスタムメトリックをAmazon CloudWatchにプッシュするコードを追加する。Amazon SNSでCloudWatchにアラームを作成し、モデルがオーバーフィットリングしたときに通知を受け取る」となります。選択肢AのLambda関数は、CloudTrailを使用することで不要です。選択肢CのLambda関数は、CloudTrailを使用することで不要です。選択肢Dは、カスタムメトリックのプッシュが含まれていません。",
            "references": [
                "https://docs.aws.amazon.com/ja_jp/sagemaker/latest/dg/logging-using-cloudtrail.html"
            ]
        },
        {
            "category": "モデリング",
            "question": "あるデータサイエンティストは、Amazon SageMakerを使って、オンライン小売業者のためにレコメンデーションモデルを開発するというタスクを任されています。同社の商品には制限上、顧客が購入する商品は5〜10年に4〜5個程度です。そのため、この企業は継続的に流入する新規顧客に依存しています。新規顧客が登録すると、企業はその顧客の好みに関する情報を収集します。以下は、データサイエンティストがアクセスできるデータの一例です。",
            "data_example": [
                {
                    "timestamp": "2021/3/4",
                    "user_id": 90,
                    "product_id": 223,
                    "preference": 0.374
                },
                {
                    "timestamp": "2021/3/4",
                    "user_id": 90,
                    "product_id": 561,
                    "preference": 0.374
                },
                {
                    "timestamp": "2021/2/21",
                    "user_id": 203,
                    "product_id": 76,
                    "preference": 0.098
                }
            ],
            "options": {
                "A": "すべてのインタラクションデータをシャッフルする。インタラクションデータの最後の10%をテストセット用に分割する",
                "B": "各ユーザーの最新の10%のインタラクションを特定する。これらのインタラクションをテストセットとして分割する",
                "C": "最もインタラクションデータの少ない10%のユーザーを特定する。これらのユーザーのすべてのインタラクションデータをテストセット用に分割する",
                "D": "10%のユーザーをランダムに選択する。これらのユーザーからのすべてのインタラクションデータをテストセットとして分割する"
            },
            "correct_answer": [
                "B"
            ],
            "explanation": "ユーザーアクティビティが豊富でないことを考慮すると、ターゲットのリークが精度に重要な影響を及ぼす可能性があります。ターゲットリークとは、ターゲットが観測された時点よりも後のデータを含めて学習させたことによってモデルの精度が変化することです。このユースケースでは、各ユーザーの最新の10%のインタラクションをテストセットにすることで、過去のデータのみを使って将来のイベントを予測する形を保つことができ、ターゲットリークを防ぎます。したがって、正解は「各ユーザーの最新の10%のインタラクションを特定する。これらのインタラクションをテストセットとして分割する」となります。選択肢Aのデータをシャッフルする方法は、ターゲットリークを防ぐことができません。選択肢Cのインタラクションデータの少ないユーザーを選ぶ方法は、ターゲットリークを防ぐことができません。選択肢Dのランダムにユーザーを選ぶ方法は、ターゲットリークを防ぐことができません。"
        },
        {
            "category": "モデリング",
            "question": "あるデータサイエンティストは、一連の検査所見に基づいて患者が特定の病気にかかっているかどうかを識別するバイナリ分類器の開発に取り組んでいます。データサイエンティストは、地域から無作為に選ばれた400人の患者の情報を持っています。その病気は人口の3%が罹患しています。データサイエンティストは、クロスバリデーションのためにどのアプローチを使用するべきでしょうか？",
            "options": {
                "A": "k=5のk-fold クロスバリデーション法",
                "B": "k=5 の層化 k-fold クロスバリデーション法",
                "C": "k=5で3回繰り返すk-fold クロスバリデーション法",
                "D": "トレーニングと検証を80/20に分割して行う"
            },
            "correct_answer": [
                "B"
            ],
            "explanation": "罹患者の割合が3%である不均衡データセットであるため、単純なK分割交差検証を行うと、罹患者の偏りが生じる可能性があります。従って、トレーニングセットでもテストセットでもラベルの割合を保ったK分割交差検証を行う必要があります。層化K分割交差検証（Stratified K-Folds cross-validator）では、ラベルの割合を揃えながらK分割交差検証をする方法です。したがって正解は「k=5 の層化 k-fold クロスバリデーション法」となります。選択肢Aの単純なk-foldクロスバリデーションは、ラベルの偏りを考慮しません。選択肢Cの繰り返しk-foldクロスバリデーションは、ラベルの偏りを考慮しません。選択肢Dの80/20分割は、ラベルの偏りを考慮しません。",
            "references": [
                "https://docs.aws.amazon.com/ja_jp/machine-learning/latest/dg/cross-validation.html",
                "https://analytics-note.xyz/machine-learning/stratified-kfolds-cross-validator/"
            ]
        },
        {
            "category": "モデリング",
            "question": "ある企業のWebサイトで、機械学習のスペシャリストが、商品の提案を行うモデルを実装しました。当初、このコンセプトは見事に機能し、消費者が平均してより多くの商品を購入する結果となりました。しかし、このスペシャリストは、ここ数か月で商品提案の効果が薄れ、消費者が以前の購入パターンに戻っていることに気付きました。このモデルは1年以上前に導入されて以来、同じモデルでした。\n\nモデルのパフォーマンスを向上させるために、スペシャリストはどのような戦略をとるべきでしょうか？",
            "options": {
                "A": "顧客の購買行動の変動に対応できないため、モデルを完全に再設計する必要がある",
                "B": "ドリフトを防ぐために、モデルのハイパーパラメータを定期的に更新する必要があります",
                "C": "モデルは、元のデータを使って最初から定期的に再学習し、顧客の購買行動の変化を扱うために正則化項を追加する必要があります",
                "D": "モデルは、元のトレーニングデータに加えて、顧客の購買行動の変化に応じた新しいデータを使って、定期的に再学習されるべきです"
            },
            "correct_answer": [
                "D"
            ],
            "explanation": "モデルが予測に成功するということは、顧客が学習データで予測できる範囲の購買行動をとっているということです。一方で、モデルが予測に失敗するということは、顧客が学習データで予測できる範囲外の購買行動をとっているということになります。この現象はデータドリフトと呼ばれ、機械学習モデルをトレーニングした時のデータと、予測を行う時点でのデータがずれていくために発生します。データドリフトを防止するためには、定期的に学習データを追加して最新の特徴量をモデルに取り込むことが必要です。選択肢Aは再設計が必要とは限らず、選択肢Bはハイパーパラメータの更新だけでは不十分です。選択肢Cは新しいデータを考慮していないため不適切です。"
        },
        {
            "category": "モデリング",
            "question": "あるデータサイエンティストが、感情分析アプリケーションを開発しています。現在はテストセットによる分類精度が低く、データサイエンティストは、データセットの語彙数が多く、用語の平均頻度が低いことが原因だと考えています。検証の精度を上げるためには、どのツールを利用すべきでしょうか？",
            "options": {
                "A": "Amazon Comprehendの構文解析とエンティティ検出",
                "B": "Amazon SageMaker BlazingText の cbow モード",
                "C": "Natural Language Toolkit（NLTK）によるステミングとストップワードの除去",
                "D": "Scikit-learnのTF-IDF ベクトル化手法"
            },
            "correct_answer": [
                "C"
            ],
            "explanation": "このケースでは、データサイエンティストがコーパスの問題を指摘しているため、モデル等の改良ではなく、コーパスの前処理にフォーカスをするべきです。ステミングやストップワード（the, a, anなどの頻度は高いが情報量が少ない単語）の削除をすることで、ノイズ除去を行うことが可能です。したがって正解は、Natural Language Toolkit（NLTK）によるステミングとストップワードの除去です。選択肢AとBは前処理ではなく、選択肢Dは語彙数の多さに対処しないため不適切です。"
        },
        {
            "category": "モデリング",
            "question": "あるデータサイエンスチームは、機械学習モデルで利用されることの多い大量のトレーニングデータを格納するデータセットリポジトリを開発しています。データサイエンティストは、毎日膨大な量の新しいデータセットを開発する可能性があるため、ソリューションは拡張性とコスト効率に優れていなければなりません。さらに、SQLによるデータの探索が可能でなければなりません。このシナリオに最も適しているのは、どのストレージ方法でしょうか？",
            "options": {
                "A": "Amazon S3にデータセットをファイルとして保存する",
                "B": "Amazon EC2インスタンスに接続されたAmazon EBSボリュームにデータセットをファイルとして保存する",
                "C": "複数ノードのAmazon Redshiftクラスタにテーブルとしてデータセットを保存する",
                "D": "Amazon DynamoDBにグローバルテーブルとしてデータセットを保存する"
            },
            "correct_answer": [
                "A"
            ],
            "explanation": "拡張性とコスト効率に優れ、SQLによるデータ探索をサポートするストレージを選ぶ必要があります。Amazon S3は、ウェブ上のどこからでも、いつでも、どんな量のデータも保存・取得できるシンプルなウェブサービスインターフェースを提供しています。Amazon S3に保存されたデータを分析するためにAmazon Athenaを使用することで、SQLクエリを使用して直接データにアクセスすることが可能です。したがって正解は、Amazon S3にデータセットをファイルとして保存するです。選択肢Bは拡張性に欠け、選択肢Cはコスト効率が悪く、選択肢DはSQLによる探索に適していません。"
        },
        {
            "category": "モデリング",
            "question": "果物加工会社の機械学習のスペシャリストは、リンゴを3つのカテゴリーに分類するシステムを開発することを課題としています。このスペシャリストは、リンゴの種類ごとに150枚の写真を集め、このデータセットを使って伝達学習によりImageNetでニューラルネットワークを学習しました。同社では、モデルの精度が85%以上であることをシステム要件としています。グリッドサーチの結果、最適なハイパーパラメータは以下のとおりとなりました。\n\n- トレーニングセットでの精度は68%\n- 検証セットでの精度が67%\n\n機械学習のスペシャリストは、システムの精度を上げるために何ができるでしょうか？",
            "options": {
                "A": "モデルをAmazon SageMakerのノートブックインスタンスにアップロードし、Amazon SageMakerのHPO機能を使用してモデルのハイパーパラメータを最適化する。",
                "B": "トレーニングセットにさらにデータを追加し、バイアスを減らすために伝達学習を使用してモデルを再学習する。",
                "C": "ImageNetで事前に学習された、より多くの層を持つニューラルネットワークモデルを使用し、転移学習を適用して分散を増加させる。",
                "D": "現在のニューラルネットワークアーキテクチャを使用して新しいモデルを学習する。"
            },
            "correct_answer": [
                "B"
            ],
            "explanation": "今回作成されたモデルは、訓練セット、検証セットの予測精度がいずれも低いため適合不足であると判断できます。また、ハイパーパラメータチューニングはすでに行われています。従って、問題はモデル自体の問題というよりは学習セット不足であるとみなすことができます。実際、学習に使用された画像は各クラスで150枚であるため、画像分類を行うためのデータセットとしては絶対量が不足していると考えることができます。選択肢Aは既に行われた手法であり、選択肢CとDはデータ不足の問題を解決しません。"
        },
        {
            "category": "モデリング",
            "question": "ある大手消費財メーカーが、次のような商品を販売しています。\n- 34種類の歯磨き粉のバリエーション\n- 48種類の歯ブラシ\n- 43種類のマウスウォッシュ\n\nAmazon S3には、これらの商品販売履歴がすべて保存されています。現在、同社はカスタムメイドの自己回帰統合移動平均（ARIMA）モデルを使ってこれらの商品の需要を予測しています。企業は、新しく発表された商品の需要を予測したいと考えています。\n\n機械学習のスペシャリストは、次のどのソリューションを実装すべきでしょうか？",
            "options": {
                "A": "新製品の需要を予測するために、カスタムARIMAモデルを作成する。",
                "B": "Amazon SageMakerのDeepARアルゴリズムを学習し、新製品の需要を予測する。",
                "C": "Amazon SageMakerのk-meansクラスタリングアルゴリズムをトレーニングして、新製品の需要を予測する。",
                "D": "新製品の需要を予測するためにカスタムXGBoostモデルを学習する。"
            },
            "correct_answer": [
                "B"
            ],
            "explanation": "Amazon SageMaker DeepAR予測アルゴリズムは、再帰型ニューラルネットワーク（RNN）を使用してスカラー（1次元）時系列を予測するための教師あり学習アルゴリズムです。ARIMAや指数平滑（ETS）などの従来の時系列予測方法は単一のモデルを各時系列に適合させますが、DeepARはすべての時系列にわたって単一のモデルを共有してトレーニングできるため、メリットがあります。\n\nDeepARはデータセットに何百も関連する時系列データが含まれている場合、標準のARIMAやETSメソッドよりも優れており、トレーニングしたモデルを使用して、新しい時系列の予測も生成することができます。したがって、正解は次の通りです：Amazon SageMakerのDeepARアルゴリズムを学習し、新製品の需要を予測する。選択肢Aは新製品に対して適用が難しく、選択肢CとDは時系列予測に適していません。"
        },
        {
            "category": "モデリング",
            "question": "機械学習のスペシャリストは、猫を識別する目的で、教師付き画像認識モデルを開発する必要があります。機械学習のスペシャリストは、ニューラルネットワークを利用した画像分類モデルを作成するために以下のような条件でモデル構築を行なっています。\n- 写真は全部で1,000枚です。\n- テストセットとして100枚の写真を選定します。\n機械学習のスペシャリストは、誤分類された写真の75%以上で、猫が写真の中で逆さまになっていることを発見しました。\n\nこの特定の誤分類を改善するために、機械学習のスペシャリストはどの方法を用いることができますか？",
            "options": {
                "A": "学習画像に回転のバリエーションを加えて学習データを増やす。",
                "B": "モデル学習のエポック数を増やす。",
                "C": "ニューラルネットワークの層の数を増やす。",
                "D": "最後から2番目の層のドロップアウト率を上げる。"
            },
            "correct_answer": [
                "A"
            ],
            "explanation": "データ拡張は既存のデータセットから様々なテクニックを用いてデータ量を数倍〜数十倍にまで拡張する方法です。例えば1枚の画像に対して、反転、輝度変異、回転、平行移動、合成などを行い、全く異なるデータを多く作成することが可能です。\n\n今回のケースであれば、猫が逆さまに写っている画像を、元画像から機械的な処理によって生成することで誤分類を減少させることができます。これによって、モデルがより多くの猫のパターンを学習することができるので、誤分類が減少します。したがって正解は以下の通りです。\n- 学習画像に回転のバリエーションを加えて学習データを増やす。選択肢B、C、Dはデータの多様性を増やすことにはつながらず、誤分類の改善には不十分です。"
        },
        {
            "category": "モデリング",
            "question": "全世界に展開をしている小売チェーンでは、Amazon Kinesis Data Firehoseを利用して、2万店舗のネットワークからAmazon S3に購入明細を取り込んでいます。より高度な機械学習モデルのトレーニングを促進するために、トレーニングデータに簡単な変換処理を追加して、一部の属性を組み合わせる必要があります。また、モデルの再トレーニングを毎日行う必要があります。膨大な数の店舗と過去のデータを取り込むことを考えると、どのアップデートが最も少ない開発作業で済むでしょうか？",
            "options": {
                "A": "Amazon S3にロードするためにAWS Storage Gatewayにローカルにデータを取り込み、AWS Glueを使って変換を行うように変更することを店舗に要求する。",
                "B": "変換ロジックを持つApache Sparkを実行するAmazon EMRクラスターを展開し、クラスターにAmazon S3に蓄積されたレコードを毎日実行させ、新しい／変換されたレコードをAmazon S3に出力させる。",
                "C": "変換ロジックを持つAmazon EC2インスタンスのフリートをスピンアップし、Amazon S3に蓄積されるデータレコードを変換させ、変換されたレコードをAmazon S3に出力する。",
                "D": "Kinesis Data Firehoseストリームの下流にAmazon Kinesis Data Analyticsストリームを挿入し、SQLを使用して生のレコード属性を単純な変換値に変換する。"
            },
            "correct_answer": [
                "D"
            ],
            "explanation": "既にAWS上に構築されたサービスと、「簡単な変換と一部の属性の組み合わせ」、「開発工数の少なさ」という要件を考慮する必要があります。\n\nAmazon Kinesis Data Analyticsは、Apache Flinkを使用して、ストリーミングデータをリアルタイムで変換および分析する最も簡単な方法です。Kinesis Data AnalyticsはFirehoseからデータを取得し、変換してS3へ書き込むことができます。これにより、既存サービスを活用しつつ、簡単な変換を少ない開発工数で行い、データを生成することができます。\n\nまた、一度設定することでインフラストラクチャの管理は不要なため、毎日行われるモデルの再トレーニングの際に常に最新のデータを提供し続けることができます。\n\nしたがって正解は以下の通りです：\n- Kinesis Data Firehoseストリームの下流にAmazon Kinesis Data Analyticsストリームを挿入し、SQLを使用して生のレコード属性を単純な変換値に変換する。選択肢A、B、Cは開発工数が多く、リアルタイム処理に適していません。"
        },
        {
            "category": "モデリング",
            "question": "機械学習のスペシャリストが、あるメディア企業のWebサイトに掲載されている人気記事の分類を支援しています。この企業では、記事が公開される前に、ランダムフォレストを使ってその人気度を予測しています。以下は、利用されたデータの例です。\n\nスペシャリストは、データセットの「曜日」列をバイナリ値に変換したいと考えています。この列の値をバイナリに変換するには、どのアプローチを使用すべきでしょうか？",
            "options": {
                "A": "2値化",
                "B": "One-Hotエンコーディング",
                "C": "トークン化",
                "D": "正規化変換"
            },
            "correct_answer": [
                "B"
            ],
            "explanation": "曜日のように、属性の種類はわかっているものの数値化できない特徴量をモデルの学習に使用するためには、One-Hotエンコーディングという処理が必要です。\n\nOne-Hotエンコーディング（ダミー変数）とは、One-Hot、つまり1つだけ1でそれ以外は0として表されたベクトルを生成することです。\n\n曜日の例であれば、「曜日」という特徴量は「月曜日」、「火曜日」、...、「日曜日」までの7つの値を持ちますが、これをOne-Hotエンコーディングすることで、「月曜日」、「火曜日」、...、「日曜日」という特徴量を作成し、その値はレコードの街灯の曜日だけ1、それ以外は0、といった表現を行います。\n\nこの操作によって、離散的であり数値を持たない特徴量を、機械学習モデルで学習可能な特徴量として扱うことができます。\n\nしたがって正解は、以下の通りです：\n- One-Hotエンコーディング。選択肢Aは2値化が適用できない場合があり、選択肢CとDは曜日のようなカテゴリカルデータの変換には不適切です。",
            "references": [
                "https://aws.amazon.com/jp/what-is/embeddings-in-machine-learning/",
                "https://www.codexa.net/get_dummies/"
            ]
        },
        {
            "category": "モデリング",
            "question": "機械学習のスペシャリストは、P3インスタンス上でAmazon SageMakerエンドポイントを実行し、内蔵のオブジェクト認識アルゴリズムを使用して、本番アプリケーションでリアルタイム予測を行っています。スペシャリストがモデルのリソース消費を調べると、モデルがGPUの一部しか使用していないことがわかりました。\n\nどのアーキテクチャを改善すれば、提供されたリソースを最大限に活用できるでしょうか？",
            "options": {
                "A": "モデルを、M5インスタンス上のバッチ変換ジョブとして再配置する",
                "B": "モデルをM5インスタンスに再デプロイする。インスタンスにAmazon Elastic Inferenceをアタッチする",
                "C": "P3dn インスタンスにモデルを再デプロイする",
                "D": "P3インスタンスを使用してAmazon Elastic Container Service（Amazon ECS）クラスターにモデルをデプロイする"
            },
            "correct_answer": [
                "B"
            ],
            "explanation": "SageMakerの推論を行うコンピューティングリソースのコスト最適化を行うためのサービスを選択する必要があります。\n\nAmazon Elastic Inferenceは、Amazon EC2やSagemakerインスタンス、Amazon ECSタスクに低コストのGPUを搭載したアクセラレーションを取り付け、深層学習の推論を実行するコストを最大75%削減することができます。\n\nAmazon Elastic Inferenceは、TensorFlow、Apache MXNet、PyTorch、およびONNXモデルをサポートします。\n\nまた、あらゆるAmazon EC2、Amazon SageMakerのインスタンスタイプやAmazon ECSタスクに、適切な量のGPUによる推論アクセラレーションを取り付けることができます。\n\nしたがって正解は、以下の通りです：\n- モデルをM5インスタンスに再デプロイする。インスタンスにAmazon Elastic Inferenceをアタッチする。選択肢A、C、Dはリソースの最適化に寄与しません。"
        },
        {
            "category": "モデリング",
            "question": "あるWebベースのビジネスを提供する会社では、ランディングページ訪問者のコンバージョンレートを増やしたいと考えています。この企業は、Amazon SageMakerを使用して、顧客の訪問に関する大きな過去のデータセットを用いて、多クラスのディープラーニングネットワークアルゴリズムを定期的に開発しました。しかし、オーバーフィッティングの問題があります。トレーニングデータでは90％の予測精度を示しているのに対し、テストデータでは70％の予測精度しか示していません。\n\nこの組織は、訪問者から購入へのコンバージョンを最適化するために、本番稼働前にモデルの汎化性能を高めなければなりません。\n\nテストデータと検証データが可能な限り高い精度でモデル化されるようにするには、どの活動をすればよいでしょうか？",
            "options": {
                "A": "学習に使用するミニバッチの学習データのランダム性を高める",
                "B": "全体のデータのうち、より多くの割合を学習データセットに割り当てる",
                "C": "学習データにL1またはL2正則化とドロップアウトを適用する",
                "D": "深層学習ネットワークの層やユニット（ニューロン）の数を減らす"
            },
            "correct_answer": [
                "C"
            ],
            "explanation": "今回は十分なデータセットを持っている、という前提でオーバーフィッティングを解消する必要があります。\n\n十分なトレーニングデータを持っている場合、モデルがトレーニングデータに対してオーバーフィットしている場合、モデルの柔軟性を減らす行動をとることに重点が置かれます。\n\nモデルの柔軟性を減らすには、以下のようなことを試すことが効果的です：\n- 特徴選択：特徴の数を減らす。単語を少なくする、N-gramのサイズを小さくする、数値値のビンの数を減らす等の操作を検討する。\n- 正則化の使用量を増やす。\n- ニューラルネットワークの層であればドロップアウト（特定のノード出力を学習時にランダムで0に落とすこと）を適用する。\n\nしたがって正解は、以下の通りです：\n- 学習データにL1またはL2正則化とドロップアウトを適用する。選択肢A、B、Dはオーバーフィッティングの解消には不十分です。",
            "references": [
                "https://ai-trend.jp/basic-study/neural-network/regularization/"
            ]
        },
        {
            "category": "モデリング",
            "question": "機械学習のスペシャリストは、世界中の企業のセキュリティイベントをリアルタイムで処理する多国籍のサイバーセキュリティ企業に採用されています。このサイバーセキュリティ企業は、機械学習を採用して、消費されるデータの異常として危険なイベントを分類できるようなシステムを開発したいと考えています。さらにこの企業は、発見した結果をデータレイクに保存し、その後の処理や分析に役立てたいと考えています。\n\nこれらのタスクを完了するために最も効果的な方法はどれですか？",
            "options": {
                "A": "Amazon Kinesis Data Firehoseを使用してデータをインジェストし、Amazon Kinesis Data AnalyticsのRandom Cut Forest（RCF）を使用して異常検知を行います。その後、Kinesis Data Firehoseを使用して、結果をAmazon S3にストリーミングする",
                "B": "Amazon EMRを使用してApache Spark Streamingにデータを取り込み、Spark MLlib with k-meansを使用して異常検知を行います。その後、データレイクとしてAmazon EMRを使用したApache Hadoop Distributed File System（HDFS）にレプリケーション係数3で結果を保存する",
                "C": "データをインジェストし、Amazon S3に保存する。AWS BatchとAWS Deep Learning AMIを使用して、Amazon S3内のデータに対してTensorFlowを使用してk-meansモデルをトレーニングする",
                "D": "データをインジェストして、Amazon S3に保存する。オンデマンドでトリガーされるAWS Glueジョブに、新しいデータを変換させる。その後、Amazon SageMaker内の内蔵Random Cut Forest（RCF）モデルを使用して、データの異常を検出する"
            },
            "correct_answer": [
                "A"
            ],
            "explanation": "異常検知を行いつつ、後続のデータレイクにデータを保存することのできるサービスを選定する必要があります。\n\nAmazon Kinesis Data Analytics for SQL Applicationsでは、標準的なSQLを使用してストリーミングデータを処理・分析することができます。このサービスでは、時系列分析の実行、リアルタイム・ダッシュボードのフィード、リアルタイム・メトリクスの作成のために、ストリーミング・ソースに対して強力なSQLコードを素早くオーサリングして実行することができます。\n\nAmazon Kinesis Data AnalyticsのRANDOM_CUT_FOREST機能は、データストリームの異常を検出します。\n\nしたがって正解は、以下の通りです：\n- Amazon Kinesis Data Firehoseを使用してデータをインジェストし、Amazon Kinesis Data AnalyticsのRandom Cut Forest（RCF）を使用して異常検知を行います。その後、Kinesis Data Firehoseを使用して、結果をAmazon S3にストリーミングする。選択肢B、C、Dはリアルタイム処理や異常検知に最適ではありません。",
            "references": [
                "https://docs.aws.amazon.com/ja_jp/quicksight/latest/user/what-is-random-cut-forest.html"
            ]
        },
        {
            "category": "モデリング",
            "question": "あるデータサイエンティストが、分類タスクのためにニューラルネットワークのミニバッチトレーニングを行っているときに、トレーニングの精度が振動していることを発見しました。この問題の原因として最も可能性が高いのは、次のうちどれでしょうか？",
            "options": {
                "A": "データセットのクラス分布が不均衡である",
                "B": "データセットのシャッフリングは無効である",
                "C": "バッチサイズが大きすぎる",
                "D": "学習率が非常に高い"
            },
            "correct_answer": [
                "D"
            ],
            "explanation": "エポックごとの精度の振動は学習率が主な原因です。学習率が高すぎると重みの更新幅が大きくなり、損失や精度が発散する可能性があります。このため、正しい対処法としては学習率を小さくすることが有効です。選択肢A、B、Cは振動の直接的な原因とはなりにくいです。"
        },
        {
            "category": "モデリング",
            "question": "機械学習のスペシャリストは、機械学習パイプラインを使用して各クライアントの訪問に関する分析を行いたいと考えている企業でモデルを開発しています。 データはAmazon Kinesis Data Streamsを使用して1秒間に最大100トランザクションの割合で取り込み、JSONデータblobのサイズは100KBでなければなりません。 このデータを効率的に取り込むために、スペシャリストがKinesis Data Streamsで採用すべきシャードの最小数は何ですか？",
            "options": {
                "A": "1シャード",
                "B": "10シャード",
                "C": "100シャード",
                "D": "1,000シャード"
            },
            "correct_answer": [
                "B"
            ],
            "explanation": "1シャードあたりのキャパシティをもとに、今回のケースで必要なシャード数を求めます。Kinesis Data Streamsの1つのシャードは1MB/秒、1,000レコード/秒のインジェストが可能です。 今回のデータ取り込みボリュームは100KB * 100 = 10 MBであるため、10個のシャードが必要です。したがって正解は10シャードです。選択肢A、C、Dはキャパシティの要件を満たしていません。",
            "references": [
                "https://docs.aws.amazon.com/ja_jp/streams/latest/dev/key-concepts.html"
            ]
        },
        {
            "category": "モデリング",
            "question": "機械学習のスペシャリストが、セキュリティを最も重視する政府機関のユーザーを対象に、PoCに取り組んでいます。このスペシャリストは、Amazon SageMakerを使用して、画像分類アプリケーション用の畳み込みニューラルネットワーク（CNN）モデルをトレーニングしています。スペシャリストは、トレーニングコンテナに置かれた悪意のあるプログラムによる不正なアクセスや遠隔地のホストへのデータ漏洩からデータを保護したいと考えています。次のアクションのうちどれが最も安全な保護を与えるでしょうか？",
            "options": {
                "A": "SageMakerの実行ロールからAmazon S3のアクセス権限を削除する",
                "B": "CNNモデルの重みを暗号化する",
                "C": "トレーニングデータセットと検証データセットを暗号化する",
                "D": "トレーニングジョブのネットワーク分離を有効にする"
            },
            "correct_answer": [
                "D"
            ],
            "explanation": "アウトバウンドの通信を遮断するための措置を行うことで、トレーニングコンテナ内に悪意あるプログラムがあったとしても外部への情報漏洩を防止することができます。ネットワーク分離を有効にすると、コンテナはAmazon S3などの他のAWSサービスに対しても、アウトバウンドのネットワークコールを行うことができなくなります。さらに、コンテナ実行環境からAWSのクレデンシャルを利用することはできません。複数のインスタンスを持つトレーニングジョブの場合、ネットワークのインバウンドおよびアウトバウンドトラフィックは、各トレーニングコンテナのピアに制限されます。SageMakerは、トレーニングまたは推論コンテナから分離して、SageMaker実行ロールを使用してAmazon S3に対してダウンロードおよびアップロード操作を実行することに変わりはありません。したがって正解は、以下の通りです。 - トレーニングジョブのネットワーク分離を有効にする。選択肢A、B、Cは外部への情報漏洩を防ぐための直接的な対策ではありません。"
        },
        {
            "category": "モデリング",
            "question": "機械学習のスペシャリストが、機械学習の分類モデルを比較・評価する際によく活用すべき指標はどれでしょうか。",
            "options": {
                "A": "リコール",
                "B": "誤判定率",
                "C": "平均絶対%誤差（MAPE）",
                "D": "ROC曲線下面積（AUC）"
            },
            "correct_answer": [
                "D"
            ],
            "explanation": "AUC（Area Under the Curve）とは、分類問題におけるROC曲線を作成した時に、グラフの曲線より下の部分の面積のことを言います。AUCは0から1までの値をとり、値が1に近いほど判別能力が高いことを示し、判別能力がランダムであるとき、AUC = 0.5となります。 AUCは、分類問題でPrecisionとRecallのバランスを考慮した包括的な評価をしたい場合に、モデルの評価指標として使用することができます。 Recallは分類問題における一つの指標ですが、一部の特殊なケースを除いて、より総合的な評価を行えるという観点でAUCの方がより適切な指標となります。 したがって正解は、以下の通りです。 - ROC曲線下面積（AUC）。選択肢A、B、Cは分類モデルの総合的な評価には不十分です。"
        },
        {
            "category": "モデリング",
            "question": "データサイエンティストは、各患者とその治療計画に関するデータを用いて、将来の健康状態を予測する機械学習モデルを構築しています。このモデルは、連続値を予測する必要があります。提供されたデータセットには、4,000人の患者のラベル付けされた転帰が含まれています。この研究では、加齢に伴って悪化することが知られている特定の症状を持つ65歳以上のグループを調査しました。初期のモデルは満足のいくものではありませんでした。データサイエンティストは、基礎データを調査しているときに、4,000人の患者のうち、450人の年齢が0であることを発見しました。この状況を改善するために、データサイエンティストは何をすべきでしょうか？",
            "options": {
                "A": "データセットから、年齢が0に設定されているレコードをすべて削除する",
                "B": "年齢が0に設定されているレコードの年齢フィールドの値を、データセットの平均値または中央値に置き換える",
                "C": "データセットから年齢の特徴を削除し、残りの特徴を使ってモデルを学習する",
                "D": "k-meansクラスタリングを使って、欠損した特徴を処理する"
            },
            "correct_answer": [
                "D"
            ],
            "explanation": "欠損値の特性などに応じて以下のような手法を使い分ける必要があります。今回の場合は、欠損値を含むレコードが11.25％（450/4000）もある場合の正しい欠損値処理の方法を考える必要があります。レコードを削除するのは適切ではなく、k-means法を用いると、最近似のレコードを発見し、その値を用いて欠損値補完をすることができます。そのため、正解はk-meansクラスタリングを使って、欠損した特徴を処理するです。選択肢Aはデータ損失が大きく、選択肢Bはバイアスを生む可能性があり、選択肢Cは重要な特徴を失う可能性があります。"
        },
        {
            "category": "モデリング",
            "question": "機械学習のスペシャリストが、写真の分類のための深層学習モデルを作成しました。一方、スペシャリストはオーバーフィッティングの問題に遭遇し、トレーニングとテストの精度がそれぞれ99％と75％になってしまいました。スペシャリストはこの状況にどのようにアプローチすべきか、また根本的な原因は何か？",
            "options": {
                "A": "最適化プロセスがローカルミニマムでトラップされたため、学習率を上げる必要がある",
                "B": "モデルが十分に一般化されていないため、全結合層でのドロップアウト率を上げる必要がある",
                "C": "モデルの複雑さが足りないので、全結合層の次の密な層の次元数を増やす必要がある",
                "D": "グローバル・ミニマムに到達する前に最適化プロセスが終了してしまったため、エポック数を増やす必要がある"
            },
            "correct_answer": [
                "B"
            ],
            "explanation": "このモデルは過剰適合であり、トレーニングデータセットに対する性能が良いが、テストデータセットに対する性能が低い状態です。モデルが汎化性能を向上させるためには、ドロップアウトを増やす、正則化の制約を増やすなどの手法が効果的です。したがって、正解は『モデルが十分に一般化されていないため、全結合層でのドロップアウト率を上げる必要がある』です。選択肢AとDはオーバーフィッティングの解決策ではなく、選択肢Cは複雑さを増すだけです。"
        },
        {
            "category": "モデリング",
            "question": "データサイエンティストは、線形回帰モデルを開発し、導き出されたp値を用いて各係数の統計的有意性を評価しています。データサイエンティストは、データセットの特徴の大部分が規則的に分布していることを観察しています。画像は、データセットの1つの特徴をプロットしたものです。線形回帰モデルの統計的仮定が満たされていることを確認するために、データサイエンティストはどのような変換を行うべきでしょうか？",
            "options": {
                "A": "指数変換",
                "B": "対数変換",
                "C": "多項式変換",
                "D": "Sinusoidal（正弦波）変換"
            },
            "correct_answer": [
                "B"
            ],
            "explanation": "統計学において、正偏差（または右偏差）分布とは、ほとんどの値が分布の左側に集まり、分布の右尾が長くなっているタイプの分布のことです。正偏差分布の場合、最も一般的な変換は対数変換です。対数変換は、分布の歪みを軽減し、データを正規分布に近づけることができます。選択肢A、C、Dは正偏差の歪みを軽減するのに適していません。",
            "references": [
                "https://best-biostatistics.com/summary/log-transformed.html"
            ]
        },
        {
            "category": "モデリング",
            "question": "このグラフは、ニューラルネットワークの学習損失と検証損失をエポック数に対してプロットしたものです。次のようなネットワークが学習されています。- 2つのDense層、1つの出力ニューロン - 各層に100個のニューロン - 30エポック。検証セットにおけるモデルの精度を向上させるために利用できる戦略はどれか？",
            "options": {
                "A": "アーリーストッピング",
                "B": "適切なシードによる重みのランダムな初期化",
                "C": "エポックの数を増やす",
                "D": "100個のニューロンで別の層を追加"
            },
            "correct_answer": [
                "A"
            ],
            "explanation": "損失値の推移を見ると、エポック数が24になったあたりからテストセットの性能が悪化していることがわかります。これは、トレーニングデータに対して過剰適合してしまっているためです。この問題を避けるための学習の戦略として、テストセットの予測性能が最大になる点で学習をストップしたいと考えます。これをアーリーストッピングと呼びます。選択肢Bは初期化の問題であり、選択肢CとDは過剰適合を悪化させる可能性があります。",
            "references": [
                "https://note.com/kiyo_ai_note/n/nac73b50f3b78",
                "https://rin-effort.com/2019/12/29/machine-learning-6/#toc16"
            ]
        },
        {
            "category": "モデリング",
            "question": "ある企業は、既存の過去の販売データを使って住宅の販売価格を予測したいと考えています。販売価格は、この企業のデータセットにおける目標変数です。説明変数には、敷地面積、居住空間と非居住空間の測定値、寝室とバスルームの数、建設年、郵便番号が含まれます。この企業は、多変量線形回帰を用いて住宅販売価格を予測したいと考えています。余計な情報を排除してモデルを単純化するために、機械学習のスペシャリストはどのようなステップを踏むべきでしょうか？",
            "options": {
                "A": "特徴量のヒストグラムを作成し、その標準偏差を算出する。偏差値の高い特徴量を削除する。",
                "B": "特徴量のヒストグラムを作成し、その標準偏差を計算する。分散の低い特徴量を取り除く。",
                "C": "データセットの相互相関を示すヒートマップを作成する。相関係数が低い特徴量を削除する。",
                "D": "すべての特徴のターゲット変数に対する相関チェックを行う。ターゲット変数の相関係数が低い特徴量を削除する。"
            },
            "correct_answer": [
                "B"
            ],
            "explanation": "特徴量選択は、機械学習モデルを使用する際に有効な特徴量の組み合わせを探索するプロセスのことで、以下の利点があります。精度向上、計算コスト削減、汎化性能の向上。分散が低い特徴量は、線形回帰においてほぼ定数項に近いため、除外すべきです。したがって正解は、特徴量のヒストグラムを作成し、その標準偏差を計算する。分散の低い特徴量を取り除く。選択肢Aは偏差の高い特徴量を誤って削除し、選択肢CとDは相関の低い特徴量を誤って削除する可能性があります。"
        },
        {
            "category": "モデリング",
            "question": "機械学習のスペシャリストは、小さなデータセットを使用してクライアントのためにPoCプロダクトを開発し、現在、Amazon SageMakerを使用してAWSでエンドツーエンドのソリューションを構築する準備をしています。Amazon RDSは、過去のトレーニングデータを保存するために使用されます。スペシャリストは、そのようなデータでモデルをトレーニングする際にどのテクニックを使用する必要がありますか？",
            "options": {
                "A": "ノートブック内にSQLデータベースへの接続コードを直接書き、データを取り込む",
                "B": "AWS Data Pipelineを使ってMicrosoft SQL ServerからAmazon S3にデータをプッシュし、ノートブック内にS3のロケーションを提供する",
                "C": "データをAmazon DynamoDBに移動させ、ノートブック内でDynamoDBへの接続を設定してデータを取り込む",
                "D": "AWS DMSを使用してデータをAmazon ElastiCacheに移動し、ノートブック内に接続を設定してデータを取り込み、高速アクセスを実現する"
            },
            "correct_answer": [
                "B"
            ],
            "explanation": "SageMakerで利用可能なデータをRDSから抽出して保存する必要があります。AWS Data Pipelineを使用すると、指定された間隔でAWSのさまざまなコンピューティングサービスおよびストレージサービスのほか、オンプレミスのデータソース間で信頼性の高いデータ処理やデータ移動を行うことを支援するサービスです。SageMakerではトレーニングデータのソースとしてS3を利用することを推奨しているため、RDSのデータをそのまま利用することはできません。そのため、RDSからS3にデータを保存した上で学習をする必要があります。選択肢Aは直接接続が推奨されず、選択肢CとDはS3を介さないため不適切です。"
        },
        {
            "category": "モデリング",
            "question": "Amazon SageMakerでは、機械学習チームが独自の学習アルゴリズムを実行しています。トレーニングアルゴリズムには外部リソースが必要です。チームは、独自のアルゴリズムコードとアルゴリズム固有のパラメータの両方をAmazon SageMakerのトレーニングジョブに含める必要があります。Amazon SageMakerでオーダーメイドのアルゴリズムを作成するために、チームはどのサービスを組み合わせるべきですか？（2つ選択してください。）",
            "options": {
                "A": "AWS Secrets Manager",
                "B": "AWS CodeStar",
                "C": "Amazon ECR",
                "D": "Amazon ECS",
                "E": "Amazon S3"
            },
            "correct_answer": [
                "C",
                "E"
            ],
            "explanation": "SageMakerでカスタムアルゴリズムのトレーニングを行うためには、コード管理とアーティファクト（モデル）管理が必要です。コード管理にはECRを使用し、モデル出力先としてS3を活用することができます。推論画像とモデルのアーティファクトの場所を提供するオプションに、次の情報を入力します。推論イメージは、Amazon ECRにDockerコンテナとして保存する必要があります。また、モデルアーティファクトはS3内の場所に保存します。選択肢Aは機密情報管理に特化しており、選択肢BとDはコード管理に適していません。",
            "references": [
                "https://aws.amazon.com/jp/blogs/news/sagemaker-custom-containers-pattern-inference/"
            ]
        },
        {
            "category": "モデリング",
            "question": "オンラインショッピングサイトの顧客データを機械学習のスペシャリストが収集しています。データには、人口統計学的情報、過去の訪問履歴、周辺地域の情報などが含まれています。機械学習のスペシャリストは、Webサイトのサービスやレコメンデーション機能を向上させるために、顧客の購買行動や好み、トレンドを特定する機械学習モデルを開発する責任があります。どのような開発方針を提案すべきでしょうか？",
            "options": {
                "A": "与えられたデータセットに対して、潜在的ディリクレ配分（LDA）アルゴリズムを用いて、顧客データベースのパターンを特定する。",
                "B": "顧客データベースのパターンを識別するための、最低3層の層とランダムな初期重みを持つニューラル・ネットワークを使用する。",
                "C": "顧客データベースのパターンを識別するための、ユーザーのインタラクションと関連関係に基づく協調的フィルタリングを使用する。",
                "D": "ランダムなサブサンプルを用いたRCF（Random Cut Forest）により、顧客データベースのパターンを識別する。"
            },
            "correct_answer": [
                "C"
            ],
            "explanation": "過去の顧客データを元にしたレコメンデーションという要件で、適切な手法を選択する必要があります。協調フィルタリングは（ユーザー、アイテム、評価）のタプルに基づいています。そのため、コンテンツベースのフィルタリングとは異なり、他のユーザーの経験を活用します。協調フィルタリングの主なコンセプトは、（観察されたユーザーとアイテムのインタラクションに基づいて）似たようなテイストを持つユーザーは、見たことのないアイテムに対しても似たようなインタラクションをする可能性が高い、というものです。\n\nコンテンツベースのフィルタリングと比較して、協調フィルタリングは、多様性（推奨されるアイテムがどれだけ似ていないか）、セレンディピティ（成功した推奨や関連性のある推奨品がどれだけ意外性があるかを示す指標）、新規性（推奨されるアイテムがどれだけユーザにとって未知であるか）について、より良い結果をもたらします。選択肢Aはトピックモデリングに特化しており、選択肢Bは過剰な複雑さを持ち、選択肢Dは異常検知に適しています。"
        },
        {
            "category": "モデリング",
            "question": "データサイエンティストは、Amazon Forecastを利用して、ある小売会社の在庫需要の予測モデルを開発したいと考えています。この会社は、Amazon S3バケットに商品の過去の在庫需要データを含む.csvファイルを格納しました。以下の表は、データセットの代表的なサンプルを含んでいます。データサイエンティストは、Amazon Forecastを利用できるようにするためにどういった変換をすべきでしょうか？",
            "options": {
                "A": "AWS GlueのETLジョブを使用して、データセットをターゲット時系列データセットとアイテムメタデータデータセットに分離する。両方のデータセットを.csvファイルとしてAmazon S3にアップロードする。",
                "B": "Amazon SageMakerのJupyterノートブックを使用して、データセットを関連時系列データセットとアイテムメタデータデータセットに分離する。両方のデータセットをAmazon Auroraにテーブルとしてアップロードする。",
                "C": "AWSバッチジョブを使用して、データセットをターゲット時系列データセット、関連時系列データセット、およびアイテムメタデータデータセットに分離する。それらをローカルマシンからForecastに直接アップロードする。",
                "D": "Amazon SageMakerのJupyterノートブックを使用して、データを最適化されたprotobuf recordIOフォーマットに変換する。このフォーマットのデータセットをAmazon S3にアップロードする。"
            },
            "correct_answer": [
                "A"
            ],
            "explanation": "Amazon Forecastで予測を行うために、データ形式も含む変換のための前処理を行う適切なサービスを選択する必要があります。AWS Glueを用いることでCSVファイルのETL変換を行うことができます。GlueのETLジョブは、データセットを時系列データセット、アイテムメタデータセットに分離することなどを自動化できます。また、マネージドサービスのため、コンピューティングリソースの管理等も必要なく、運用などによるオーバーヘッドを最小化できます。なお、Forecastはカンマ区切り値（CSV）ファイル形式のみをサポートしています。タブ、スペース、コロン、その他の文字を使用して値を区切ることはできません。選択肢BはAuroraが不適切で、選択肢Cは直接アップロードが非推奨で、選択肢Dはフォーマットが不適切です。"
        },
        {
            "category": "モデリング",
            "question": "あるゲーム会社は、プレイヤーが無料で登録でき、特定の機能を利用するためには有料会員となるオンラインゲームを導入しました。この会社は、新規ユーザーが1年以内にプレミアム会員になるかどうかを予測できる自動システムを開発したいと考えています。この企業は、100万人の消費者から集めたデータをラベル化してまとめています。\n\nトレーニングデータセットには、1,000個のポジティブなサンプル（1年以内に有料会員となったユーザー）と、999,000個のネガティブなサンプル（有料会員にはならなかったユーザー）が含まれています。各データサンプルには、ユーザーの年齢、デバイス、場所、プレイ行動など、ユーザーに関する200の属性が含まれています。\n\nデータサイエンスチームは、このデータセットにランダムフォレストモデルを構築し、トレーニングセットでは99%以上の精度に収束しました。しかし、テストデータセットでの予測精度は十分ではありませんでした。",
            "options": {
                "A": "ランダムフォレストにディープツリーを追加して、モデルがより多くの特徴を学習できるようにする",
                "B": "テストデータセットのサンプルのコピーをトレーニングデータセットに含める",
                "C": "正のサンプルを複製し、複製したデータに少量のノイズを加えることで、より多くの正のサンプルを生成する",
                "D": "偽陰性の方が偽陽性よりもコスト値への影響が大きくなるようにコスト関数を変更する",
                "E": "偽陽性が偽陰性よりもコスト値に与える影響が大きくなるように、コスト関数を変更する"
            },
            "correct_answer": [
                "C",
                "D"
            ],
            "explanation": "起きている問題は、有料会員のデータが圧倒的に少ないデータセットであることに起因しています。そのため、不均衡データセットに対する改善策を実施する必要があります。\n\nまず一つ目の対策は、データセットを追加で生成し、不均衡の度合いを緩和することです。SMOTEなどのデータ生成アルゴリズムを用いて、有料会員のデータサンプルを作成することによって、モデルのテストセットでの性能が向上する可能性があります。\n\n二つ目の対策は、学習時の各クラスのコスト値の重みを変更することです。今回であれば、偽陰性（正しくは有料会員にならない）がよりも偽陽性（正しくは有料会員になる）の方が起こりづらく、また重視する必要があります。コスト関数を、偽陰性の方がコスト値が大きくなるように変更することで、モデルの性能が向上する可能性があります。選択肢Aは過学習のリスクがあり、選択肢Bはデータリークの可能性があり、選択肢Eは不適切なコスト調整です。"
        },
        {
            "category": "モデリング",
            "question": "ある都市では、大気汚染の影響を軽減するために、大気の質を監視したいと考えています。機械学習のスペシャリストは、その都市の大気質を、汚染物質の単位（ppm）で、翌々日に予測する必要があります。このプロトタイプの性能は、前年の日次データにしかアクセスできません。Amazon SageMakerで最適な結果を得るために最も可能性の高いモデルはどれですか？",
            "options": {
                "A": "Amazon SageMakerのk-Nearest-Neighbors（kNN）アルゴリズムを、predictor_typeがregressorの1年間のデータからなる単一の時系列に使用する",
                "B": "Amazon SageMakerのRandom Cut Forest（RCF）を、データの全年からなる単一の時系列に使用する",
                "C": "予測子のタイプを持つデータの全年からなる単一の時系列にAmazon SageMaker Linear Learnerアルゴリズムを使用する",
                "D": "予測子のタイプが分類子であるデータのフルイヤーからなる単一の時系列上でAmazon SageMaker Linear Learnerアルゴリズムを使用する"
            },
            "correct_answer": [
                "C"
            ],
            "explanation": "Amazon SageMakerの線形学習者アルゴリズムは、トレーニング、検証、テストの3つのデータチャンネルをサポートしています。アルゴリズムは、エポック毎に検証損失を記録し、検証データのサンプルを使用して、最適なモデルを検証して選択します。\n\nシナリオでは、都市の大気汚染の原因である特定の物質濃度（NO2）の濃度を予測するモデルを構築しなければなりません。一般的な時系列アプローチでは、前日のNO2値とデータの他の特徴（湿度、風向、気圧）に基づいて目標値（NO2）を予測することを目的とします。この目的に適したモデルとして、回帰分析が可能なSageMaker Linear Learnerアルゴリズムが推奨されます。選択肢AはkNNが時系列に適しておらず、選択肢Bは異常検知に特化しており、選択肢Dは分類に適しています。"
        },
        {
            "category": "モデリング",
            "question": "機械学習エンジニアが回帰モデルをトレーニングしましたが、最初の反復では最適化が必要です。スペシャリストは、モデルがターゲットを過大評価しているか、過小評価しているかを理解する必要があります。スペシャリストは、ターゲットを過大評価しているか、過小評価しているかを判断するためにどのオプションを使用できますか？",
            "options": {
                "A": "混同行列",
                "B": "残余プロット",
                "C": "曲線下面積（AUC）",
                "D": "ROC 曲線"
            },
            "correct_answer": [
                "B"
            ],
            "explanation": "残余プロットを使用することで、予測値と実際の値の差異を視覚化し、モデルが過大評価または過小評価をしている部分を特定することができます。選択肢Aは分類問題に適しており、選択肢CとDは分類モデルの評価に使用されます。"
        },
        {
            "category": "モデリング",
            "question": "機械学習エンジニアは、1,000 レコードと 50 の特徴量を持つデータセットに線形最小二乗回帰モデルを適用しています。ML スペシャリストは、トレーニングの前に、2 つの特徴量が完全に線形に依存していることに気づきました。なぜこれが線形最小二乗回帰モデルの問題になるのでしょうか？",
            "options": {
                "A": "最適化中に損失関数を変更し、トレーニング中に失敗する可能性があります。",
                "B": "最適化中に特異行列が作成される可能性があり、一意のソリューションを定義できません。",
                "C": "トレーニング中にバックプロパゲーションアルゴリズムが失敗する可能性があります。",
                "D": "データ内に非線形の依存関係が導入され、モデルの線形の仮定が無効になる可能性があります。"
            },
            "correct_answer": [
                "B"
            ],
            "explanation": "最小二乗法では、特徴量間に完全な線形依存関係が存在すると、特異行列が生じ、一意の解が得られなくなる問題が発生します。選択肢Aは損失関数の問題ではなく、選択肢Cは回帰モデルに適用されず、選択肢Dは非線形依存の問題ではありません。",
            "references": [
                "https://www.youtube.com/watch?v=Zz1sgYxrA-k"
            ]
        },
        {
            "category": "モデリング",
            "question": "大手メーカーは、次の製品を販売しています。\n\n- 34 種類の歯磨き粉\n- 48 種類の歯ブラシ\n- 43 種類のうがい薬\n\nこれらすべての製品の全販売履歴は、Amazon S3 で入手できます。現在、同社はこれらの製品の需要を予測するために、カスタムビルドの自己回帰和分移動平均 (ARIMA) モデルを使用しています。同社は、間もなく発売される新製品の需要を予測したいと考えています。\n\n機械学習エンジニアはどのソリューションを適用する必要がありますか？",
            "options": {
                "A": "カスタム XGBoost モデルをトレーニングして、新製品の需要を予測します。",
                "B": "Amazon SageMaker k-means クラスタリングアルゴリズムをトレーニングして、新製品の需要を予測します。",
                "C": "Amazon SageMaker DeepAR アルゴリズムをトレーニングして、新製品の需要を予測します。",
                "D": "カスタム ARIMA モデルをトレーニングして、新製品の需要を予測します。"
            },
            "correct_answer": [
                "C"
            ],
            "explanation": "Amazon SageMaker DeepAR アルゴリズムは、時系列データを使用して将来の需要を予測するための強力なツールで、新製品の需要予測にも適しています。選択肢Aは時系列に特化しておらず、選択肢Bはクラスタリングに適しており、選択肢Dは新製品の予測に適していません。"
        },
        {
            "category": "モデリング",
            "question": "あるデータサイエンティストは、トレーニングプロセス中に複数のパラメータを変更することによるモデルの最適化に取り組んでいます。このデータサイエンティストが観察したところ、同じパラメータを使用して複数回実行すると、損失関数は異なるとはいえ、安定した値に収束することが明らかになりました。トレーニングプロセスを改善するために、このデータサイエンティストはどうすればよいですか。",
            "options": {
                "A": "学習率を大きくする。バッチサイズは変更しない。",
                "B": "学習率を小さくする。バッチサイズを下げる。",
                "C": "学習率を小さくする。バッチサイズは変更しない。",
                "D": "学習率は変更しない。バッチサイズを上げる。"
            },
            "correct_answer": [
                "B"
            ],
            "explanation": "おそらく損失関数曲線が大きく、局所的に極小値が複数あるため、トレーニングの実行が妨げられています。バッチサイズを下げると、データサイエンティストが局所的な最小値の問題を乗り越える確率が上がります。学習率を小さくすると、全体的な損失関数の最小値の過度の変動を回避できます。これに関する説明については、こちらのリンク先にある論文を参照してください。",
            "references": [
                "https://www.youtube.com/watch?v=vgmZij4iaVw"
            ]
        },
        {
            "category": "探索的データ分析",
            "question": "データサイエンティストがリアルタイムストリーミングデータに対して、サーバレスのインジェスチョンおよび分析ソリューションを作成しようとしています。出力は既存のビジネスインテリジェンスダッシュボードに統合されます。この問題に最適な解決策を尋ねています。",
            "options": {
                "A": "AWS Glue Data Catalogを使用して、データを収集し、Athenaでクエリを実行。",
                "B": "Amazon EMRを使用してデータを処理し、S3に保存。",
                "C": "JSONレコードをS3に格納し、Lambda関数で処理。",
                "D": "データをApache Kafkaを介して収集し、リアルタイムで処理。"
            },
            "correct_answer": [
                "C"
            ],
            "explanation": "JSONレコードをS3に保存し、Lambda関数でリアルタイムに処理することで、サーバレスで効率的なデータインジェスチョンおよび分析が可能です。これは、リアルタイムストリーミングデータに対して最も適したサーバレスソリューションです。選択肢A（Glue Data CatalogとAthena）はバッチ処理に適していますが、リアルタイム処理には適していません。選択肢B（EMR）はサーバレスではありません。選択肢D（Apache Kafka）はストリーミングデータの収集には適していますが、完全なサーバレスソリューションではありません。"
        },
        {
            "category": "探索的データ分析",
            "question": "ある企業が毎日100TBの予測データを生成し、可視化レポートをビジネスチームに提出する必要があります。最も労力の少ない方法を尋ねています。",
            "options": {
                "A": "Amazon EMRワークフローを使用してプレシジョンリコールデータを生成し、Amazon S3に保存。",
                "B": "Amazon QuickSightを使用してプレシジョンリコールデータを可視化し、結果をビジネスチームに共有。",
                "C": "Amazon Athenaでクエリを実行してデータをビジネスチームに共有。",
                "D": "Amazon ESを使用してデータをビジネスチームに共有。"
            },
            "correct_answer": [
                "B"
            ],
            "explanation": "Amazon QuickSightを使用することで、大量データの可視化と結果の共有が容易に行えます。これは最も労力の少ない方法です。選択肢A（Amazon EMR）は大規模データ処理には適していますが、可視化と共有に追加の労力が必要です。選択肢C（Amazon Athena）はクエリ実行には適していますが、可視化機能が限られています。選択肢D（Amazon ES）は検索と分析には強力ですが、ビジネスユーザー向けの可視化と共有機能が不足しています。"
        },
        {
            "category": "探索的データ分析",
            "question": "雇用データの分析を行っています。データセットは、10の異なる特徴量を持つ個人の約1,000万件の観測データで構成されています。データサイエンティストは、予備調査の段階で、収入と年齢の分布が典型的ではないことを発見しました。収入レベルの分布は右偏差を示しており、また、年齢分布は同じく右偏差を示しています。\n\nデータサイエンティストは、歪んでしまったデータに対して、どのような特徴変換を行うことができますか？（2つ選択）",
            "options": {
                "A": "クロスバリデーション",
                "B": "数値ビニング",
                "C": "高次元多項式変換",
                "D": "対数変換",
                "E": "ワンホットエンコーディング"
            },
            "correct_answer": [
                "B",
                "D"
            ],
            "explanation": "歪んだデータに対して、対数変換とビニングが有効な特徴変換になります。対数変換によりデータの分布を正規分布に近づけることができ、数値ビニングはデータを特定の範囲に分割して分析のしやすさを向上させます。選択肢Aはモデルの評価手法であり、選択肢Cは次元を増やすものであり、選択肢Eはカテゴリ変数のエンコーディングであり、いずれも歪みの修正には不向きです。"
        },
        {
            "category": "探索的データ分析",
            "question": "あるビジネスでは、毎日100テラバイトの予測値を生成する機械学習予測サービスを運営しています。機械学習のスペシャリストは、予測結果から得られた日次のPrecision-Recall曲線を可視化し、読み取り専用のフォーマットでビジネスチームに提供しなければなりません。\n\nコーディングの作業量が最も少ないのはどの方法か？",
            "options": {
                "A": "Precision-Recallデータを生成するために毎日Amazon EMRワークフローを実行し、その結果をAmazon S3に保存する。ビジネスチームにS3への読み取り専用のアクセス権を与えます。",
                "B": "Amazon QuickSightで毎日のPrecision-Recallデータを生成し、ビジネスチームと共有するダッシュボードで結果を公開する。",
                "C": "Precision-Recallデータを生成するために毎日Amazon EMRワークフローを実行し、Amazon S3に結果を保存する。Amazon QuickSightで結果を可視化し、ビジネスチームと共有するダッシュボードで公開する。",
                "D": "Amazon ESで毎日Precision-Recallデータを生成し、ビジネスチームと共有するダッシュボードで結果を公開する。"
            },
            "correct_answer": [
                "C"
            ],
            "explanation": "大量データの処理にはAmazon EMRが適しており、データをS3に保存し、Amazon QuickSightで可視化してダッシュボードで共有することで、エンドユーザー向けの効率的な可視化と共有が可能です。選択肢AはS3へのアクセス権を与えるだけで可視化がありません。選択肢BはQuickSightのみでデータ生成が含まれていません。選択肢DはESを使用するもので、QuickSightの可視化機能を活用できません。"
        },
        {
            "category": "探索的データ分析",
            "question": "ある会社のデータサイエンティストは、Amazon SageMakerのノートブックインスタンスを使用してデータの調査と分析を行います。これには、Amazon SageMaker上でネイティブにアクセスできないいくつかのPythonパッケージをノートブックインスタンスにインストールする必要があります。機械学習のスペシャリストは、データサイエンティストの必須パッケージがノートブックインスタンス上で自動的にアクセスできることをどのように保証できますか？",
            "options": {
                "A": "基盤となるAmazon EC2インスタンスにAWS Systems Manager Agentをインストールし、Systems Manager Automationを使用してパッケージインストールコマンドを実行する。",
                "B": "実行するパッケージインストールコマンドを含むセルを持つJupyterノートブックファイル(.ipynb)を作成し、各Amazon SageMakerノートブックインスタンスの/etc/initディレクトリの下にファイルを配置する。",
                "C": "Jupyterノートブックのコンソールからcondaパッケージマネージャを使用して、必要なcondaパッケージをノートブックのデフォルトカーネルに適用する。",
                "D": "パッケージインストールコマンドでAmazon SageMakerのライフサイクル構成を作成し、ノートブックインスタンスにライフサイクル構成を割り当てます。"
            },
            "correct_answer": [
                "D"
            ],
            "explanation": "Amazon SageMakerのライフサイクル構成を使用することで、インスタンスの起動時に指定のスクリプトを実行して必要なパッケージを自動インストールすることが可能です。これにより、ノートブックインスタンスが立ち上がるたびに指定された環境が自動で設定され、データサイエンティストが使用するパッケージが常にインストールされます。選択肢AはSystems Managerの使用に関するもので、SageMakerのライフサイクル構成ほど効率的ではありません。選択肢Bは手動の設定に依存しており、スケーラビリティが限られています。選択肢Cはcondaの使用に関するもので、ライフサイクル構成ほど自動化されていません。"
        },
        {
            "category": "探索的データ分析",
            "question": "ある飛行機のエンジンメーカーでは、200のパフォーマンス指標を時系列で集計しています。エンジニアは、テスト中の重大な製造上の問題をニアリアルタイムで検出する必要があります。また、すべてのデータは今後の大規模な分析で使用できるように保持し続けなければなりません。欠陥をニアリアルタイムで検出するためには、どの戦略が最も効果的でしょうか？",
            "options": {
                "A": "AWS IoT Analyticsを使用して、インジェスト、ストレージ、およびさらなる分析を行う。AWS IoT Analytics内のJupyter notebooksを使用して、異常の分析を実行する",
                "B": "インジェスト、ストレージ、およびさらなる分析にはAmazon S3を使用する。Amazon EMRクラスターを使用して、Apache Spark MLのk-meansクラスタリングを実行し、異常を判断する",
                "C": "インジェスト、ストレージ、およびさらなる分析にAmazon S3を使用する。Amazon SageMakerのRCF（Random Cut Forest）アルゴリズムを使用して異常を判断する",
                "D": "Amazon Kinesis Data Firehoseを取り込みに使用し、Amazon Kinesis Data AnalyticsのRandom Cut Forest（RCF）を使用して異常検知を行います。Kinesis Data Firehoseを使用して、さらなる分析のためにAmazon S3にデータを保存する"
            },
            "correct_answer": [
                "D"
            ],
            "explanation": "ニアリアルタイムで異常検知を行うためには、マネージドサービスを用いた自動的な異常検知サービスが必要です。Kinesis Data Analyticsに組み込まれているRANDOM_CUT_FOREST関数を使用すると、Kinesis Data Streamsに保存されているセンサーデータに対してリアルタイムに異常を検出することができます。処理された異常データは、Kinesis Data Firehoseの配信ストリームにロードされます。後続のフローとしては、さらなる分析のためにデータをAmazon S3に保存することもできます。選択肢Aはリアルタイム処理に適しておらず、選択肢BとCはリアルタイム性が欠けています。"
        },
        {
            "category": "探索的データ分析",
            "question": "ユニグラムとバイグラムの両方を使用した単語頻度 - 逆文書頻度 (tf–idf) 行列が、次の 2 つの文で構成されるテキストコーパスで構築されています。\n1. Please call the number below.\n2. Please do not call us.\nこの tf–idf 行列の次元はどれですか。",
            "options": {
                "A": "(2, 16)",
                "B": "(2, 8)",
                "C": "(2, 10)",
                "D": "(8, 10)"
            },
            "correct_answer": [
                "A"
            ],
            "explanation": "2 つの文、一意のユニグラムが 8 個、一意のバイグラムが 8 個あります。つまり、結果は (2, 16) です。 フレーズは「Please call the number below」と「Please do not call us」です。 それぞれの独立した単語 (ユニグラム) は、「Please」、「call」、「the」、「number」、「below」、「do」、「not」、「us」です。一意のバイグラムは、「Please call」、「call the」、「the number」、「number below」、「Please do」、「do not」、「not call」、「call us」です。 tf–idf のベクトル化については、こちらのリンク先を参照してください。"
        },
        {
            "category": "機械学習の実装と運用",
            "question": "機械学習スペシャリストが、ある企業の売上改善のためのシステムを設計しています。この目的は、企業が持つユーザーの行動や製品の嗜好に関する大量の情報を使用して、他のユーザーとの類似性に基づき、ユーザーが望む製品を予測することです。スペシャリストは、この目的を達成するために何をすべきかという質問です。",
            "options": {
                "A": "Apache Spark MLを使用し、Amazon EMR上にコンテンツベースのフィルタリング推奨エンジンを構築する。",
                "B": "Apache Spark MLを使用し、Amazon EMR上に協調フィルタリング推奨エンジンを構築する。",
                "C": "Apache Spark MLを使用し、Amazon EMR上にモデルベースのフィルタリング推奨エンジンを構築する。",
                "D": "Apache Spark MLを使用し、Amazon EMR上に組み合わせフィルタリング推奨エンジンを構築する。"
            },
            "correct_answer": [
                "B"
            ],
            "explanation": "ユーザー間の類似性に基づく推奨には協調フィルタリングが適しており、Apache Spark MLを用いてAmazon EMR上に協調フィルタリング推奨エンジンを構築するのが最適です。協調フィルタリングとは、ユーザーの過去の行動や評価を基に、他のユーザーの行動を参考にして推奨を行う手法です。Apache Spark MLは、分散処理を可能にする機械学習ライブラリであり、Amazon EMRは、クラウド上で大規模データ処理を行うためのサービスです。他の選択肢が間違っている理由は以下の通りです。\n\nA: コンテンツベースのフィルタリングは、ユーザーの過去の行動に基づく推奨には適していません。\n\nC: モデルベースのフィルタリングは、協調フィルタリングとは異なるアプローチであり、ユーザー間の類似性を直接利用しません。\n\nD: 組み合わせフィルタリングは、複数のフィルタリング手法を組み合わせるものであり、特定の手法に特化していないため、ユーザー間の類似性に基づく推奨には最適ではありません。"
        },
        {
            "category": "機械学習の実装と運用",
            "question": "機械学習スペシャリストが、小規模なデータサンプルを使用して企業のための概念実証を完了しました。今度は、Amazon RDSに保存されている履歴トレーニングデータを使用して、最終的なソリューションを実装したいと考えています。データのトレーニングを行うためのアプローチを尋ねています。",
            "options": {
                "A": "ノートブック内でSQLデータベースに直接接続してデータを取得。",
                "B": "AWS Data Pipelineを使用してSQL ServerからAmazon S3にデータをプッシュし、S3ロケーションをノートブックで提供。",
                "C": "データをAmazon DynamoDBに移動し、ノートブックで接続を設定してデータを取得。",
                "D": "データをAmazon ElastiCacheに移動し、ノートブックで接続を設定してデータを取得し、高速アクセス。"
            },
            "correct_answer": [
                "B"
            ],
            "explanation": "AWS Data Pipelineを使用してRDSからS3にデータを移行し、S3をSageMakerノートブックからアクセスすることで、効率的にデータを利用できます。これは大規模データセットを扱う際の標準的なアプローチです。他の選択肢が間違っている理由は以下の通りです。\n\nA: ノートブック内でSQLデータベースに直接接続する方法は、データのスケーラビリティやパフォーマンスに制約があり、大規模データセットには適していません。\n\nC: Amazon DynamoDBはNoSQLデータベースであり、SQLデータベースからのデータ移行には適していません。また、データの取得においても制約があります。\n\nD: Amazon ElastiCacheはインメモリデータストアであり、トレーニングデータの保存や取得には適していません。"
        },
        {
            "category": "機械学習の実装と運用",
            "question": "機械学習スペシャリストがオンラインショッピングのウェブサイトの顧客データを受け取ります。データには、年齢、訪問履歴、購入履歴、場所などの情報が含まれています。スペシャリストは、より良いサービスと製品の推奨を提供するために、顧客の購買パターンを識別するための機械学習アプローチを見つける必要があります。推奨される解決策を尋ねています。",
            "options": {
                "A": "Latent Dirichlet Allocation (LDA)を使用して、顧客データの潜在的なテーマを特定する。",
                "B": "深層ニューラルネットワークを使用し、顧客データベース内のパターンを特定するために隠れ層を最小3つ以上持つモデルを作成する。",
                "C": "相関性の高い特徴に基づいて、クラスターを作成し、類似の購買パターンを持つ顧客グループを識別するための協調フィルタリングを実行する。",
                "D": "ランダムフォレスト（Random Cut Forest, RCF）アルゴリズムを使用して、顧客データベース内のパターンを識別する。"
            },
            "correct_answer": [
                "C"
            ],
            "explanation": "協調フィルタリングは、相関性の高い特徴に基づいて顧客をクラスタリングし、類似の購買パターンを持つ顧客グループを特定するのに適しています。これは、オンラインショッピングの推奨システムでよく使用される手法です。他の選択肢が間違っている理由は以下の通りです。\n\nA: LDAは主にテキストデータに適しており、顧客購買パターンの分析には不向きです。\n\nB: 深層ニューラルネットワークは複雑で、解釈が難しい可能性があり、過学習のリスクがあります。\n\nD: RCFは異常検出に特化しており、顧客グループの識別には最適ではありません。"
        },
        {
            "category": "機械学習の実装と運用",
            "question": "機械学習スペシャリストが、大規模な会社と協力して機械学習を使用して顧客をカテゴリーに分け、6か月以内に誰が購入しそうかを予測する手助けをしています。利用可能なデータに基づいて、適切な機械学習アプローチを選択する必要があります。",
            "options": {
                "A": "線形回帰",
                "B": "分類",
                "C": "クラスタリング",
                "D": "強化学習"
            },
            "correct_answer": [
                "B"
            ],
            "explanation": "顧客が購入するかどうかを予測する問題は分類問題であり、分類アプローチが適しています。この場合、6か月以内に購入するかしないかの二値分類になります。他の選択肢が間違っている理由は以下の通りです。\n\nA: 線形回帰は連続値の予測に使用され、二値分類には適していません。\n\nC: クラスタリングはデータをグループ化する手法であり、購入予測には直接的に適用できません。\n\nD: 強化学習はエージェントが環境と相互作用しながら学習する手法であり、購入予測には不適切です。"
        },
        {
            "category": "機械学習の実装と運用",
            "question": "ニューラルネットワークのトレーニング中、データサイエンティストはトレーニングの損失が振動していることに気付きます。この問題の最も可能性の高い原因を尋ねています。",
            "options": {
                "A": "データのクラス分布が不均衡。",
                "B": "データセットが十分に大きくない。",
                "C": "バッチサイズが大きすぎる。",
                "D": "学習率が高すぎる。"
            },
            "correct_answer": [
                "D"
            ],
            "explanation": "学習率が高すぎると、最適化プロセスが不安定になり、損失が振動します。これは、勾配降下法において大きすぎるステップを踏むことで、最適解を行き過ぎてしまうためです。選択肢A（クラス分布の不均衡）は、損失の振動よりも、特定のクラスの予測精度に影響を与える可能性がありますが、直接的に損失の振動を引き起こすわけではありません。選択肢B（データセットのサイズ不足）は、一般的に過学習につながる可能性がありますが、損失の振動の直接的な原因ではありません。むしろ、損失が単調に減少しない原因となる可能性があります。選択肢C（バッチサイズが大きすぎる）は、通常、学習の安定性を高める傾向があり、振動の原因とは考えにくいです。むしろ、バッチサイズが小さすぎる場合に振動が起こりやすくなります。"
        },
        {
            "category": "機械学習の実装と運用",
            "question": "オンライン辞書サイトが、他のコンテンツと似たコンテキストで表示されるウィジェットの評価を行っています。どのアプローチが最適かを尋ねています。",
            "options": {
                "A": "顧客に評価を求めてフィードバックを収集する。",
                "B": "顧客にウィジェットをテストさせ、MTurkを使用してデータを収集する。",
                "C": "ウィジェットの埋め込みテストを実施し、彼らがどれほど効果的かを確認する。",
                "D": "ランダムにウィジェットを埋め込み、結果を分析する。"
            },
            "correct_answer": [
                "C"
            ],
            "explanation": "実際のユーザー環境でウィジェットの効果をテストすることで、直接的なフィードバックを得られます。これにより、実際の使用状況下でのウィジェットの効果を測定できます。ウィジェットの埋め込みテストは、実際のコンテキストでの効果を評価できるため、最も適切なアプローチです。選択肢A（顧客評価）は主観的な意見に頼りすぎる可能性があります。また、実際の使用状況を正確に反映しない可能性があります。選択肢B（MTurkの使用）は、実際のユーザー環境を反映しない可能性があります。MTurkのワーカーは実際のサイト利用者とは異なる行動をとる可能性があります。選択肢D（ランダムな埋め込み）は、コンテキストの類似性を考慮していないため、適切な評価ができない可能性があります。ランダムな配置では、ウィジェットの効果を正確に測定できない可能性があります。"
        },
        {
            "category": "機械学習の実装と運用",
            "question": "機械学習スペシャリストがAmazon SageMakerを使用して、複数のデータサイエンティストが使用するノートブックを設定しています。SageMakerのエンドポイントでCPUやGPUの使用率、ネットワークパフォーマンス、レイテンシーなどをトラッキングするためには、どのサービスが適しているかを尋ねています。",
            "options": {
                "A": "AWS CloudTrail",
                "B": "AWS Health",
                "C": "AWS Trusted Advisor",
                "D": "Amazon CloudWatch",
                "E": "AWS Config"
            },
            "correct_answer": [
                "D"
            ],
            "explanation": "Amazon CloudWatchは、CPU、メモリ、ネットワークパフォーマンス、レイテンシーなどのメトリクスをトラッキングおよび監視するために最適です。CloudWatchは、AWSリソースのリアルタイムモニタリングを提供し、カスタムメトリクスの作成も可能です。SageMakerエンドポイントの詳細なパフォーマンスメトリクスを収集、分析、可視化できます。選択肢A（CloudTrail）はAWS APIコールのログを記録するためのサービスで、パフォーマンスモニタリングには適していません。APIの使用状況は追跡できますが、リソースの使用率やパフォーマンスは追跡できません。選択肢B（AWS Health）はAWSサービスの健全性を確認するためのもので、詳細なメトリクス監視には適していません。サービスの全体的な状態は確認できますが、個別のリソースのパフォーマンスは追跡できません。選択肢C（Trusted Advisor）はAWSベストプラクティスのチェックを行うサービスです。コスト最適化、セキュリティ、パフォーマンス改善の推奨事項を提供しますが、リアルタイムのメトリクス監視には適していません。選択肢E（AWS Config）はAWSリソースの設定変更を追跡するサービスで、パフォーマンスモニタリングには適していません。設定の変更履歴は追跡できますが、リソースの使用率やパフォーマンスは追跡できません。"
        },
        {
            "category": "機械学習の実装と運用",
            "question": "機械学習スペシャリストが、過剰に時間がかかるハイパーパラメータチューニングを行っています。チューニングプロセスを最適化するためには、どの方法が適しているかを尋ねています。",
            "options": {
                "A": "モデルの最も重要なハイパーパラメータのみを調整。",
                "B": "入力データの次元を削減。",
                "C": "シードを調整し、訓練セットを減らす。",
                "D": "使用するコア数を制限し、バッチサイズを減らす。"
            },
            "correct_answer": [
                "A"
            ],
            "explanation": "最も重要なハイパーパラメータに焦点を当てることで、探索空間を狭め、チューニング時間を短縮できます。これにより、効率的にモデルのパフォーマンスを向上させることができます。重要なハイパーパラメータに集中することで、最も影響力のある要素を最適化し、時間を節約できます。選択肢B（次元削減）は、データの前処理として有効ですが、ハイパーパラメータチューニングの直接的な最適化ではありません。次元削減は全体的な学習プロセスを改善する可能性がありますが、チューニングプロセス自体を最適化するものではありません。選択肢C（訓練セットの縮小）は、チューニング時間を短縮する可能性がありますが、モデルの性能に悪影響を与える可能性があります。訓練データを減らすことで、モデルが十分に学習できず、精度が低下する可能性があります。選択肢D（コア数の制限とバッチサイズの縮小）は、計算リソースを制限することになり、チューニングプロセスを遅くする可能性があります。これは最適化というよりも、リソースの制限につながり、効率的なチューニングには適していません。"
        },
        {
            "category": "機械学習の実装と運用",
            "question": "Amazon Pollyを使ってプレーンテキストを音声に変換し、会社のアナウンスメントを発信していますが、現在のアナウンス内容が最新ではなくなってきています。この問題を解決するための方法を尋ねています。",
            "options": {
                "A": "テキストデータを適切に変換する。",
                "B": "音声データの調整を行う。",
                "C": "テキストの発音を改善。",
                "D": "AWS Textractを使用し、最新の内容を生成する。"
            },
            "correct_answer": [
                "A"
            ],
            "explanation": "最新のテキストデータを正しく変換し、Amazon Pollyに入力することで、最新のアナウンスを生成できます。これは、アナウンス内容を更新する最も直接的で効果的な方法です。選択肢B（音声データの調整）は、既に生成された音声を変更することになり、内容の更新には適していません。選択肢C（テキストの発音改善）は、内容の更新ではなく、発音の質の問題に対処するものです。選択肢D（AWS Textractの使用）は、画像からテキストを抽出するサービスであり、この状況には適していません。"
        },
        {
            "category": "機械学習の実装と運用",
            "question": "ゲーム会社が、プレイ無料のゲームで特定の機能に支払いを促すための自動化システムを開発しています。このシステムは、新規ユーザーの支払い行動を予測するために100,000件のサンプルデータを使用していますが、データの不均衡が発生しており、どの修正が有効かを尋ねています。",
            "options": {
                "A": "データセットのサイズを増やして学習を促進する。",
                "B": "クラスの重みを調整して少数派のクラスに対する誤分類の影響を軽減する。",
                "C": "サンプルを増やしてバランスを取る。",
                "D": "誤分類コスト関数を調整して、誤分類の影響を軽減する。"
            },
            "correct_answer": [
                "B"
            ],
            "explanation": "クラスの重みを調整することで、少数派クラスの誤分類の影響を軽減し、不均衡データに対処できます。これにより、モデルは少数派クラスにも適切に注目します。選択肢A（データセット増加）は不均衡を解決しない可能性があります。選択肢C（サンプル増加）はオーバーサンプリングやアンダーサンプリングを示唆していますが、データの質に影響を与える可能性があります。選択肢D（誤分類コスト関数調整）は有効ですが、クラスの重み調整ほど直接的ではありません。"
        },
        {
            "category": "機械学習の実装と運用",
            "question": "データサイエンティストが、データリポジトリを設計して、機械学習で一般的に使用される大量のトレーニングデータを保存しようとしています。スケーラビリティとコスト効率を考慮した最適なストレージソリューションを尋ねています。",
            "options": {
                "A": "S3にデータを保存。",
                "B": "データをAmazon EBSボリュームに保存。",
                "C": "Amazon RDSにデータを保存。",
                "D": "Amazon DynamoDBにデータを保存。"
            },
            "correct_answer": [
                "A"
            ],
            "explanation": "Amazon S3はスケーラビリティとコスト効率に優れ、大量のトレーニングデータの保存に最適です。無制限の容量、低コスト、高い耐久性を提供します。選択肢B（EBS）はEC2インスタンスに接続する必要があり、大規模データには適していません。選択肢C（RDS）は構造化データに適していますが、大量の非構造化データには向いていません。選択肢D（DynamoDB）はNoSQLデータベースで、機械学習の大規模データセット保存には最適ではありません。"
        },
        {
            "category": "機械学習の実装と運用",
            "question": "機械学習スペシャリストが、小売業者向けに製品推奨システムを設計しています。顧客のデータを使用して推奨システムを構築するための最適なアプローチを尋ねています。",
            "options": {
                "A": "AWS GlueとKinesis Data Streamsを使用して、リアルタイムのデータを収集し、Athenaを使って分析する。",
                "B": "AWS GlueとKinesis Firehoseを使用してデータを収集し、SageMakerを使って推奨モデルをトレーニングする。",
                "C": "Amazon AthenaとAWS Glueを使って、データを処理し、リアルタイム推奨モデルを作成する。",
                "D": "Amazon Kinesis Data FirehoseとSageMakerを使って推奨システムを作成する。"
            },
            "correct_answer": [
                "B"
            ],
            "explanation": "AWS GlueとKinesis Firehoseでデータを効率的に収集し、SageMakerで推奨モデルをトレーニングすることで効果的な推奨システムが構築できます。この組み合わせにより、データの収集、処理、モデルのトレーニングが一貫して行えます。選択肢A（Athenaを使用）は分析には適していますが、推奨モデルの構築には適していません。選択肢C（AthenaとGlue）はデータ処理には適していますが、モデルトレーニングの部分が欠けています。選択肢D（FirehoseとSageMaker）はデータ収集と処理の部分が不十分です。"
        },
        {
            "category": "機械学習の実装と運用",
            "question": "ニューヨーク市の公共交通データセットを使用してベイズネットワークを実装している機械学習スペシャリストが、離散変数の確率分布について尋ねています。バスが10分ごとに出発し、バス停で待っているニューヨーカーの数を予測するために適切な確率分布を選択する必要があります。",
            "options": {
                "A": "ポアソン分布",
                "B": "一様分布",
                "C": "正規分布",
                "D": "二項分布"
            },
            "correct_answer": [
                "A"
            ],
            "explanation": "ポアソン分布は、一定期間内のイベント発生回数をモデル化するのに適しており、バス停で待つ人数の予測に最適です。これは、ランダムに発生する離散的なイベントの数を表現するのに適しています。選択肢B（一様分布）は全ての結果が等確率の場合に使用され、この状況には適していません。選択肢C（正規分布）は連続変数に適しており、離散的な人数の予測には適していません。選択肢D（二項分布）は成功/失敗の2つの結果を持つ試行の回数を表すのに適しており、この状況には適していません。",
            "references": [
                "https://www.youtube.com/watch?v=b5G2ZR5lSjc"
            ]
        },
        {
            "category": "機械学習の実装と運用",
            "question": "大規模企業内でAmazon SageMakerノートブックを使用しているが、ITセキュリティチームは、ノートブックインスタンスがVPC内で安全に管理されていないことを懸念しています。ノートブックを安全に保つための最適なアプローチを尋ねています。",
            "options": {
                "A": "SageMakerノートブックをプライベートサブネットに関連付ける。",
                "B": "SageMakerノートブックとS3バケットを同じVPCに関連付ける。",
                "C": "NATゲートウェイを使用してインターネットに接続する。",
                "D": "SageMakerノートブックにNATゲートウェイとセキュリティグループを設定する。"
            },
            "correct_answer": [
                "A"
            ],
            "explanation": "SageMakerノートブックをプライベートサブネットに配置することで、外部からのアクセスを制限し、セキュリティを強化できます。これは最も直接的で効果的な方法です。選択肢B（S3バケットとの関連付け）はデータのセキュリティには有効ですが、ノートブックインスタンス自体のセキュリティ向上には直接関係しません。選択肢C（NATゲートウェイ）はインターネットアクセスを提供しますが、セキュリティ強化には不十分です。NATゲートウェイはアウトバウンドトラフィックを許可するため、外部からの攻撃を防ぐことはできません。選択肢D（NATゲートウェイとセキュリティグループ）は部分的に正しいですが、プライベートサブネットほど包括的なセキュリティは提供しません。",
            "references": [
                "https://www.youtube.com/watch?v=m1XknpF5x5s"
            ]
        },
        {
            "category": "機械学習の実装と運用",
            "question": "オンライン小売業者が、30％の欠損データを含む大規模データセットを保持しています。この欠損データを処理する最適なアプローチを尋ねています。",
            "options": {
                "A": "リストワイズ削除",
                "B": "最後の観測値のキャリーオーバー",
                "C": "多重代入法",
                "D": "平均値補完"
            },
            "correct_answer": [
                "C"
            ],
            "explanation": "多重代入法は、高い割合の欠損データに対しても有効で、データのばらつきを維持しながら欠損値を推定します。これは、30%という高い欠損率に対して最も適切な方法です。選択肢A（リストワイズ削除）は、30%もの大量のデータを失うことになり、情報の損失が大きすぎます。選択肢B（最後の観測値のキャリーオーバー）は時系列データに適していますが、一般的なデータセットでは適切ではありません。選択肢D（平均値補完）は簡単ですが、データの変動を無視するため、30%もの欠損がある場合には適切ではありません。",
            "references": [
                "https://www.youtube.com/watch?v=TAMWvZa8yzI"
            ]
        },
        {
            "category": "機械学習の実装と運用",
            "question": "企業がAmazon SageMaker環境を設定していますが、セキュリティポリシーによりインターネットアクセスが制限されています。これに対して最も適切なアプローチを尋ねています。",
            "options": {
                "A": "VPC内にNATゲートウェイを作成する。",
                "B": "SageMakerのVPCインターフェースを使用する。",
                "C": "VPCペアリングを設定する。",
                "D": "SageMakerをプロキシ経由で接続する。"
            },
            "correct_answer": [
                "B"
            ],
            "explanation": "SageMakerのVPCインターフェースを使用することで、VPC内から安全にSageMakerリソースにアクセスできます。これにより、インターネットアクセスを必要とせずにSageMakerサービスを利用できます。選択肢A（NATゲートウェイ）はインターネットアクセスを提供しますが、セキュリティポリシーに違反する可能性があります。選択肢C（VPCペアリング）はVPC間の接続に使用されますが、SageMakerへのアクセス問題を直接解決しません。選択肢D（プロキシ経由の接続）は可能ですが、VPCインターフェースほど効率的でなく、追加の設定が必要になります。"
        },
        {
            "category": "機械学習の実装と運用",
            "question": "セキュリティ機関が複数のロケーションに設置されたカメラでビデオストリーミングを行い、リアルタイムで顔認識を行うシステムの最適な拡張方法を尋ねています。",
            "options": {
                "A": "各カメラストリームをAmazon Kinesisに統合し、顔認識を実行。",
                "B": "Amazon Rekognition Videoを使用して顔認識を行い、ストリームを処理。",
                "C": "AWS Lambda関数を使用して、顔認識を行う。",
                "D": "Amazon Rekognition Imageを使用して、画像を処理。"
            },
            "correct_answer": [
                "B"
            ],
            "explanation": "Amazon Rekognition Videoはリアルタイムのビデオストリームに対して顔認識を実行でき、拡張性に優れています。これは、複数のカメラからのビデオストリームを効率的に処理できる最適なソリューションです。選択肢A（Kinesis）はストリーミングデータの処理に適していますが、顔認識機能は含まれていません。選択肢C（Lambda）は小規模な処理には適していますが、大規模なビデオストリームの処理には適していません。選択肢D（Rekognition Image）は静止画像の処理に適していますが、ビデオストリームの処理には適していません。"
        },
        {
            "category": "機械学習の実装と運用",
            "question": "ある製造会社が、200個のパフォーマンスメトリクスをリアルタイムで測定しています。エンジンの欠陥を予測するために最適なアプローチを尋ねています。",
            "options": {
                "A": "Amazon Kinesis Data StreamsとAWS Lambdaを使用して異常を検出する。",
                "B": "AWS IoT Analyticsを使用して、データを保存し、分析する。",
                "C": "Kinesis Data Firehoseを使用して、リアルタイムデータをストリーミングする。",
                "D": "Kinesis Data Analyticsを使用して異常を分析する。"
            },
            "correct_answer": [
                "B"
            ],
            "explanation": "AWS IoT Analyticsは大量のリアルタイムデータを効率的に保存・分析でき、エンジン欠陥の予測に適しています。これは、IoTデバイスからのデータ収集、処理、保存、分析を一貫して行えるサービスです。選択肢A（Kinesis Data StreamsとLambda）はリアルタイム処理に適していますが、大量のメトリクスの分析には複雑になる可能性があります。選択肢C（Kinesis Data Firehose）はデータのストリーミングに適していますが、分析機能が不足しています。選択肢D（Kinesis Data Analytics）はストリームデータの分析に適していますが、IoT特有の機能が不足しています。"
        },
        {
            "category": "機械学習の実装と運用",
            "question": "機械学習スペシャリストがAmazon SageMakerでデータをトレーニングしています。データはCSV形式で保存されており、トレーニング速度に影響しています。データを最適化する方法を尋ねています。",
            "options": {
                "A": "Apache Parquetフォーマットに変換。",
                "B": "AWS Glueを使用して圧縮データをApache Parquetに変換。",
                "C": "RecordIOフォーマットに変換。",
                "D": "SageMakerのハイパーパラメータ最適化を使用して自動的に最適化。"
            },
            "correct_answer": [
                "C"
            ],
            "explanation": "RecordIOフォーマットに変換することが、SageMakerでのトレーニング速度を最適化する最良の方法です。RecordIOは、SageMakerに最適化されたバイナリ形式で、データの読み込みと処理を高速化します。選択肢A（Apache Parquet）は列指向のデータ保存に適していますが、SageMakerの全アルゴリズムに最適化されているわけではありません。選択肢B（AWS Glueを使用したParquet変換）は有効ですが、RecordIOほどSageMakerに最適化されていません。選択肢D（ハイパーパラメータ最適化）はモデルのパフォーマンス向上には有効ですが、データフォーマットの最適化には関係ありません。",
            "references": [
                "https://docs.aws.amazon.com/ja_jp/sagemaker/latest/dg/cdf-training.html"
            ]
        },
        {
            "category": "機械学習の実装と運用",
            "question": "機械学習スペシャリストがSageMakerでTensorFlowプロジェクトを進めていますが、オフラインでの作業を続ける必要がある場合の最適なソリューションを尋ねています。",
            "options": {
                "A": "ローカル環境にPythonとJupyterをインストール。",
                "B": "SageMaker用のTensorFlow Dockerコンテナをローカルにダウンロード。",
                "C": "TensorFlowをローカル環境でインストールし、SageMakerでトレーニング。",
                "D": "ローカル環境でJupyterノートブックを使用して開発を継続。"
            },
            "correct_answer": [
                "B"
            ],
            "explanation": "SageMaker用のTensorFlow Dockerコンテナをローカルにダウンロードすることで、オフライン環境でもSageMakerと同様の環境で開発が可能です。これにより、環境の一貫性が保たれ、オンラインとオフラインの切り替えがスムーズになります。選択肢A（PythonとJupyterのインストール）は基本的な開発環境は整いますが、SageMaker固有の機能や設定が欠けています。選択肢C（ローカルでTensorFlowをインストール）はTensorFlowの使用は可能ですが、SageMaker固有の機能が利用できません。選択肢D（ローカルでJupyterノートブックを使用）は開発の継続は可能ですが、SageMaker環境との完全な互換性は保証されません。"
        },
        {
            "category": "機械学習の実装と運用",
            "question": "データサイエンティストが、GZIPファイルからリアルタイムでデータをストリーミングする際に、最も低いレイテンシーでSQLを使用できるソリューションを尋ねています。",
            "options": {
                "A": "AWS Lambdaを使い、データを変換。",
                "B": "AWS Glueを使用してデータを変換。",
                "C": "Kinesis Client Libraryを使用してデータを変換。",
                "D": "S3に保存してデータをストリーミング。"
            },
            "correct_answer": [
                "C"
            ],
            "explanation": "Kinesis Client Libraryを使用してデータを変換することが、最も低いレイテンシーでSQLを使用できるソリューションです。KCLは高性能なストリーミング処理を提供し、GZIPファイルの解凍とデータの変換を効率的に行えます。選択肢A（AWS Lambda）は小規模なデータ処理には適していますが、大量のストリーミングデータには適していません。選択肢B（AWS Glue）はバッチ処理に適していますが、リアルタイムストリーミングには適していません。選択肢D（S3に保存）は追加の遅延を引き起こし、リアルタイム処理の要件を満たしません。",
            "references": [
                "https://dev.classmethod.jp/articles/first-time-kinesis-data-analytics/"
            ]
        },
        {
            "category": "機械学習の実装と運用",
            "question": "機械学習スペシャリストが、多量のデータを保存するための最適なフォーマットを尋ねています。データはJSON形式で保存されており、サイズが大きいため変換が必要です。",
            "options": {
                "A": "Apache Parquetフォーマットに変換",
                "B": "JSON形式のまま保存",
                "C": "CSV形式に変換",
                "D": "XML形式に変換"
            },
            "correct_answer": [
                "A"
            ],
            "explanation": "Apache Parquetフォーマットは列指向で圧縮効率が高く、大量データの保存に最適です。高速なクエリ実行と効率的なストレージ使用が可能です。JSONはサイズが大きく非効率的です。CSVは構造化データには適していますが、Parquetほどの圧縮効率はありません。XMLは冗長で大量データには不向きです。"
        },
        {
            "category": "機械学習の実装と運用",
            "question": "機械学習スペシャリストが毎日のETLジョブを実行し、大量のデータセットを処理しています。ETLジョブが失敗した場合の対策を尋ねています。",
            "options": {
                "A": "AWS Lambdaを使用してデータセットをアップロードし、失敗時にはSNS通知を送信する。",
                "B": "AWS Step Functionsを使用してデータセットをアップロードし、失敗時にはSNS通知を送信する。",
                "C": "AWS Lambdaを使って他のジョブをトリガーし、SNSで失敗を通知する。",
                "D": "AWS Glueを使って、データセットの失敗を通知する。"
            },
            "correct_answer": [
                "B"
            ],
            "explanation": "AWS Step Functionsを使用すると、ETLジョブのワークフローを管理し、失敗時にSNS通知を送信するなどのアクションを自動化できます。また、再試行やエラーハンドリングも容易です。Lambdaは短時間の処理に適していますが、大量データの処理には向いていません。Glueは ETL ツールですが、ジョブの管理や通知には特化していません。"
        },
        {
            "category": "機械学習の実装と運用",
            "question": "データサイエンティストが、既存のオンプレミスのETLプロセスをクラウドに移行する必要があります。現在のプロセスはPySparkを使用してデータを統合しています。クラウド移行の要件を考慮した最適なソリューションを尋ねています。",
            "options": {
                "A": "AWS Lambdaを使用してPySparkジョブをトリガーし、EMRに移行する。",
                "B": "AWS Glueを使用してETLを実行し、データをAmazon S3に保存する。",
                "C": "AWS Fargateを使用して、既存のETLプロセスをスケジュールし、実行する。",
                "D": "Kinesis Data Streamsを使用してETLプロセスを実行し、Amazon S3に保存する。"
            },
            "correct_answer": [
                "B"
            ],
            "explanation": "AWS GlueはPySparkベースのETLをサポートしており、S3にデータを保存することでクラウドへの移行がスムーズに行えます。また、サーバーレスで管理が容易です。LambdaはPySparkジョブの実行には適していません。EMRは有効ですが、Glueよりも管理が複雑です。Fargateはコンテナ実行環境であり、ETL専用ではありません。Kinesis Data Streamsはストリーミングデータ処理に適していますが、バッチETLプロセスには最適ではありません。",
            "references": [
                "https://docs.aws.amazon.com/ja_jp/glue/latest/dg/aws-glue-programming-python.html"
            ]
        },
        {
            "category": "機械学習の実装と運用",
            "question": "航空機エンジンの製造会社が、200個のパフォーマンスメトリクスをリアルタイムで測定しています。エンジンの欠陥を予測するために最適なアプローチを尋ねています。",
            "options": {
                "A": "Amazon Kinesis Data StreamsとAWS Lambdaを使用して異常を検出する。",
                "B": "AWS IoT Analyticsを使用して、データを保存し、分析する。",
                "C": "Kinesis Data Firehoseを使用して、リアルタイムデータをストリーミングする。",
                "D": "Kinesis Data Analyticsを使用して異常を分析する。"
            },
            "correct_answer": [
                "B"
            ],
            "explanation": "AWS IoT Analyticsは大量のリアルタイムデータを効率的に保存・分析でき、エンジン欠陥の予測に適しています。IoTデバイスからのデータ収集、前処理、保存、分析までを一貫して行えます。Kinesis Data StreamsとLambdaの組み合わせは可能ですが、大量のメトリクスの分析には複雑になる可能性があります。Kinesis Data Firehoseはデータの移動に適していますが、分析機能はありません。Kinesis Data Analyticsはリアルタイム分析に適していますが、IoTデータに特化した機能はIoT Analyticsの方が豊富です。"
        },
        {
            "category": "機械学習の実装と運用",
            "question": "機械学習スペシャリストが、エンドポイントのインスタンス設定に最適な「MaxWaitTimeInSeconds」パラメータを決定する必要があります。最適な設定値を尋ねています。",
            "options": {
                "A": "10秒",
                "B": "90秒",
                "C": "600秒",
                "D": "2400秒"
            },
            "correct_answer": [
                "B"
            ],
            "explanation": "90秒はバランスの取れた待機時間であり、適度なレイテンシーを維持しつつ、レスポンスを迅速に提供できます。10秒は短すぎる可能性があり、多くのリクエストがタイムアウトする恐れがあります。600秒（10分）は長すぎる可能性があり、ユーザー体験が低下する可能性があります。2400秒（40分）は非常に長く、ほとんどの場合で実用的ではありません。",
            "references": [
                "https://docs.aws.amazon.com/ja_jp/sagemaker/latest/dg/model-managed-spot-training.html"
            ]
        },
        {
            "category": "機械学習の実装と運用",
            "question": "機械学習スペシャリストが、リアルタイム処理が必要なデータと、毎時間更新するデータを処理する方法を尋ねています。どのサービスが最適かを選択する必要があります。",
            "options": {
                "A": "AWS DMS",
                "B": "Amazon Kinesis",
                "C": "AWS Data Pipeline",
                "D": "Amazon ES",
                "E": "Amazon Athena"
            },
            "correct_answer": [
                "B"
            ],
            "explanation": "Amazon Kinesisはリアルタイムデータストリーミングとバッチ処理の両方に対応しており、異なる更新頻度のデータを効率的に処理できます。リアルタイムデータはKinesis Data Streamsで、毎時間更新データはKinesis Data Firehoseで処理可能です。AWS DMSはデータベース移行サービスで、この用途には適していません。AWS Data Pipelineはバッチ処理向けで、リアルタイム処理には適していません。Amazon ESは検索と分析向けで、データ処理には直接適していません。Amazon Athenaはクエリサービスで、データ処理には適していません。"
        },
        {
            "category": "機械学習の実装と運用",
            "question": "トラック運行会社が、トラックから取得したイメージデータを保管し、特定のIAMユーザーにアクセス権を与えたいと考えています。最適なストレージソリューションを尋ねています。",
            "options": {
                "A": "DynamoDBを使用してデータを保存する。",
                "B": "Amazon S3バケットを使用し、IAMポリシーでアクセスを制御する。",
                "C": "EFSを使用して、IAMユーザーにアクセス権を付与する。",
                "D": "Amazon EFSを使用してデータをEC2インスタンスに保存する。"
            },
            "correct_answer": [
                "B"
            ],
            "explanation": "Amazon S3はイメージデータの保存に適しており、IAMポリシーを使用して特定のユーザーにアクセス権を付与できます。S3は大容量のオブジェクトストレージで、高い耐久性と可用性を提供します。DynamoDBはNoSQLデータベースで、イメージデータの保存には適していません。EFSはファイルシステムですが、S3と比べてコストが高く、イメージデータの保存には過剰な機能を持っています。EC2インスタンスにEFSを使用するのは、スケーラビリティとコスト面で効率的ではありません。",
            "references": [
                "https://docs.aws.amazon.com/ja_jp/AmazonS3/latest/userguide/security-access-management.html"
            ]
        },
        {
            "category": "機械学習の実装と運用",
            "question": "Amazon Pollyは、企業のアナウンスを自動化する目的で、平文のテキストを音声に変換するために企業で使用されています。しかし、企業の頭文字（AWS）が誤って発音されています。この問題について、機械学習のスペシャリストはどう対処すればいいのでしょうか？",
            "options": {
                "A": "現在の文書を発音タグ付きのSSMLに変換する。",
                "B": "適切な発音の辞書を作成する。",
                "C": "発音の目安となるスピーチマークを出力する。",
                "D": "Amazon Lexを使って、テキストファイルを発音用に前処理する。"
            },
            "correct_answer": [
                "B"
            ],
            "explanation": "Amazon Pollyのカスタム語彙（ボキャブラリー）を使うことで、特定の単語の発音を変更することができます。この問題では、適切な発音の辞書を作成し、Pollyにその辞書を読み込ませることで、特定の頭文字（AWS）の発音を正しく制御できます。選択肢AはSSMLを使用する方法ですが、辞書を使う方が簡便です。選択肢Cはスピーチマークの出力であり、発音の修正には直接関係しません。選択肢DはAmazon Lexを使用するもので、Pollyの発音修正には不適です。"
        },
        {
            "category": "機械学習の実装と運用",
            "question": "Amazon Personalize は、ある小売企業がマーケティングキャンペーン中に消費者に個別の商品を提案するために使用されています。この企業は、新しいバージョンのソリューションを導入した後、現在の顧客に対する提案商品の売上が大きく伸びたことにすぐに気付きましたが、その後すぐにその売上は減少しました。トレーニングのために、マーケティングキャンペーン以前の履歴データにしかアクセスできません。データサイエンティストは、ソリューションにどのような調整を加えるべきでしょうか？",
            "options": {
                "A": "Amazon Personalizeのイベントトラッカーを使用して、リアルタイムのユーザーインタラクションを含める。",
                "B": "ユーザーのメタデータを追加し、Amazon PersonalizeのHRNN-Metadataレシピを使用する。",
                "C": "Amazon SageMakerに内蔵されている因数分解マシン（FM）アルゴリズムを使用して、新しいソリューションを実装する。",
                "D": "Amazon Personalizeのインタラクションデータセットにイベントタイプとイベントバリューのフィールドを追加する。"
            },
            "correct_answer": [
                "A"
            ],
            "explanation": "Amazon Personalizeでは過去のユーザの行動によるアイテムとのインタラクションデータを元にしたレコメンドが可能です。イベントトラッカーを作成すると、キャンペーンによるリアルタイムレコメンドに即座に反映させることができます。選択肢Bはメタデータの追加であり、リアルタイム性の向上には直接関係しません。選択肢CはSageMakerを使用するもので、Personalizeの機能を活用するには不適です。選択肢Dはイベントデータの追加ですが、リアルタイムのインタラクションを含めるには不十分です。"
        },
        {
            "category": "機械学習の実装と運用",
            "question": "機械学習のスペシャリストが、データセットから100万語を解析する新しい自然言語処理プログラムを開発しています。その目的は、Word2Vecを実行して文の埋め込みを行い、様々な形式の予測を可能にすることです。以下は、そのデータセットからの抜粋です。\n\n\"The swift BROWN FOX leaps over the sluggish dog.\"\n\nスペシャリストは、再現可能な方法でデータを適切にクリーンアップするために、次のどのアクションを実行する必要がありますか？（3つ選択）",
            "options": {
                "A": "品詞のタグ付けを行い、動作動詞と名詞のみを残す。",
                "B": "文章を小文字にしてすべての単語を正規化する。",
                "C": "英語のストップワード辞書を使ってストップワードを削除する。",
                "D": "\"quck\"のタイプグラフィを\"quick\"に修正する。",
                "E": "文章中のすべての単語をOne-Hotでエンコードする。",
                "F": "文章を単語にトークン化する。"
            },
            "correct_answer": [
                "B",
                "C",
                "F"
            ],
            "explanation": "文字を小文字にすることで、すべてのデータに統一性を持たせることができます。ストップワードの除去は、文に重要な意味を持たない単語を除去するプロセスです。単語のトークン化は、文章を個々の単語に分割するプロセスで、Word2Vecモデルがエンベッディングを作成するための入力として使用されます。選択肢Aは品詞のタグ付けであり、必須ではありません。選択肢Dはタイプミスの修正であり、一般的なクリーンアップには含まれません。選択肢EはOne-Hotエンコーディングであり、Word2Vecの前処理には不適です。"
        },
        {
            "category": "機械学習の実装と運用",
            "question": "ある企業がAmazon SageMakerの環境を構築しています。インターネットを介した通信は、企業のデータセキュリティポリシーで禁止されています。Amazon SageMakerノートブックインスタンスへの直接のインターネットアクセスを許可することなく、Amazon SageMakerサービスを有効にするにはどうすればよいですか？",
            "options": {
                "A": "企業のVPC内にNATゲートウェイを作成する。",
                "B": "Amazon SageMakerのトラフィックをオンプレミスのネットワーク経由でルーティングする。",
                "C": "企業VPC内にAmazon SageMaker VPCインターフェースエンドポイントを作成する。",
                "D": "Amazon SageMakerをホストするAmazon VPCとのVPCピアリングを作成する。"
            },
            "correct_answer": [
                "C"
            ],
            "explanation": "Amazon SageMakerをデフォルトの設定で利用する場合、APIのリクエスト・レスポンスなどのデータ通信はインターネットを経由します。セキュリティのためにインターネットを経由せずにアクセスしたい場合には、そのサービスのVPCエンドポイントが必要です。これにより、企業のVPC内でインターネットを経由せずにSageMakerサービスにアクセスすることが可能になります。選択肢AはNATゲートウェイを使用するため、インターネットを経由する可能性があります。選択肢Bはオンプレミス経由であり、複雑です。選択肢DはVPCピアリングを使用するため、インターネットを経由する可能性があります。"
        },
        {
            "category": "機械学習の実装と運用",
            "question": "さまざまな業務上のKPIから収集したデータからレポートやダッシュボードを作成するビジネスインテリジェンスツールを構築しました。この企業では、役員が自然言語を使ってレポートからデータを取得できるようにすることで、役員のエクスペリエンスを向上させたいと考えています。またエグゼクティブが文字や音声のインターフェースを使って互いにコミュニケーションできるようにしたいと考えています。\n\nこの会話型インターフェースを提供するために、どのサービスを使用することができますか。（3つ選択）",
            "options": {
                "A": "Alexa for Business",
                "B": "Amazon Connect",
                "C": "Amazon Lex",
                "D": "Amazon Polly",
                "E": "Amazon Comprehend",
                "F": "Amazon Transcript"
            },
            "correct_answer": [
                "C",
                "E",
                "F"
            ],
            "explanation": "音声インターフェースはAmazon Lex、音声のテキスト変換はAmazon Transcribe、自然言語処理はAmazon Comprehendを利用することで実現できます。Amazon Lexは音声入力に対応した対話型インターフェースを構築するため、Amazon Comprehendはテキストから価値あるインサイトを発見するため、Amazon Transcribeは音声をテキストに変換するために使用されます。選択肢Aはビジネス向けのAlexaであり、選択肢Bはコンタクトセンター向けのサービスであり、選択肢Dはテキストを音声に変換するもので、会話型インターフェースには不向きです。"
        },
        {
            "category": "機械学習の実装と運用",
            "question": "機械学習のスペシャリストがTensorFlowのプロジェクトを与えられ、Wi-Fiにアクセスできない状態で長時間作業することを要求されます。作業を続けるために、スペシャリストはどの戦略を使うべきでしょうか？",
            "options": {
                "A": "ノートパソコンにPython3とboto3をインストールし、その環境を使ってコード開発を続ける。",
                "B": "Amazon SageMakerで使用されるTensorFlow DockerコンテナをGitHubからローカル環境にダウンロードし、Amazon SageMaker Python SDKを使用してコードをテストする。",
                "C": "tensorflow.orgからTensorFlowをダウンロードして、SageMaker環境にTensorFlowカーネルをエミュレートする。",
                "D": "SageMakerのノートブックをローカル環境にダウンロードし、ラップトップにJupyter Notebooksをインストールして、ローカルノートブックで開発を続行する。"
            },
            "correct_answer": [
                "B"
            ],
            "explanation": "ローカルでSageMakerのエスティメータを作成してデプロイする必要があります。Amazon SageMaker Python SDKはローカルモードをサポートしており、エスティメータを作成してローカル環境でデプロイすることができます。これにより、SageMakerのマネージドトレーニングまたはホスティング環境で実行する前に、深層学習スクリプトをローカルで最適な方法でテストできます。選択肢AはSageMakerの環境を再現するには不十分です。選択肢CはTensorFlowのエミュレーションに関するもので、SageMakerの機能を活用するには不適切です。選択肢Dはローカルでの開発環境を整える方法ですが、SageMakerの特定の機能を活用するには不十分です。"
        },
        {
            "category": "機械学習の実装と運用",
            "question": "機械学習のスペシャリストは、Amazon SageMaker サービス API へのコールを保護したいと考えています。このスペシャリストは、Amazon SageMaker サービス API 用の VPC インターフェースエンドポイントで Amazon Virtual Private Cloud（VPC）を作成し、特定のインスタンスと IAM ユーザーからのトラフィックを保護しようとしています。VPC には単一のパブリックサブネットが設定されています。トラフィックのセキュリティを確保するために、機械学習のスペシャリストはどの手順の組み合わせを取るべきですか？（2つ選択）",
            "options": {
                "A": "VPCエンドポイントポリシーを追加して、IAMユーザーのアクセスを許可する。",
                "B": "ユーザーのIAMポリシーを変更して、Amazon SageMaker Service API コールへのアクセスのみを許可する。",
                "C": "エンドポイントネットワークインターフェースのセキュリティグループを変更して、インスタンスへのアクセスを制限する。",
                "D": "インスタンスへのアクセスを制限するために、エンドポイントネットワークインターフェース上のACLを変更する。",
                "E": "SageMaker Runtime VPC のエンドポイントインターフェースを VPC に追加する。"
            },
            "correct_answer": [
                "A",
                "C"
            ],
            "explanation": "APIコールの保護のために、適切なネットワーク設定およびセキュリティ設定を行う必要があります。VPCエンドポイントポリシーを設定することで、IAMユーザーによるアクセスをコントロールします。また、エンドポイントネットワークインターフェースのセキュリティグループを設定することで、特定のインスタンスにのみアクセスを制限できます。選択肢BはIAMポリシーの変更であり、VPCエンドポイントのセキュリティには直接関係しません。選択肢DはACLの変更であり、セキュリティグループの設定ほど詳細な制御はできません。選択肢Eはエンドポイントの追加であり、セキュリティの強化には直接関係しません。"
        },
        {
            "category": "機械学習の実装と運用",
            "question": "機械学習のスペシャリストは、会社のVPCのプライベートサブネット内でAmazon SageMakerノートブックインスタンスを使用しています。機械学習のスペシャリストは、Amazon SageMakerノートブック・インスタンスのAmazon EBSボリュームのスナップショットをとる必要があります。一方、機械学習のスペシャリストは、VPC内のAmazon SageMakerノートブックインスタンスのEBSボリュームまたはAmazon EC2インスタンスを見つけることができません。なぜ機械学習のスペシャリストはVPC内のインスタンスを見ることができないのでしょうか？",
            "options": {
                "A": "Amazon SageMakerのノートブックインスタンスは、顧客アカウント内のEC2インスタンスをベースにしていますが、VPCの外で実行されます。",
                "B": "Amazon SageMakerのノートブックインスタンスは、顧客アカウント内のAmazon ECSサービスに基づいています。",
                "C": "Amazon SageMakerノートブックインスタンスは、AWSサービスアカウント内で動作するEC2インスタンスに基づいています。",
                "D": "Amazon SageMakerノートブックインスタンスは、AWSサービスアカウント内で実行されているAWS ECSインスタンスに基づいています。"
            },
            "correct_answer": [
                "C"
            ],
            "explanation": "SageMakerノートブックインスタンスは、Amazon SageMakerのマネージドVPCで動作しており、AWSが管理するマネージドVPC上のEC2に自身が依存する形で実行されます。そのため、EBSボリュームやEC2インスタンスをプライベートサブネット内で見つけることができません。選択肢AはVPCの外で実行されるとしていますが、実際にはAWSのマネージドVPC内で実行されます。選択肢BとDはECSに基づいているとしていますが、SageMakerノートブックインスタンスはEC2に基づいています。",
            "references": [
                "https://aws.amazon.com/jp/blogs/news/understanding-amazon-sagemaker-notebook-instance-networking-configurations-and-advanced-routing-options/"
            ]
        },
        {
            "category": "機械学習の実装と運用",
            "question": "あるデータサイエンティストは、ストリーミングのオンライントラフィックデータを取り込むためのパイプラインを構築しています。このパイプラインの一部として、データサイエンティストは、異常なWebトラフィックパターンを識別する技術を構築する必要があります。データサイエンティストはラベル付けされていない履歴データにアクセスします。このソリューションでは以下のことを行う必要があります。- 異常なWebトラフィックの各アイテムの異常スコアを計算する。- 異常なイベントの検出を、時系列で発生するWebのトレンドに適応させる。これらの要件を満たすために、データサイエンティストはどの方法を用いるべきでしょうか？",
            "options": {
                "A": "過去のWebトラフィックデータを使用して、Amazon SageMakerのRCF（Random Cut Forest）内蔵モデルを使用した異常検知モデルをトレーニングする。Amazon Kinesisデータストリームを使用して、受信したWebトラフィックデータを処理する。前処理のAWS Lambda関数を取り付け、RCFモデルを呼び出して各レコードの異常スコアを計算することで、データのエンリッチメントを実行する。",
                "B": "歴史的なWebトラフィックデータを使用して、Amazon SageMaker内蔵のXGBoostモデルを使用して異常検出モデルをトレーニングする。Amazon Kinesisデータストリームを使用して、受信したWebトラフィックデータを処理する。各レコードの異常スコアを計算するためにXGBoostモデルを呼び出すことでデータエンリッチメントを実行するために、前処理AWS Lambda関数を構築する。",
                "C": "Amazon Kinesis Data Firehoseを使用してストリーミングデータを収集する。配信ストリームをAmazon Kinesis Data Analyticsの入力ソースとしてマッピングする。k-Nearest Neighbors（kNN）SQLエクステンションを使用してストリーミングデータに対してリアルタイムで実行するSQLクエリを書き、タンプリングウィンドウを使用して各レコードの異常スコアを計算する。",
                "D": "Amazon Kinesis Data Firehoseを使用してストリーミングデータを収集する。配信ストリームをAmazon Kinesis Data Analyticsの入力ソースとしてマッピングする。Amazon Random Cut Forest（RCF）SQLエクステンションを使用してストリーミングデータに対してリアルタイムで実行するSQLクエリを書き、スライディングウィンドウを使用して各レコードの異常スコアを計算する。"
            },
            "correct_answer": [
                "D"
            ],
            "explanation": "Amazon Kinesis Data FirehoseとAmazon Kinesis Data Analyticsを組み合わせて、リアルタイムで異常検知を行うのに適しています。RCF（Random Cut Forest）アルゴリズムはラベル付けされていないデータに対して適用でき、スライディングウィンドウを用いることで、時系列のトレンドを反映しながら異常検知を行えます。選択肢AとBは、SageMakerを使用しており、リアルタイム処理には適していません。選択肢CのkNNは、異常検知においてRCFほど効果的ではありません。"
        },
        {
            "category": "機械学習の実装と運用",
            "question": "あるメーカーは、ラベル付きの過去の販売データのかなりのコレクションを保持しています。このメーカーは、各四半期に製造すべき部品のユニット数を予測したいと考えています。この問題に対処するために、どの機械学習技術を利用すべきでしょうか？",
            "options": {
                "A": "ロジスティック回帰",
                "B": "ランダムカットフォレスト（RCF）",
                "C": "主成分分析（PCA）",
                "D": "線形回帰"
            },
            "correct_answer": [
                "D"
            ],
            "explanation": "今回のケースでは、製造すべき部品のユニット数を特定するために、回帰問題を解く必要があります。回帰問題は、分類問題とは異なり、具体的な連続値を予測する問題です。RCF、PCAは教師なし学習であるため不適切で、ロジスティック回帰は分類問題に適したモデルであるため誤りです。したがって正解は、以下の通りです。\n- 線形回帰"
        },
        {
            "category": "機械学習の実装と運用",
            "question": "データサイエンティストは、GZIPファイルのデータストリームをリアルタイムに把握したいと考えています。また、把握する際にはSQLを使用してストリームを照会することを考えています。遅延が最も少なくなる確認方法は次のうちどれでしょうか？",
            "options": {
                "A": "Amazon Kinesis Data AnalyticsにAWS Lambda関数を使用してデータを変換する。",
                "B": "AWS Glueと、データを変換するためのカスタムETLスクリプト。",
                "C": "Amazon Kinesis Client Libraryでデータを変換し、Amazon ESクラスターに保存する。",
                "D": "Amazon Kinesis Data Firehoseによるデータの変換とAmazon S3バケットへの保存。"
            },
            "correct_answer": [
                "A"
            ],
            "explanation": "Kinesis Analyticsアプリケーションは、Lambda関数を使用して生のデータレコードを自動的に処理し、変換されたデータをSQLコードに送信してさらに処理することができます。この変換処理はKinesis Analyticsを使用しているためほとんど発生しません。Kinesis AnalyticsとLambda関数の組み合わせは、GZIPの変換や、データ転送などの一般的なユースケースで使用されます。選択肢BのAWS Glueは、バッチ処理に適しており、リアルタイム処理には不向きです。選択肢CのKinesis Client Libraryは、データの変換に特化していません。選択肢DのKinesis Data Firehoseは、データの変換に時間がかかる可能性があります。\nしたがって正解は、以下の通りです。\n- Amazon Kinesis Data AnalyticsにAWS Lambda関数を使用してデータを変換する。"
        },
        {
            "category": "機械学習の実装と運用",
            "question": "ある企業では、膨大な数の工場を運営し、複雑なサプライチェーンのつながりを維持しており、予期せぬ機械の故障により、複数の工場の操業が停止する可能性があります。データサイエンティストは、工場のセンサーデータを調査して、予防的なメンテナンスが必要な機器を検出し、予定外のダウンタイムを回避するために、修理担当者を配置したいと考えています。1台の機械のセンサーデータには、温度、電圧、振動、回転数、圧力など、最大で200のデータが含まれています。同社では、これらのセンサーデータを取得するために、工場内にWi-FiとLANを設置しました。多くの工場では、安定した高速インターネット環境がないにもかかわらず、メーカーはニアリアルタイムでの推論機能を維持したいと考えています。",
            "options": {
                "A": "Amazon SageMakerでモデルをデプロイする。センサーデータをこのモデルに通して、メンテナンスが必要なマシンを予測する。",
                "B": "AWS IoT Greengrass上のモデルを各工場にデプロイする。センサーデータをこのモデルに通して、どの機械がメンテナンスを必要としているかを推察する。",
                "C": "Amazon SageMakerのバッチ変換ジョブでモデルをデプロイする。毎日のバッチレポートで推論を生成し、メンテナンスが必要なマシンを特定する。",
                "D": "Amazon SageMakerにモデルをデプロイし、IoTルールを使用してAmazon DynamoDBテーブルにデータを書き込みます。AWS Lambda関数でテーブルからDynamoDBストリームを消費し、エンドポイントを呼び出す。"
            },
            "correct_answer": [
                "B"
            ],
            "explanation": "工場内のエッジデバイスに推論モデルをデプロイして、エッジ推論を行うことでWi-Fi環境が悪い工場であってもニアリアルタイムの推論が可能です。AWS IoT Greengrassは、エッジデバイスにローカルコンピューティング、メッセージング、データキャッシング、同期、ML推論機能を提供します。AWS IoT Greengrass Connectorsと長時間稼働するラムダ関数により、既存のあらゆる産業用プロトコルやデバイスと統合することができます。さらに、AWS IoT Greengrassでは、ゲートウェイのローカルリソースにアクセスすることができ、GPIOやシリアルポート、その他のインターフェースを介してセンサーデータを受信し、デバイスを管理することができるようになります。AWS IoT Greengrassは、SCADAやMESなどの上位システムと接続し、産業用デバイスからの情報を充実させ、現場からMES/スピーデータをフィードバックすることができます。選択肢AのSageMakerは、インターネット接続が不安定な環境ではリアルタイム推論に適していません。選択肢Cのバッチ変換ジョブは、リアルタイム推論には不向きです。選択肢DのDynamoDBとLambdaの組み合わせは、リアルタイム推論には適していません。\nしたがって正解は、以下の通りです。\n- AWS IoT Greengrass上のモデルを各工場にデプロイする。センサーデータをこのモデルに通して、どの機械がメンテナンスを必要としているかを推察する。"
        },
        {
            "category": "機械学習の実装と運用",
            "question": "機械学習のスペシャリストが、k = [1..10]の場合のk-meansの調査結果を説明するために下のグラフを作成しました。このグラフから、最適なkの値を現実的に選ぶにはどうしたらよいでしょうか？",
            "options": {
                "A": "1",
                "B": "4",
                "C": "7",
                "D": "10"
            },
            "correct_answer": [
                "B"
            ],
            "explanation": "k-means法ではクラスター数をハイパーパラメータとしてモデル作成者が決定する必要があります。適切なクラスター数を決定するための考え方として、エルボー法を用いることが適切です。エルボー法は、クラスター数を変えながら誤差を計算し、結果を図示することで最適（と思われる）クラスター数を推定する手法です。クラスター数を横軸に、対応する誤差の値を縦軸にした折れ線グラフを見た際に、クラスター数を増やしてもエラーの数があまり変化しなくなる点（ここを肘＝エルボーとする）が現れます。エルボー法では、こういった点に対応するクラスター数を適切なクラスター数と仮定します。\n今回であればクラスター数4あたりでエラーの数の変化が緩やかになり始めることがわかります。選択肢Aの1は、クラスター数が少なすぎてデータの多様性を捉えられません。選択肢Cの7と選択肢Dの10は、クラスター数が多すぎて過剰にデータを分割する可能性があります。したがって正解は、以下の通りです。\n- 4"
        },
        {
            "category": "機械学習の実装と運用",
            "question": "Amazon Textractは、ある企業で日常的に使用されており、スキャンされた何千もの法律文書からテキストデータを抽出しています。この会社は、この情報をローン申請の自動処理に利用しています。一部の書類はビジネスバリデーションを通過しないため、人間の審査員に戻されて調査されます。この作業は、ローン申請書の処理に必要な時間を増加させます。ローン申請書の処理を迅速に行うために、この企業は何をすべきでしょうか？",
            "options": {
                "A": "信頼度の低い予測をAmazon SageMaker Ground TruthにルーティングするようにAmazon Textractを構成する。ビジネス検証を行う前に、それらの単語に対して手動レビューを行う",
                "B": "非同期操作の代わりに、Amazon Textractの同期操作を使用する",
                "C": "信頼度の低い予測をAmazon Augmented AI（Amazon A2I）にルーティングするようにAmazon Textractを構成する。ビジネス検証を行う前に、それらの単語に対して手動レビューを行う",
                "D": "画像内のテキストを検出するAmazon Rekognitionの機能を使用して、スキャンした画像からデータを抽出する。この情報を使って、ローン申請を処理する"
            },
            "correct_answer": [
                "C"
            ],
            "explanation": "一部のマニュアル作業を自動化・最適化するための適切な方法を選択する必要があります。Amazon Augmented AIは、人によるレビューに必要なワークフローを簡単に構築できる機械学習サービスです。Amazon A2Iを使用すると構造化されたデータかどうかに関係なく、スキャンされたドキュメント内の重要なデータポイントを抽出し、出力を人にレビューしてもらうことができます。したがって正解は、信頼度の低い予測をAmazon Augmented AI（Amazon A2I）にルーティングするようにAmazon Textractを構成する。ビジネス検証を行う前に、それらの単語に対して手動レビューを行う、となります。選択肢AのSageMaker Ground Truthは、データラベリングに使用されるもので、レビューには適していません。選択肢Bの同期操作は、レビューの効率化には直接関係しません。選択肢DのRekognitionは、画像内のオブジェクト検出に使用されるもので、テキストデータの抽出には適していません。"
        },
        {
            "category": "機械学習の実装と運用",
            "question": "機械学習のスペシャリストは、ある企業の顧客の購買行動に関する情報を含む構造化されたデータセットを与えられます。各顧客は数千のデータ列と数百の数値列で表されています。スペシャリストの目的は、これらの列が消費者のグルーピングに利用できるかどうかを判断し、その結果を可能な限り迅速に表示することです。\n\nスペシャリストはこれらのタスクにどのように取り組むべきでしょうか？",
            "options": {
                "A": "t-SNE（t-distributed stochastic neighbor embedding）アルゴリズムを用いて数値特徴を埋め込み、散布図を作成する",
                "B": "異なるkの値に対してユークリッド距離測定を用いてk-meansを実行し、エルボープロットを作成する",
                "C": "t-SNE（t-distributed stochastic neighbor embedding）アルゴリズムを用いて数値特徴を埋め込み、折れ線グラフを作成する",
                "D": "異なる値のkに対してユークリッド距離測定を用いてk-meansを実行し、各クラスタ内の各数値列の箱ひげ図を作成する"
            },
            "correct_answer": [
                "A"
            ],
            "explanation": "特徴量が数百にもおよるため、それをそのまま用いてグルーピングすると特徴量の分布がわからなくなります。\n\n従って、特徴量を削減して2次元散布図として表示することで、直感的にグルーピングの可否を理解することができます。\n\nt-Distributed Stochastic Neighbor Embedding（t-SNE）は、高次元データの探索に用いられる非線形次元削減アルゴリズムです。\nt-SNEは主成分分析（PCA）と並んで有効な次元削減技術です。\n\nしたがって正解は、以下の通りです：\n- t-SNE（t-distributed stochastic neighbor embedding）アルゴリズムを用いて数値特徴を埋め込み、散布図を作成する。選択肢B、C、Dは迅速な可視化には不向きです。"
        },
        {
            "category": "機械学習の実装と運用",
            "question": "ある機械学習のスペシャリストが、ニューヨークの公共交通機関に関するデータセットを対象に、ベイジアンネットワークを開発しています。確率変数の1つは離散的で、バスが平均で10分ごとに運行していると仮定した場合、ニューヨーカーがバスを待つ時間を反映しています。\n\n機械学習のスペシャリストは、この変数の事前分布としてどの確率分布を使うべきでしょうか？",
            "options": {
                "A": "ポアソン分布",
                "B": "一様分布",
                "C": "正規分布",
                "D": "二項分布"
            },
            "correct_answer": [
                "A"
            ],
            "explanation": "バスの待ち時間は、バスの乗車予定客がバス停に到着する確率分布に依存します。乗車予定客がバス停に到着するのを完全にランダムだと仮定すると、ポアソン分布であるとみなすことができます。\n\nポアソン分布とは、ある期間に平均λ回起こる事象が、ある期間にX回起きる確率の分布、と定義できます。\n\n今回の例であれば、求めたい値は、バスの乗車予定客がある期間に平均λ人来る事象が、10分でX回起きる確率の分布から計算ができます。\n\nしたがって正解は、以下の通りです：\n- ポアソン分布。選択肢B、C、Dはこのシナリオに適した分布ではありません。"
        },
        {
            "category": "機械学習の実装と運用",
            "question": "データサイエンティストは、高速のリアルタイムストリーミングデータのためのサーバーレスのインジェスチョンとアナリティクスソリューションを開発する必要があります。\n\nデータの整合性を犠牲にすることなく、インジェスチョンプロセスでは、JSONから入ってくるレコードをバッファリングし、クエリに最適化されたカラムナのフォーマットに変換する必要があります。また、出力データストリームへのアクセス性を高め、アナリストがSQLを用いてデータを照会し、既存のビジネスインテリジェンスダッシュボードにピン留めできるようにする必要があります。\n\n要件を満たすために、データサイエンティストはどのソリューションを構築すべきでしょうか？",
            "options": {
                "A": "受信データ形式のAWS Glue Data Catalogにスキーマを作成する。Amazon Kinesis Data Firehose配信ストリームを使用してデータをストリーミングし、Amazon S3に配信する前に、AWS Glue Data Catalogを使用してデータをApache ParquetまたはORC形式に変換する。アナリストがAmazon Athenaを使用してAmazon S3から直接データを照会し、Athena Java Database Connectivity（JDBC）コネクタを使用してBIツールに接続する",
                "B": "各JSONレコードをAmazon S3のステージングロケーションに書き込みます。S3 Putイベントを使用して、データをApache ParquetまたはORC形式に変換し、Amazon S3の処理済みデータロケーションに導入するAWS Lambda関数をトリガーする。アナリストに、Amazon Athenaを使用してAmazon S3から直接データを照会させ、Athena Java Database Connectivity（JDBC）コネクタを使用してBIツールに接続する",
                "C": "各JSONレコードをAmazon S3のステージングロケーションに書き込みます。S3 Putイベントを使用して、データをApache ParquetまたはORC形式に変換し、Amazon RDS PostgreSQLデータベースに挿入するAWS Lambda関数をトリガーする。アナリストにRDSデータベースからのクエリとダッシュボードの実行をさせる",
                "D": "Amazon Kinesis Data Analyticsを使用してストリーミングデータを取り込み、リアルタイムのSQLクエリを実行してレコードをApache Parquetに変換してからAmazon S3に配信する。アナリストは、Amazon Athenaを使用してAmazon S3のデータを照会し、Athena Java Database Connectivity（JDBC）コネクタを使用してBIツールに接続する"
            },
            "correct_answer": [
                "A"
            ],
            "explanation": "要件を整理すると、ストリームデータのバッファリングおよび変換（JSON to カラムナー）、SQLによるデータ抽出およびBIツールとのネイティブな連携が必要になります。\n\nAWS Glue Data Catalogを利用した処理が有効です。AWS Glue Data Catalogを利用したストリーミングETLジョブを追加することで、連続的に実行されるストリーミング抽出/変換/ロード（ETL）ジョブを作成し、Amazon Kinesis Data Streams、Apache Kafka、Amazon Managed Streaming for Apache Kafka（Amazon MSK）などのストリーミングソースからのデータを使用できます。\n\nしたがって正解は、以下の通りです：\n- 受信データ形式のAWS Glue Data Catalogにスキーマを作成する。Amazon Kinesis Data Firehose配信ストリームを使用してデータをストリーミングし、Amazon S3に配信する前に、AWS Glue Data Catalogを使用してデータをApache ParquetまたはORC形式に変換する。アナリストがAmazon Athenaを使用してAmazon S3から直接データを照会し、Athena Java Database Connectivity（JDBC）コネクタを使用してBIツールに接続する。選択肢B、C、Dはリアルタイム処理やカラムナフォーマットへの変換に最適ではありません。",
            "references":[
                "https://blog.serverworks.co.jp/2023/01/05/091657"
            ]
        },
        {
            "category": "機械学習の実装と運用",
            "question": "あるオンラインリセラーは、巨大な複数列のデータセットを持っていますが、ある列のデータの30%が欠けています。機械学習のスペシャリストによると、データセットのいくつかの列を利用して欠損データを再現できる可能性があります。 データセットの整合性を保つために、スペシャリストはどの再構築技術を利用すべきでしょうか？",
            "options": {
                "A": "リストワイズ削除",
                "B": "last observation carried forward (LOCF)",
                "C": "マルチプルインピュテーション",
                "D": "平均値補完"
            },
            "correct_answer": [
                "C"
            ],
            "explanation": "欠損値の特性などに応じて以下のような手法を使い分ける必要があります。\n- リストワイズ削除（リストワイズ法）は、欠損値が含まれるレコードを全て削除する方法です。この方法によって、欠損値のないクリーンなデータのみが入手できますが、欠損値を含むレコードが多い場合は貴重なトレーニングデータを削除してしまうため注意が必要です。\n- last observation carried forward (LOCF) は、最後に観測された値でその特徴量の全ての欠損値を補完する単一補完法の一種です。\n- マルチプルインピュテーションは、欠損値のない他の特徴量を用いて欠損値のある特徴量を推定するモデルを作成し、そのモデルに基づいて値を補完する方法です。\n- 平均値/中央値補完は、特定の特徴量の平均値/中央値を算出して、その特徴量の全ての欠損値に対してその値を補完する単一補完法の一種です。\n欠損値を含むレコードが30%もある場合の正しい欠損値処理の方法を考える必要があります。まず、レコードを削除すると、30%ものデータを失うことになるため今回は不適切です。また、LOCFや平均値補完法といった単一補完では、30%ものデータに対して行うと、データのバイアスが増してしまいます。今回のように欠損値を含むレコードが多数ある場合は、他の特徴量から特定の特徴量を表現するモデルを作成し、モデルを基に欠損値補完が必要です。したがって正解は、以下の通りです。 - マルチプルインピュテーション。選択肢A、B、Dはデータのバイアスを増やす可能性が高く、適切ではありません。"
        },
        {
            "category": "機械学習の実装と運用",
            "question": "あるトラック運送会社では、世界中の車両からリアルタイムに画像データを収集しています。このデータは日々蓄積され続けており、毎日約100GBの新しいデータが作成されています。この企業は、データを許可されたIAMユーザーのみが利用できることを保証しながら、機械学習を用いたアプリケーションを開発したいと考えています。 処理の柔軟性が最も高く、IAMのアクセス制御をサポートするストレージはどれか？",
            "options": {
                "A": "Amazon DynamoDBなどのデータベースを使用してイメージを保存し、IAMポリシーを設定して目的のIAMユーザーのみにアクセスを制限する",
                "B": "Amazon S3にバックアップされたデータレイクを使用して生の画像を保存し、バケットポリシーを使用して権限を設定する",
                "C": "Hadoop Distributed File System (HDFS) を備えたAmazon EMRをセットアップしてファイルを保存し、IAMポリシーを使用してEMRインスタンスへのアクセスを制限する",
                "D": "IAMポリシーでAmazon EFSを設定し、IAMユーザーが所有するAmazon EC2インスタンスでデータを利用できるようにする"
            },
            "correct_answer": [
                "B"
            ],
            "explanation": "日々増え続ける膨大なデータを安全に保存するためのストレージオプションを選択する必要があります。Amazon Simple Storage Service (S3) は、構造化・非構造化データ向けの最大かつ最もパフォーマンスの高いオブジェクトストレージサービスで、データレイクを構築する上で最適なストレージサービスです。 Amazon S3では、99.999999999% (11 9s) の耐久性でデータが保護された安全な環境で、あらゆる規模のデータレイクをコスト効率よく構築・拡張することが可能です。 Amazon S3上に構築されたデータレイクでは、AWSのネイティブサービスを使用して、ビッグデータ分析、人工知能 (AI)、機械学習 (ML)、ハイパフォーマンスコンピューティング (HPC)、メディアアナリティクスアプリケーションを実行し、非構造化データセットから洞察を得ることができます。選択肢A、C、DはS3のような拡張性とコスト効率を提供しません。"
        },
        {
            "category": "機械学習の実装と運用",
            "question": "機械学習のスペシャリストは、データの前処理をして学習用のデータを準備する役割を担っています。データによっては、ニアリアルタイムで処理しなければならないものもあれば、1時間単位で転送するものもあります。データクリーニングや特徴量エンジニアリングのための既存のAmazon EMR MapReduceオペレーションがあります。次のサービスのうち、MapReduceジョブにデータを供給することができるものはどれですか？（2つ選択）",
            "options": {
                "A": "AWS DMS",
                "B": "Amazon Kinesis",
                "C": "AWS Data Pipeline",
                "D": "Amazon Athena",
                "E": "Amazon Elastic Search"
            },
            "correct_answer": [
                "B",
                "C"
            ],
            "explanation": "EMRと連携可能なAWSサービスを選定する必要があります。AWS Data Pipelineは、データの移動と変換を自動化するために使用できるサービスであり、Amazon EMRと連携してデータのワークフローを構築できます。また、Amazon KinesisはEMRを使用してニアリアルタイムのデータ送信を実現できます。したがって、正解はAmazon KinesisとAWS Data Pipelineです。選択肢Aはデータ移行に特化しており、選択肢DとEはデータ供給に適していません。"
        },
        {
            "category": "機械学習の実装と運用",
            "question": "機械学習のスペシャリストが、ある企業の売上を伸ばすための技術を開発しています。ユーザーの行動や商品の好みに関する企業の膨大なデータを活用し、他のユーザーとの類似性に基づいて消費者がどのようなものを好むかを予測することが目的です。この目的を達成するために、スペシャリストはどのような行動をとるべきでしょうか？",
            "options": {
                "A": "Amazon EMR上でApache Spark MLを使ったコンテンツベースのフィルタリング・レコメンデーション・エンジンの構築",
                "B": "Amazon EMR上でApache Spark MLを使って協調フィルタリング推薦エンジンを構築する",
                "C": "Amazon EMR上のApache Spark MLでモデルベースのフィルタリングレコメンデーションエンジンを構築する",
                "D": "Amazon EMR上のApache Spark MLを使った組み合わせ型フィルタリング・レコメンデーション・エンジンの構築"
            },
            "correct_answer": [
                "B"
            ],
            "explanation": "Amazon EMRは、Apache Sparkを実行するのに最適なサービスです。協調フィルタリング（Collaborative Filtering, CF）は、多くのユーザーの嗜好情報を蓄積し、あるユーザと嗜好の類似した他のユーザの情報を用いて自動的に推薦を行う方法論です。これにより、過去の顧客データから適切なレコメンデーションを実現できます。したがって正解は、Amazon EMR上でApache Spark MLを使って協調フィルタリング推薦エンジンを構築するです。選択肢Aは個別のユーザーに焦点を当てすぎ、選択肢Cはモデルの複雑さが増し、選択肢Dは実装が複雑です。",
            "references": [
                "https://youtu.be/4Ufua4wFR2w?si=r8YK3F90WJ9fgwH8"
            ]
        },
        {
            "category": "機械学習の実装と運用",
            "question": "機械学習のスペシャリストは、エンドポイントの自動スケーリング構成に最適なSageMakerVariantInvocationsPerInstance設定を決定することを任されています。スペシャリストは、単一のインスタンスで負荷テストを実施し、サービスを低下させないために必要な1秒あたりの最大リクエスト数（RPS）は約20RPSであると結論付けました。スペシャリストは、今回が初めての導入であることから、呼び出しの安全係数を0.5に設定したいと考えています。SageMakerVariantInvocationsPerInstanceの設定は、前述のパラメータと、インスタンスごとの起動数が1分ごとに監視されていることに基づいて、スペシャリストは何を設定すべきでしょうか？",
            "options": {
                "A": "10",
                "B": "30",
                "C": "600",
                "D": "2,400"
            },
            "correct_answer": [
                "C"
            ],
            "explanation": "AWSのプラクティスに則り、呼び出しの必要なパフォーマンス特性を特定し、安全係数を設定しています。SageMakerVariantInvocationsPerInstanceメトリクスの正しい値は、次の式を使って計算できます： SageMakerVariantInvocationsPerInstance = (MAX_RPS * SAFETY_FACTOR) * 60。 ここで、MAX_RPS = 20、SAFETY_FACTOR = 0.5のため、設定値は600です。選択肢AとBはRPSの計算が不十分で、選択肢Dは過剰です。"
        },
        {
            "category": "機械学習の実装と運用",
            "question": "機械学習のスペシャリストは、長期間稼働するAmazon EMRクラスターの構築を任されています。EMRクラスターには、1つのマスターノード、10のコアノード、20のタスクノードがあります。スペシャリストはEMRクラスターのスポットインスタンスを使用してコストを削減します。スペシャリストがSpot Instancesで起動すべきノードは？",
            "options": {
                "A": "マスターノード",
                "B": "コアノードのいずれか",
                "C": "タスクノードのいずれか",
                "D": "コアノードとタスクノードの両方"
            },
            "correct_answer": [
                "C"
            ],
            "explanation": "Amazon EC2スポットインスタンスは、AWSクラウド上で利用可能な予備の計算能力を、オンデマンド価格よりも大幅な割引価格で提供します。マスターとコアのインスタンスグループをオンデマンドインスタンスとして起動して通常の容量を処理し、タスクインスタンスグループをスポットインスタンスとして起動してピーク時の負荷要件に対応するのが一般的です。一方で、マスターとコアはクラスターの一貫した管理を行う必要があるため、スポットインスタンスの利用は一般的には非推奨となっています。選択肢AとBは重要なノードであり、選択肢Dはリスクが高いです。",
            "references":[
                "https://docs.aws.amazon.com/ja_jp/emr/latest/ManagementGuide/emr-plan-instances-guidelines.html"
            ]
        },
        {
            "category": "機械学習の実装と運用",
            "question": "あるデータサイエンティストは、オンプレミスのETLプロセスをクラウドに移行する責任を負っています。現在のプロセスは定期的に実行される予定で、PySparkを使用して多数の巨大なデータソースを下流で実行される処理用の単一の出力に変換します。データサイエンティストは、以下の要件でクラウドソリューションを提供するように指示されています。\n\n- 複数のデータソースを統合する。\n- 既存のPySparkロジックを再利用する。\n- 既存のスケジュールでソリューションを実行する。\n- 管理が必要なサーバーの数を最小限にする。\n\nデータサイエンティストは、このソリューションの構築にどのアーキテクチャを使用すべきですか？",
            "options": {
                "A": "生データをAmazon S3に書き込みます。AWS Lambda関数をスケジュールして、既存のスケジュールに基づいてSparkステップを永続的なAmazon EMRクラスターに送信する。既存のPySparkロジックを使用して、EMRクラスターでETLジョブを実行し、Amazon S3の「処理済み」の場所に結果を出力させ、ダウンストリームの使用のためにアクセスできるようにする。",
                "B": "生データをAmazon S3に書き込む。入力データに対してETL処理を行うAWS Glue ETLジョブを作成する。ETLジョブをPySparkで書き、既存のロジックを活用する。既存のスケジュールに基づいてETLジョブをトリガーするために、新しいAWS Glueトリガーを作成する。ETLジョブの出力先を、Amazon S3の「処理済み」の場所に結果を出力させ、ダウンストリームの使用のためにアクセスできるようにする。",
                "C": "生データをAmazon S3に書き込みます。AWS Lambda関数をスケジュールして、既存のスケジュールで実行し、Amazon S3からの入力データを処理する。LambdaのロジックをPythonで書き、ETL処理を行うために既存のPySparkのロジックを実装する。Lambda関数に、Amazon S3の「処理済み」の場所に結果を出力させ、ダウンストリームの使用のためにアクセスできるようにする。",
                "D": "Amazon Kinesis Data Analyticsを使用して入力データをストリームし、ストリームに対してリアルタイムのSQLクエリを実行して、ストリーム内で必要な変換を実行する。"
            },
            "correct_answer": [
                "B"
            ],
            "explanation": "データレイクおよび下流で利用されるデータの保存先にはS3が最適です。AWS Glueは、機械学習、アプリケーション開発のためのデータの発見、準備、結合を行うサーバーレスのデータ統合サービスです。AWS Glueでは、既存のPySparkロジックを再利用し、トリガーによりスケジュール通りにジョブを実行できるため、最も効率的なソリューションです。選択肢Aはサーバー管理が必要で、選択肢CはLambdaの制約があり、選択肢Dはリアルタイム処理に特化しています。"
        },
        {
            "category": "機械学習の実装と運用",
            "question": "ラベル付けされていない写真、テキスト、オーディオ、ビデオ映像の大規模なコレクションを持つメディア企業が、リサーチチームが関連する情報を迅速に特定できるように、資産のインデックスを作成しようとしています。この企業は、機械学習の経験がほとんどない社内の研究者の作業を迅速化するために、機械学習を使用したいと考えています。\n\nアセットのインデックス作成には、どの方法が一番早いでしょうか？",
            "options": {
                "A": "Amazon Rekognition、Amazon Comprehend、およびAmazon Transcribeを使用して、データを明確なカテゴリー/クラスにタグ付けする",
                "B": "Amazon Mechanical Turk Human Intelligence Tasksのセットを作成し、すべての映像にラベルを付ける",
                "C": "Amazon Transcribeを使用して、音声をテキストに変換する。Amazon SageMakerのNeural Topic Model（NTM）およびObject Detectionアルゴリズムを使用して、データを個別のカテゴリー/クラスにタグ付けする",
                "D": "AWS Deep Learning AMIおよびAmazon EC2 GPUインスタンスを使用して、オーディオトランスクリプションとトピックモデリングのカスタムモデルを作成し、オブジェクト検出を使用してデータを個別のカテゴリー/クラスにタグ付けする"
            },
            "correct_answer": [
                "A"
            ],
            "explanation": "大量なデータを迅速にラベル付けする必要があるため、AWSのマネージドサービスを使用して機械的にラベル付けを行うことが最適です。\n\nAmazon Rekognitionは、強力な画像分析をアプリケーションに簡単に追加できるようにするサービスです。Amazon Comprehendは、機械学習を使用してテキスト内で意味や関係性を検出する自然言語処理（NLP）サービスです。また、Amazon Transcribeは音声をテキストに簡単に変換するためのサービスで、これらのサービスを組み合わせることで、自動的にインデックスを作成し、カテゴリー化することができます。選択肢Bは手動ラベリングが必要で、選択肢CとDはカスタムモデルの開発が必要で時間がかかります。",
            "references": ["https://aws.amazon.com/jp/rekognition/content-moderation/"]
        },
        {
            "category": "機械学習の実装と運用",
            "question": "ある機械学習チームには Amazon S3 に複数の大量の CSV データセットがあります。 Amazon SageMaker 線形学習アルゴリズムで構築されたモデルは、これまで同様のサイズのデータセットでトレーニングするのに数時間かかっています。チー��のリーダーはトレーニングプロセスを短縮する必要があります。機械学習のスペシャリストは、この問題を解決するためにどうすればよいですか。",
            "options": {
                "A": "Amazon SageMaker のパイプモードを使用する。",
                "B": "Amazon Machine Learning を使用してモデルをトレーニングする。",
                "C": "Amazon Kinesis を使用して Amazon SageMaker にデータをストリーミングする。",
                "D": "AWS Glue を使用して CSV データセットを JSON 形式に変換する。"
            },
            "correct_answer": [
                "A"
            ],
            "explanation": "Amazon SageMaker のパイプモードを使用すると、データがコンテナに直接ストリーミングされるため、トレーニングジョブのパフォーマンスが向上します。パイプモードの場合、トレーニングジョブは Amazon S3 から直接データをストリーミングします。ストリーミングされることで、トレーニングジョブの開始がスピードアップして、スループットが向上します。パイプモードでは、トレーニングインスタンス用の Amazon EBS ボリュームのサイズも低減されます。B はこのシナリオには当てはまりません。C はストリーミングを取り込むソリューションであるとはいえ、このシナリオには当てはまりません。D の場合、データ構造が変換されます。"
        }
    ]
}